#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import numpy as np
import pandas as pd
from subprocess import check_output
print(check_output(["ls", "../../../input/crawford_weekly-sales-transactions"]).decode("utf8"))
import matplotlib.pyplot as plt
sales=pd.read_csv("../../../input/crawford_weekly-sales-transactions/Sales_Transactions_Dataset_Weekly.csv")
print(sales.describe().T)
print(sales.head())


# In[ ]:


def dddraw(X_reduced,name):
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    # To getter a better understanding of interaction of the dimensions
    # plot the first three PCA dimensions
    fig = plt.figure(1, figsize=(8, 6))
    ax = Axes3D(fig, elev=-150, azim=110)
    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)
    titel="First three directions of "+name 
    ax.set_title(titel)
    ax.set_xlabel("1st eigenvector")
    ax.w_xaxis.set_ticklabels([])
    ax.set_ylabel("2nd eigenvector")
    ax.w_yaxis.set_ticklabels([])
    ax.set_zlabel("3rd eigenvector")
    ax.w_zaxis.set_ticklabels([])

    print()


# Clustering the products
# ---

# In[ ]:


from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis
from sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection
from sklearn.cluster import KMeans,Birch
import statsmodels.formula.api as sm
from scipy import linalg
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures
def rmsle(y_predicted, y_real):
    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
def procenterror(y_predicted, y_real):
     return ( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ).round()

n_col=50
X = sales.drop(['Product_Code','W51','Normalized 51'],axis=1)
def rmsle(y_predicted, y_real):
    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
def procenterror(y_predicted, y_real):
     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)

    
Y=sales['W51']
X=X.fillna(value=0)  #nasty NaN
#scaler = MinMaxScaler()
#scaler.fit(X)
#X=scaler.transform(X)
#poly = PolynomialFeatures(2)
#X=poly.fit_transform(X)


names = [ 'PCA', 'FastICA', 'Gauss', 'KMeans', 'SparsePCA', 'SparseRP', 'Birch', 'NMF',  ] 

classifiers = [  PCA(n_components=n_col), FastICA(n_components=n_col), GaussianRandomProjection(n_components=3), KMeans(n_clusters=n_col),  SparseRandomProjection(n_components=n_col, dense_output=True), Birch(branching_factor=10, n_clusters=7, threshold=0.5), NMF(n_components=n_col),   ] 
correction= [1,1,0,0,0,0,0,0,0]

temp=zip(names,classifiers,correction)
print(temp)

for name, clf,correct in temp:
    Xr=clf.fit_transform(X,Y)
    dddraw(Xr,name)
    res = sm.OLS(Y,Xr).fit()
    #print(res.summary())  # show OLS regression
    #print(res.predict(Xr).round()+correct)  #show OLS prediction
    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction
    
    print('Ypredict',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction
    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y)) #
    
    
    
    


# Clustering the weeks
# ---
# 

# In[ ]:


n_col=4
kolom=sales.Product_Code
X = (sales.drop(['Product_Code'],axis=1).T)[:51] #sales 51weeks 
X.columns=kolom

def rmsle(y_predicted, y_real):
    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
def procenterror(y_predicted, y_real):
     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)

    
Y=X['P33']
X=X.fillna(value=0)  #nasty NaN

names = [ 'PCA', 'FastICA', 'Gauss', 'KMeans', 'SparsePCA', 'SparseRP', 'Birch', 'NMF',  ] 

classifiers = [  PCA(n_components=n_col), FastICA(n_components=n_col), GaussianRandomProjection(n_components=3), KMeans(n_clusters=n_col),  SparseRandomProjection(n_components=n_col, dense_output=True), Birch(branching_factor=10, n_clusters=3, threshold=0.5), NMF(n_components=n_col),   ] 
correction= [1,1,0,0,0,0,0,0,0]

temp=zip(names,classifiers,correction)
print(temp)

for name, clf,correct in temp:
    Xr=clf.fit_transform(X,Y)
    dddraw(Xr,name)
    res = sm.OLS(Y,Xr).fit()
    #print(res.summary())  # show OLS regression
    #print(res.predict(Xr).round()+correct)  #show OLS prediction
    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction
    
    print('Ypredict',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction
    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y)) #
    
    
    


# In[ ]:


from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}

    
# import some data to play with
#X = df_new[df_new['split']==0]
X = X.drop(['P33'],axis=1)
def rmsle(y_predicted, y_real):
    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
def procenterror(y_predicted, y_real):
     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)

#Y=df_new[df_new['split']==0]


#X=X.replace([np.inf, -np.inf], np.nan).fillna(value=0)
#print(X) #nasty NaN
#scaler = MinMaxScaler()
#scaler.fit(X)
#X=scaler.transform(X)
#poly = PolynomialFeatures(2)
#X=poly.fit_transform(X)


names = [  'SVC', 'kSVC', 'KNN', 'DecisionTree', 'RandomForestClassifier',  'HuberRegressor', 'Ridge', 'Lasso', 'LassoCV', 'Lars',  'SGDClassifier', 'RidgeClassifier', 'LogisticRegression', 'OrthogonalMatchingPursuit',  ] 

classifiers = [  SVC(), SVC(kernel = 'rbf', random_state = 0), KNeighborsClassifier(n_neighbors = 1), DecisionTreeClassifier(), RandomForestClassifier(n_estimators = 200),  HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95), Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True), Lasso(alpha=0.05), LassoCV(), Lars(n_nonzero_coefs=10),  SGDClassifier(), RidgeClassifier(), LogisticRegression(), OrthogonalMatchingPursuit(),  ] 
correction= [0,0,0,0,0,0,0,0,0,0,0,0]

temp=zip(names,classifiers,correction)
print(temp)

for name, clf,correct in temp:
    regr=clf.fit(X,Y)
    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)
    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score

    # Confusion Matrix
    print(name,'Confusion Matrix')
    print(confusion_matrix(Y, np.round(regr.predict(X) ) ) )
    print('--'*40)

    # Classification Report
    print('Classification Report')
    print(classification_report(Y,np.round( regr.predict(X) ) ))

    # Accuracy
    print('--'*40)
    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)
    print('Accuracy', logreg_accuracy,'%')

