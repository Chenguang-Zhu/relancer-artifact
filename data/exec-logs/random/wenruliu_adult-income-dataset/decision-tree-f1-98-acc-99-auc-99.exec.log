fold: 0 - cp:1 train: 0.8044928044928045 test: f1=0.5334434351775392, acc=0.8016964024568587
fold: 0 - cp:2 train: 0.8206973206973207 test: f1=0.5537225649117773, acc=0.8180169640245686
fold: 0 - cp:3 train: 0.8225985725985727 test: f1=0.6251162790697674, acc=0.834980988593156
fold: 0 - cp:4 train: 0.8221598221598222 test: f1=0.6390458254865035, acc=0.8318221702252121
fold: 0 - cp:5 train: 0.8254943254943256 test: f1=0.6554948391013965, acc=0.8340450424100614
fold: 0 - cp:6 train: 0.8927986427986427 test: f1=0.7974349302150132, acc=0.9057619186896754
fold: 0 - cp:7 train: 0.9163449163449164 test: f1=0.8533792933645206, acc=0.9303305059959053
fold: 0 - cp:8 train: 0.9145899145899146 test: f1=0.8350090229440578, acc=0.9251243053524423
fold: 0 - cp:9 train: 0.912981162981163 test: f1=0.8246628131021195, acc=0.9201520912547528
fold: 0 - cp:10 train: 0.9141804141804142 test: f1=0.8416833667334669, acc=0.9260602515355367
fold: 0 - cp:11 train: 0.9593717093717095 test: f1=0.9287918055041343, acc=0.9662474407721556
fold: 0 - cp:12 train: 0.9663039663039663 test: f1=0.9467138153883672, acc=0.9744369698742322
fold: 0 - cp:13 train: 0.9774189774189774 test: f1=0.9696896217264792, acc=0.9853758408891489
fold: 0 - cp:14 train: 0.9796127296127296 test: f1=0.971490698236289, acc=0.9861947937993565
fold: 0 - cp:15 train: 0.9807534807534808 test: f1=0.9744397334948516, acc=0.9876572097104417
fold: 0 - cp:16 train: 0.9805779805779806 test: f1=0.969895605729546, acc=0.9854928341620357
fold: 0 - cp:17 train: 0.9797589797589797 test: f1=0.9697189922480621, acc=0.9853758408891489
fold: 0 - cp:18 train: 0.9807827307827307 test: f1=0.9677341096555071, acc=0.9844398947060544
fold: 0 - cp:19 train: 0.9812214812214812 test: f1=0.9684542586750788, acc=0.9847908745247148
fold: 1 - cp:1 train: 0.8007602673898884 test: f1=0.5118248175182482, acc=0.8043758043758044
fold: 1 - cp:2 train: 0.8213219498022961 test: f1=0.6021912620045989, acc=0.827951327951328
fold: 1 - cp:3 train: 0.836472886660065 test: f1=0.6663409868099659, acc=0.8401778401778401
fold: 1 - cp:4 train: 0.8378765390347047 test: f1=0.6657513729405892, acc=0.8433368433368433
fold: 1 - cp:5 train: 0.8331382567168716 test: f1=0.5855579868708971, acc=0.8338013338013338
fold: 1 - cp:6 train: 0.8936239911614268 test: f1=0.7979456344732556, acc=0.9056394056394056
fold: 1 - cp:7 train: 0.9267038614254337 test: f1=0.8730218068535825, acc=0.9403884403884404
fold: 1 - cp:8 train: 0.9267330361738411 test: f1=0.8675993005246065, acc=0.937989937989938
fold: 1 - cp:9 train: 0.9259138950587243 test: f1=0.8632317914002757, acc=0.9361764361764362
fold: 1 - cp:10 train: 0.9257676620217564 test: f1=0.8543664493031582, acc=0.9333684333684333
fold: 1 - cp:11 train: 0.9607779692009921 test: f1=0.9211050273768044, acc=0.9629109629109629
fold: 1 - cp:12 train: 0.9622111419315443 test: f1=0.9309625996321276, acc=0.967064467064467
fold: 1 - cp:13 train: 0.9820414572695808 test: f1=0.9717732207478891, acc=0.9863109863109863
fold: 1 - cp:14 train: 0.9786779571232076 test: f1=0.9722390592799126, acc=0.9866034866034866
fold: 1 - cp:15 train: 0.9799649481446393 test: f1=0.9673597678916828, acc=0.9842049842049843
fold: 1 - cp:16 train: 0.981398137984474 test: f1=0.9721115537848606, acc=0.9864864864864865
fold: 1 - cp:17 train: 0.979409156526611 test: f1=0.9733720648753328, acc=0.9871299871299871
fold: 1 - cp:18 train: 0.9798187185295281 test: f1=0.9712629799565321, acc=0.9860769860769861
fold: 1 - cp:19 train: 0.9806083632417277 test: f1=0.9740072202166065, acc=0.9873639873639873
fold: 2 - cp:1 train: 0.8099333099333099 test: f1=0.5590474871936868, acc=0.8136882129277566
fold: 2 - cp:2 train: 0.812916812916813 test: f1=0.5575498575498575, acc=0.8183094472067856
fold: 2 - cp:3 train: 0.8211360711360711 test: f1=0.6258360655737705, acc=0.8331090962269669
fold: 2 - cp:4 train: 0.8177138177138177 test: f1=0.6563064691001085, acc=0.8331090962269669
fold: 2 - cp:5 train: 0.8231250731250731 test: f1=0.6446322067594434, acc=0.8326996197718631
fold: 2 - cp:6 train: 0.8897273897273897 test: f1=0.7859044238813537, acc=0.9011991810470897
fold: 2 - cp:7 train: 0.9187141687141687 test: f1=0.8469145074477405, acc=0.9284586136297163
fold: 2 - cp:8 train: 0.9143851643851644 test: f1=0.8401780038143674, acc=0.9264697279906405
fold: 2 - cp:9 train: 0.916169416169416 test: f1=0.8426241941600303, acc=0.9271716876279614
fold: 2 - cp:10 train: 0.9175149175149175 test: f1=0.8369386464263124, acc=0.9245978356244516
fold: 2 - cp:11 train: 0.9587282087282087 test: f1=0.9292481850621386, acc=0.9663644340450425
fold: 2 - cp:12 train: 0.9605124605124605 test: f1=0.9363992172211351, acc=0.9695817490494296
fold: 2 - cp:13 train: 0.973060723060723 test: f1=0.953782534663099, acc=0.9777712781515063
fold: 2 - cp:14 train: 0.982040482040482 test: f1=0.9748877835739415, acc=0.9878911962562152
fold: 2 - cp:15 train: 0.9805779805779805 test: f1=0.969667556418345, acc=0.9853758408891489
fold: 2 - cp:16 train: 0.9814262314262314 test: f1=0.9705061293846341, acc=0.9857853173442527
fold: 2 - cp:17 train: 0.9791447291447292 test: f1=0.9694610049884413, acc=0.9853173442527055
fold: 2 - cp:18 train: 0.9805487305487306 test: f1=0.9714146697482059, acc=0.9862532904357999
fold: 2 - cp:19 train: 0.9813969813969814 test: f1=0.967961992934584, acc=0.9846153846153847
fold: 3 - cp:1 train: 0.7997367695835174 test: f1=0.5162378743146351, acc=0.7987012987012987
fold: 3 - cp:2 train: 0.820678586033055 test: f1=0.6007594250067806, acc=0.8277758277758278
fold: 3 - cp:3 train: 0.8300379897244661 test: f1=0.658802410565457, acc=0.8443313443313444
fold: 3 - cp:4 train: 0.8268792466265559 test: f1=0.6279718422101208, acc=0.8361413361413361
fold: 3 - cp:5 train: 0.8267620275107407 test: f1=0.6109541111559682, acc=0.8308763308763308
fold: 3 - cp:6 train: 0.8908452007422527 test: f1=0.7834580743371812, acc=0.9001404001404001
fold: 3 - cp:7 train: 0.9155600759192242 test: f1=0.8468222442899702, acc=0.9278109278109278
fold: 3 - cp:8 train: 0.9149458116175476 test: f1=0.847, acc=0.9283959283959284
fold: 3 - cp:9 train: 0.9136296903318428 test: f1=0.8362220547601105, acc=0.9237159237159237
fold: 3 - cp:10 train: 0.9150042911449088 test: f1=0.8314462171899125, acc=0.9233649233649234
fold: 3 - cp:11 train: 0.9587891451485274 test: f1=0.9243676742751388, acc=0.9641394641394642
fold: 3 - cp:12 train: 0.9640536747754811 test: f1=0.9369150358575423, acc=0.9696384696384697
fold: 3 - cp:13 train: 0.9785609022565035 test: f1=0.9711330286264134, acc=0.9859599859599859
fold: 3 - cp:14 train: 0.9834162531284666 test: f1=0.9710022861268199, acc=0.9859014859014859
fold: 3 - cp:15 train: 0.9820415257067105 test: f1=0.9746499275712217, acc=0.9877149877149877
fold: 3 - cp:16 train: 0.9823925021047155 test: f1=0.9739004349927501, acc=0.9873639873639873
fold: 3 - cp:17 train: 0.9811932748584596 test: f1=0.9720710917664127, acc=0.9864864864864865
fold: 3 - cp:18 train: 0.9825095569714194 test: f1=0.96944177093359, acc=0.9851409851409851
fold: 3 - cp:19 train: 0.9813395660669876 test: f1=0.9706236455574284, acc=0.9857259857259857
fold: 0 - cp:14 train: 0.9896327591810206 test: f1=0.9854368932038836, acc=0.9929804036267914
fold: 1 - cp:14 train: 0.9893077673058175 test: f1=0.9842233009708738, acc=0.9923954372623575
fold: 2 - cp:14 train: 0.9887552811179721 test: f1=0.9746682750301567, acc=0.9877157063468851
fold: 3 - cp:14 train: 0.9888202794930127 test: f1=0.9765201685731486, acc=0.9885931558935361
fold: 4 - cp:14 train: 0.9889827754306142 test: f1=0.969258589511754, acc=0.9850833577069319
fold: 5 - cp:14 train: 0.9891452713682158 test: f1=0.9765484064942874, acc=0.9885931558935361
fold: 6 - cp:14 train: 0.9887877803054923 test: f1=0.9780755176613886, acc=0.9894706054401872
fold: 7 - cp:14 train: 0.9891127721806955 test: f1=0.9739235900545785, acc=0.987423223164668
fold: 8 - cp:14 train: 0.9882027949301267 test: f1=0.9728424864212432, acc=0.986838256800234
fold: 9 - cp:14 train: 0.9900880223283567 test: f1=0.9789029535864979, acc=0.9897600936220011



PC9
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay

PC4
Features:
	workclass_Local-gov workclass_Never-workeddot: graph is too large for cairo-renderer bitmaps. Scaling by 0.372175 to fit

	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC8
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Federal-gov workclass_Never-worked
	marital-status_Married-AF-spouse
	workclass_?^2

PC14
Features:
	workclass_Local-gov workclass_Never-worked
	marital-status_Married-AF-spouse
	workclass_Never-worked
	occupation_Armed-Forces

PC2
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	occupation_Armed-Forces
	workclass_Never-worked

PC10
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	workclass_Local-gov

PC3
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay
	race_Asian-Pac-Islander

PC7
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_State-gov
	occupation_Armed-Forces
	workclass_Never-worked

PC5
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay
	workclass_Never-worked

PC12
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	marital-status_Married-AF-spouse
	occupation_Armed-Forces

PC13
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Never-worked
	workclass_Without-pay

PC6
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	marital-status_Married-AF-spouse

PC11
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC1
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay
	marital-status_Married-AF-spouse

Normalized confusion matrix
[[9.99614198e-01 3.85802469e-04]
 [8.47457627e-03 9.91525424e-01]]

              precision    recall  f1-score   support

           0       1.00      1.00      1.00      2592
           1       1.00      0.99      1.00       826

    accuracy                           1.00      3418
   macro avg       1.00      1.00      1.00      3418
weighted avg       1.00      1.00      1.00      3418

