Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f7e34e72348>

Traceback (most recent call last):
  File "sales-exploration.py", line 86, in <module>
    res = sm.OLS(Y,Xr).fit()
AttributeError: module 'statsmodels.formula.api' has no attribute 'OLS'
[Try Solution]: OrderedDict([('old_fqn', 'statsmodels.formula.api.OLS'), ('new_fqn', 'statsmodels.regression.linear_model.OLS'), ('line_no', 86)])Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f5ccdbfc348>

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
PCA %error 26.2 rmsle 0.4559724874683822

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
FastICA %error 26.2 rmsle 0.45597248747658736

Ypredict [ 0.  6.  7.  3. 10.  3.  3. 10. 10. 19.  6.  5. 11.  7. 34. 29. 19. 31.
 19.  7.  8.  5.  6. 25. 27.  9. 24. 32. 10. 35.  1.  1. 10. 33. 26. 28.
 32. 33. 32. 25. 20. 27. 32. 27. 25. 14. 35. 25. 22.  7. 23. 23.  1. 31.
 27. 25. 23. 25.  9. 25. 22. 18. 22. 25.  9. 34. 22.  8. 15. 14.  7. 25.
 29. 10. 31. 17.  6. 29. 21. 32.  9.  4. 20. 25. 19. 21. 10. 27. 27. 14.
  7. 25.  3. 10. 14. 22. 28.  2.  6. 14. 31. 29.  6.  4.  6. 10. 16.  1.
  4.  7.  4. 29. 23.  7. 14.  5.  6.  6. 20. 19.  5.  9.  3.  6.  8.  3.
  3. 41. 32. 28. 26. 35. 21. 30. 30. 25. 33. 31. 22. 24. 23. 18. 29.  7.
  7.  1. 11. -1.  7.  4.  4. 11.  9. 12.  3.  0. -3.  5.  3.  2.  4.  8.
  2.  4.  2.  8. 22. 27. 33. 32.  7. 29. 25. 29. 26. 28. 26. 33. 26. 30.
 25. 32. 27. 34. 31. 25. 24. 17. 28. 25. 27. 31. 25. 15.  3. 30.  8. 15.
  4. 12.  5. 15.  4.  2. 18.  4. 10. 31. 10. 16.  9. -1.  0.  0.  0.  0.
  1. -1.  1.  0. -1.  0.  0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  1.  0.  1.  1.  0.  0.
  0.  0.  1.  0.  0.  0.  0.  0. 23. 21. 19.  8.  1.  4.  7. 22. 10. 14.
  3.  1.  0.  0. -1. -1.  0.  0.  0.  0.  1.  0.  0. 19.  8. 15.  1.  1.
  1.  0.  6.  4.  5.  3.  6.  3.  5.  3.  6.  2.  4.  2.  3.  3.  3.  3.
  2.  2.  6.  2.  3.  6.  3.  6.  4.  3.  4.  2.  1.  3.  4.  2.  3. 11.
  2.  4.  4.  5.  3.  1.  5.  9.  6.  2.  5.  3.  4.  4.  1.  1.  2.  3.
  1.  2.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.  4.  0.  2.  8.  0.  3.
  3.  0. 14.  1.  1.  2.  8.  4.  6.  3.  2.  1.  3.  1.  1.  1.  0.  0.
  0.  0.  0. -1.  1.  0.  2.  2.  4.  3.  2.  5.  2.  2.  5.  1.  8. 12.
 18. 13.  3.  3. 13. 12. 18.  6. 22. 10. 42. 15. 15.  5.  6.  1.  3.  0.
  1.  0.  0.  1.  0.  2.  0.  0.  0.  0.  1.  0.  7. 12.  5. 12.  4.  0.
 18. 13.  6.  0.  0.  1. -1.  0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  0.
  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  0.  0.  0.  5. -2.  0.  1. -1.  0.  3.  1.  0.  4.  5.  8.  8.  9.
  3.  7.  9. 12.  5.  9.  8.  5.  4.  2.  8.  7.  4.  7. 17. 13. 17.  3.
 18.  9.  6.  7. 17. 18. 13.  8.  9. 13. 10.  9. 13. 14. 10.  6.  5.  9.
 16. 15. 13. 11. 14. 12.  2. 12. 26. 12. 15. 13. 18. 12.  6. 14. 10.  7.
  9. 12. 14. 12.  8. 24. 21. 10. 10.  7.  1. 14. 16. 18. 22. 12. 16.  5.
  1.  2.  8.  7.  4. 11.  3.  3.  2.  4.  4.  1.  4.  0.  1.  1.  0.  1.
  0.  2.  2.  1.  4.  3.  1.  7.  5.  2.  3.  5.  2.  2.  4.  5. -1.  4.
  0. 15.  6.  1.  2.  2.  1.  1. -1.  1.  0.  0.  2.  6.  3.  8. 28.  2.
 26.  4. 21. 26. 25. 23. 11. 20. 25.  6.  7. 12. 11.  6. 13.  0.  6.  6.
  6. 13.  7. 13.  4. 20.  0. 19.  3.  3.  0.  0.  0.  0.  0.  0.  0.  0.
  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1. -1.  0.  1.  0.  0.  1.
  1.  1.  4.  6.  5.  4.  0.  0.  1.  0. -1.  0.  0.  0.  0.  4.  3.  0.
  0.  0.  0.  1.  0.  0.  1.  0.  1.  3.  1.  1.  4.  3.  3.  1.  4.  3.
  1.  0.  1.  0.  0.  0.  0.  0.  0. -1.  0.  1.  0.  0.  0.  1.  0.  1.
  0.  0.  0.  2.  0.  0.  1.  1.  4.  1.  0.  0.  0.  1.  1.  0.  5.  1.
  1.  0.  0.  1.  1.  3.  2.  0.  1.  0.  1. -1.  0.  0.  0.  2.  0.  4.
  2.  0.  2.  3.  2.  1.  0.  1.  1.  0.  0.  1. -1.  0.  0.  1. 14.  6.
 14.  4.  3.  4.  0.  3.  3.  3.  6.  1.  3.  2.  1.  6.  3.  1.  5.  3.
  3.  0.  4.  3.  1.  6.  2.  3.  2.  1.  4.  1.  3.  7.  0.  2.  1.  1.
  0.]
Gauss %error 39.9 rmsle 0.7063412041940412

Ypredict [ 8.  5.  9. 10.  9.  6.  5.  9. 11. 22. 13.  5. 10. 16. 20. 25. 24. 22.
 23.  9.  9.  9.  4. 21. 24. 11. 23. 25. 16. 24.  8.  9. 15. 26. 24. 20.
 23. 24. 24. 15. 23. 24. 28. 21. 20. 21. 27. 25. 25.  8. 17. 28.  6. 23.
 21. 21. 26. 25.  8. 23. 21. 17. 21. 20.  8. 30. 24.  9. 19. 22.  9. 22.
 16.  9. 23. 19.  5. 22. 22. 28. 10.  9. 29. 24. 21. 21. 23. 24. 21. 23.
 10. 23.  7. 10. 16. 26. 23.  6. 12. 18. 13. 21.  8.  3.  4. 12. 18.  6.
  9.  9.  5. 29. 20.  8. 11. 10.  5. 12. 26. 24.  9. 10.  4.  7. 10.  5.
  4. 24. 22. 22. 22. 23. 21. 25. 20. 24. 24. 24. 23. 27. 26. 26. 24. 10.
  8.  8.  8.  4.  9.  6.  4. 10. 10.  9.  4.  4.  8.  4.  3.  9.  4.  8.
  5. 11. 10. 10. 24. 18. 30. 27.  8. 25. 22. 22. 21. 20. 27. 23. 25. 25.
 21. 25. 21. 22. 24. 20. 27. 25. 25. 24. 21. 25. 17. 22.  6. 25. 12. 14.
  6. 26.  5. 24.  8.  5. 22.  5. 11. 41. 12. 18. 13.  2.  0.  1.  0.  0.
  1.  0.  0.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.
  0.  0.  3.  0.  2.  0.  0.  0. 29. 43. 32. 12.  4. 13. 10. 27. 21. 27.
  4.  1.  0.  0.  0.  0.  1.  0.  0.  0.  3.  2.  2. 20. 12. 28.  4.  2.
  1.  0.  5.  4.  6. 12.  7.  6.  6.  5. 11.  4.  6.  5.  5.  9.  7.  4.
  5.  4. 11.  5.  5.  6.  5. 12.  5.  5.  4.  4. 10.  5.  4.  5.  5. 12.
  4.  5.  4.  5.  5.  6.  5.  7.  9. 10.  3.  5.  5.  5.  3.  2.  5.  4.
  5.  4.  5.  1.  1.  1.  1.  0.  0.  0.  0.  0.  6.  4.  3.  8.  3.  5.
  3.  1. 19.  4.  5.  5. 12.  6.  6.  4.  5.  3.  3.  2.  2.  2.  1.  1.
  0.  0.  0.  0.  1.  0.  5.  5.  5.  4.  6.  9.  3.  4.  5.  5. 14. 19.
 20. 26.  5.  5. 22. 12. 29. 22. 21. 19. 71. 25. 25.  5. 14.  6.  3.  3.
  1.  1.  0.  0.  0.  3.  1.  1.  0.  0.  0.  0. 12. 18. 11. 13.  7.  5.
 20. 14.  9.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  2.  1.  1.  1.
  0.  0.  1.  1.  0.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.
  0.  1.  0.  0.  5.  3.  2.  3.  3.  2.  3.  2.  1.  6. 10. 20. 15. 13.
  5. 12. 19. 15. 14. 17. 21. 11.  7.  3.  9.  6.  7. 18. 25. 15. 21. 15.
 25.  9.  9. 10. 32. 31. 19. 16. 10. 22. 15. 10. 20. 16. 11. 12. 14. 12.
 16. 26. 12. 15. 15. 20.  6. 12. 29. 13. 27. 17. 23. 19.  9. 21. 12. 16.
 12. 15. 21. 16. 11. 22. 23.  8.  9.  7.  5. 21. 13. 20. 29. 16. 13. 11.
  5.  5. 10. 10.  9. 17.  3.  8.  5.  6.  9.  3.  6.  1.  1.  1.  2.  1.
  0.  4.  5.  4.  5.  4.  4. 11.  5.  5.  4.  5.  5.  3.  5.  5.  5. 11.
  4. 20.  9.  7.  4.  2.  1.  0.  3.  2.  1.  1.  6.  6.  7. 20. 34. 10.
 35.  8. 21. 25. 25. 24. 23. 30. 24.  8.  8.  9. 11.  8. 10.  9.  8. 12.
 11. 10.  7. 15.  6. 22.  0. 26.  4.  6.  0.  0.  0.  0.  0.  0.  1.  1.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  1.  1.  1.  0.  0.  2.
  1.  1.  7. 13.  9.  7.  0.  0.  1.  0.  0.  0.  0.  0.  0.  5.  3.  1.
  1.  1.  1.  5.  5.  1.  1.  1.  2.  5.  3.  2.  6. 10.  6.  2.  5.  3.
  3.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  3.  6.  2.  1.  1.  1.  3.  2.  0. 11.  3.
  6.  2.  2.  1.  1.  4.  2.  3.  1.  1.  1.  1.  1.  0.  0.  5.  0.  8.
  2.  1.  4.  8.  4.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  1. 23.  5.
 19.  6.  5.  4.  3.  5.  7.  5. 11.  3.  7.  2.  1.  6.  7.  5.  9.  4.
  6.  2.  5.  6.  4.  9.  4.  6.  2.  1.  5.  3.  5. 10.  1.  6.  2.  1.
  1.]
KMeans %error 23.4 rmsle 0.41156386136366246

Ypredict [ 5.  4.  8.  8. 10.  4.  6.  8.  9. 17.  8.  6. 11. 14. 22. 27. 24. 23.
 21.  9.  6.  8.  5. 26. 21.  9. 24. 23. 11. 22.  8. 10. 14. 30. 25. 23.
 28. 27. 29. 18. 23. 24. 32. 25. 22. 19. 26. 26. 27.  7. 11. 28.  5. 27.
 21. 23. 24. 27.  7. 24. 20. 14. 18. 21. 10. 26. 23. 10. 18. 22.  7. 22.
 20.  8. 22. 19.  7. 21. 24. 26. 12. 10. 23. 24. 21. 20. 17. 21. 19. 23.
 11. 23.  5. 11. 15. 30. 23.  5. 14. 20. 21. 22.  9.  2.  5. 11. 18.  4.
 10.  9.  5. 28. 24.  7.  8. 10.  6. 15. 25. 26.  7.  9.  4.  6. 11.  5.
  2. 23. 26. 19. 25. 26. 21. 23. 26. 26. 28. 29. 25. 24. 23. 27. 23. 13.
  5.  7.  7.  1.  7.  7.  2. 10. 10.  9.  5.  4.  7.  3.  4.  7.  4.  8.
  7. 11.  7.  6. 24. 20. 27. 27.  6. 23. 29. 25. 27. 30. 27. 27. 28. 25.
 19. 21. 18. 27. 24. 22. 25. 21. 22. 27. 25. 26. 22. 18.  5. 24. 11. 15.
  7. 26.  4. 22.  8.  3. 25.  4. 10. 42. 12. 15. 10.  1.  1.  1.  0.  1.
  2.  0.  0.  2.  1.  1.  1.  2.  1.  1.  1.  0. -1.  0.  0. -1.  2.  1.
  0.  0.  0.  2.  1.  1.  1.  2.  0.  1.  3.  1.  2.  2.  0.  1.  1.  0.
  1.  0.  4.  0.  3.  0.  0.  1. 29. 48. 31. 13.  3. 15. 10. 23. 21. 24.
  5.  1.  0.  1.  0.  0.  1.  1.  0.  1.  3.  2.  2. 17. 12. 27.  4.  2.
  0.  0.  6.  3.  8. 11.  7.  9.  5.  5. 12.  4.  6.  4.  5.  8.  5.  4.
  5.  5. 10.  3.  4.  5.  5.  9.  4.  6.  5.  4.  8.  5.  5.  6.  4. 13.
  5.  5.  3.  7.  5.  7.  4.  6.  9.  9.  3.  4.  4.  6.  3.  3.  5.  3.
  4.  4.  5.  2.  3.  1.  1.  1.  1.  1.  1.  0.  7.  4.  4.  7.  3.  5.
  3.  1. 17.  3.  4.  3. 11.  5.  9.  3.  6.  3.  3.  3.  1.  2.  2.  0.
  0. -1.  1.  1.  2.  1.  5.  7.  4.  4.  5.  7.  3.  3.  5.  5. 13. 17.
 19. 28.  4.  6. 18. 13. 28. 19. 18. 19. 60. 23. 26.  5. 13.  5.  4.  3.
  3.  2.  1.  1.  0.  3.  1.  1.  1.  1.  2.  0. 13. 17.  8. 13.  6.  4.
 19. 17.  7.  2.  1.  2.  2.  0.  1.  1.  1.  1.  2.  2.  1.  2.  1.  0.
  1.  0.  2.  1.  1.  0.  1.  1.  0.  1.  1.  2.  1.  1.  0.  1.  1.  2.
  0.  2.  1.  1.  4.  3.  2.  4.  3.  3.  3.  2.  0.  5.  9. 18. 13. 13.
  5. 13. 17. 14. 10. 16. 21.  9.  7.  3. 10.  6.  4. 17. 25. 15. 20. 16.
 22. 10.  9. 11. 27. 30. 17. 15. 12. 20. 15. 12. 20. 14. 11. 12. 10. 11.
 15. 25. 10. 17. 14. 21.  5. 12. 23. 17. 27. 16. 24. 16.  8. 17. 11. 17.
 12. 12. 22. 13. 12. 20. 24. 11.  9.  7.  4. 19. 13. 19. 22. 15. 12. 11.
  5.  5.  9.  7.  8. 17.  1.  9.  5.  5. 10.  4.  6.  0.  3.  0.  2.  1.
  0.  4.  5.  3.  5.  4.  3. 13.  6.  5.  4.  4.  6.  3.  5.  4.  4.  8.
  5. 20. 11.  6.  4.  0.  1.  0.  3.  3.  1.  2.  6.  8.  7. 17. 34. 10.
 35.  8. 18. 24. 23. 25. 26. 27. 24.  8.  7.  8. 11.  8. 10.  9.  5. 15.
 10. 12.  7. 11.  6. 21. -1. 25.  4.  7.  1.  0.  0.  1.  0.  1.  2.  2.
  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  2.  0.  2.  0.  3.
  2.  2.  7. 15.  9.  7.  1.  0.  2.  0. -1.  1.  1.  0.  0.  4.  3.  1.
  1.  1.  0.  5.  5.  2.  0.  1.  2.  5.  2.  2.  5. 10.  5.  3.  6.  4.
  3.  0.  1.  2.  0.  1.  0.  0. -1.  0.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  1.  2.  3.  2.  4.  3.  1.  1.  1.  3.  3.  0. 11.  3.
  7.  1.  1.  1.  1.  2.  0.  1.  2.  2.  2.  0.  0.  0.  0.  6.  0.  9.
  3.  1.  2.  9.  4.  2.  1.  1.  2.  0.  1.  2.  1.  0.  1.  1. 21.  4.
 21.  6.  4.  2.  2.  6.  7.  6.  9.  4.  8.  3.  1.  6.  8.  4.  9.  3.
  7.  3.  5.  7.  4.  8.  5.  7.  4.  0.  6.  3.  6. 10.  1.  5.  2.  2.
  1.]
SparsePCA %error 26.8 rmsle 0.49139347324821425

Ypredict [10.  0.  7.  8.  9.  6.  1.  9. 13. 20. 21.  3.  5. 18. 20. 22. 25. 23.
 29.  9.  5.  5.  0. 14. 20. 21. 28. 25. 16. 21.  7.  7. 11. 26. 28. 19.
 29. 24. 25. 22. 27. 16. 27. 16. 20. 14. 20. 27. 23.  9.  7. 20.  9. 26.
 25. 19. 20. 31. 13. 21. 26. 10. 20. 18.  8. 23. 24.  6. 22. 30.  6. 23.
 18. 10. 24. 15.  6. 20. 21. 24. 12.  7. 33. 21. 16. 25. 24. 24. 28. 18.
  9. 19. 10.  9. 14. 21. 25.  5. 11. 21. 15. 29.  6.  4.  4. 21. 20.  4.
 10.  7.  9. 35. 16.  8. 10. 12.  2. 10. 19. 32. 12. 12.  4.  4.  6.  5.
  0. 22. 22. 15. 24. 20. 17. 26. 16. 16. 28. 24. 24. 29. 23. 25. 24. 12.
  7. 13.  3.  7.  5.  3.  4. 11. 10.  6.  5.  4.  5.  5.  7. 11.  6. 13.
  3.  9. 11. 11. 28. 18. 33. 32.  9. 25. 15. 24. 19. 13. 28. 26. 25. 16.
 15. 20. 19. 30. 28. 20. 28. 24. 22. 31. 24. 31. 21. 27.  2. 28. 10. 16.
  2. 26.  4. 23.  7.  4. 17.  5. 12. 45. 15. 14. 14.  1.  1.  0.  0.  1.
  0.  2.  1.  0.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  1.  0.  0.  0.  0.  3.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.
  0.  0.  8.  0.  2.  0.  0.  0. 34. 43. 23.  9.  3. 19. 14. 22. 14. 31.
  5.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0. 18. 15. 29.  1.  2.
  0.  2.  6.  5.  1.  4.  3.  7.  2.  8. 12.  6.  1.  6.  2. 11.  7.  7.
  9.  2.  8.  4.  4.  6.  6. 13.  9.  2.  6.  7.  5. 10.  3.  2.  4.  8.
  6.  1.  1.  5.  3.  6.  6.  9.  9.  9.  6.  4.  3.  5.  2.  1.  8.  6.
  5.  3.  8.  1.  0.  1.  2.  0.  0.  1.  0.  0.  8.  2.  5.  9.  2.  5.
  2.  0. 27.  7.  6.  6. 12.  3.  5.  3.  4.  4.  1.  3.  1.  0.  1.  0.
  2.  0.  0.  0.  2.  0.  9.  4.  5.  2.  9. 10.  5.  6.  3.  5. 11. 18.
 23. 19.  9.  0. 25. 13. 31. 24. 19. 21. 73. 31. 22.  4. 16.  6.  4.  3.
  0.  0.  2.  0.  0.  1.  1.  1.  0.  1.  0.  1. 11. 20. 14. 14. 16.  9.
 28. 14.  9.  0.  1.  0.  0.  0.  0.  0.  0.  0.  3.  4.  2.  3.  0.  2.
  0.  1.  0.  2.  0.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  2.  4.  3.  3.  4.  1.  5.  2.  2.  2.  8. 20. 20.  9.
  4.  6. 21. 10.  9. 15. 24. 11.  4.  4. 12.  6.  8. 15. 21. 12. 11. 13.
 28. 12.  6. 14. 28. 35. 22. 16. 12. 20. 14. 17. 21. 22. 10.  9. 13. 12.
 11. 27. 11. 16. 15. 21.  6. 15. 38. 10. 37. 22. 20. 25.  9. 17. 19. 11.
  9. 10. 22. 15. 14. 30. 18.  6.  7.  8.  3. 16.  6. 29. 30. 21. 14. 15.
  5.  6.  9. 10. 10. 25.  3.  4.  1.  4. 11.  5.  5.  0.  2.  0.  1.  0.
  1.  7.  4.  1.  5.  5.  1. 11.  5.  4.  0.  8.  8.  6.  2.  4.  3.  7.
  4. 21. 16.  6.  3.  1.  0.  2.  4.  2.  1.  5.  9.  5. 10. 15. 30. 12.
 39.  4. 18. 33. 22. 19. 30. 25. 23.  4. 10. 11. 12.  8.  8.  6.  8.  9.
 13.  8.  7. 14.  3. 20.  1. 18.  4.  5.  0.  0.  0.  0.  0.  0.  1.  0.
  1.  0.  0.  0.  1.  2.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  2.
  0.  1.  7. 20.  9. 10.  0.  0.  0.  1.  1.  2.  0.  0.  0.  5.  1.  0.
  4.  0.  1.  4.  7.  0.  0.  1.  2.  3.  1.  5.  6. 17.  8.  1.  2.  5.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2. 12.  4.
  0.  0.  1.  5.  3.  1.  0.  6.  3.  3.  1.  1.  0.  2.  3.  0.  6.  6.
  8.  3.  1.  0.  1.  1.  2.  2.  1.  1.  2.  2.  1.  0.  0.  4.  0. 12.
  4.  2.  1. 11.  8.  1.  0.  0.  0.  0.  0.  0.  2.  1.  0.  0. 30.  3.
 25.  4.  4.  2.  7. 11.  8.  3. 13.  4.  9.  3.  2.  4.  3.  5. 10.  3.
 12.  2.  9.  0.  3.  8.  4. 14.  2.  1.  5.  4.  4. 11.  0.  5.  3.  0.
  1.]
SparseRP %error 0.0 rmsle 0.017457467380136024

Ypredict [ 7.  4.  6.  9. 10.  5.  6.  7. 10. 17.  8.  6. 11. 14. 21. 25. 23. 24.
 23. 10.  5.  9.  5. 23. 21.  9. 24. 21. 13. 21.  9. 10. 13. 29. 24. 22.
 28. 25. 27. 17. 22. 23. 31. 23. 23. 21. 27. 23. 25.  7. 11. 27.  5. 27.
 20. 22. 23. 27.  8. 24. 21. 15. 18. 21. 10. 27. 23. 11. 18. 20.  6. 23.
 22.  9. 24. 20.  6. 20. 24. 25. 12. 10. 27. 27. 21. 20. 18. 22. 20. 22.
 12. 23.  6. 10. 15. 29. 22.  6. 15. 20. 22. 21.  9.  2.  4. 11. 17.  5.
 10.  9.  6. 31. 22.  8.  9.  9.  5. 15. 24. 24.  7. 11.  4.  6. 11.  4.
  3. 22. 26. 20. 24. 27. 21. 24. 25. 26. 27. 29. 26. 23. 22. 26. 23. 13.
  5.  9.  7.  3.  6.  7.  3. 10. 10. 10.  6.  5.  8.  3.  4.  8.  4.  7.
  7. 12.  9.  7. 24. 22. 29. 27.  7. 22. 27. 24. 27. 28. 28. 24. 25. 24.
 19. 21. 20. 26. 24. 21. 24. 23. 21. 25. 25. 28. 21. 17.  6. 24. 11. 16.
  6. 25.  6. 22.  7.  4. 24.  4. 10. 40. 11. 14.  9.  2.  1.  1.  0.  1.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  1.
  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  2.  1.  2.  1.  0.  0.  0.  1.
  0.  0.  4.  0.  3.  1.  0.  0. 30. 49. 33. 14.  2. 16. 10. 22. 19. 25.
  5.  1.  1.  1.  1.  1.  1.  0.  0.  0.  3.  2.  3. 16. 12. 27.  4.  2.
  1.  1.  6.  4.  7. 12.  7.  7.  5.  5. 11.  4.  6.  5.  5. 10.  5.  3.
  6.  6. 10.  4.  5.  6.  5. 10.  5.  6.  6.  5. 10.  6.  5.  6.  5. 14.
  6.  7.  4.  8.  6.  7.  4.  7.  8. 10.  4.  4.  5.  6.  3.  2.  5.  3.
  4.  3.  5.  1.  2.  0.  1.  1.  1.  0.  0.  0.  7.  4.  3.  7.  3.  4.
  3.  1. 18.  4.  5.  4. 11.  4.  8.  4.  5.  3.  3.  2.  1.  2.  1.  1.
  1.  0.  0.  1.  1.  0.  7.  7.  4.  5.  5.  7.  3.  3.  5.  4. 14. 16.
 18. 29.  5.  5. 18. 12. 28. 21. 18. 18. 61. 24. 27.  5. 14.  7.  4.  3.
  2.  2.  1.  0.  0.  3.  1.  1.  1.  0.  1.  1. 14. 17.  9. 14.  7.  5.
 20. 15.  8.  1.  1.  1.  1.  1.  1.  0.  1.  1.  2.  2.  2.  1.  1.  1.
  1.  1.  2.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.
  1.  1.  1.  1.  3.  2.  1.  4.  4.  2.  3.  2.  1.  5.  8. 19. 14. 14.
  5. 13. 17. 15. 12. 17. 22. 11.  7.  4. 12.  5.  6. 17. 25. 14. 19. 18.
 24. 10. 10. 11. 28. 30. 17. 15. 13. 20. 15. 11. 20. 15. 11. 13. 11. 11.
 14. 27. 11. 17. 12. 20.  6. 13. 24. 15. 26. 17. 24. 16.  9. 18. 10. 16.
 12. 12. 22. 13. 10. 21. 22. 10.  8.  6.  5. 19. 12. 20. 23. 14. 12. 11.
  5.  6. 10. 10.  8. 17.  2.  9.  5.  5. 11.  4.  6.  1.  2.  1.  2.  1.
  1.  4.  5.  3.  5.  4.  3. 13.  6.  5.  4.  3.  5.  3.  5.  5.  4.  8.
  5. 20. 11.  6.  4.  2.  1.  0.  3.  3.  1.  2.  7.  8.  8. 17. 34. 11.
 34.  8. 20. 25. 23. 24. 29. 28. 24.  8.  6.  9. 11.  9. 10. 12.  7. 15.
 12. 10.  6. 11.  7. 21.  1. 26.  4.  6.  1.  0.  1.  0.  0.  0.  2.  1.
  1.  0.  1.  0.  1.  1.  0.  1.  0.  0.  2.  1.  1.  1.  0.  0.  0.  2.
  2.  1.  7. 17.  9.  7.  0.  1.  1.  1.  0.  1.  0.  0.  1.  4.  2.  1.
  2.  2.  1.  5.  5.  1.  1.  1.  1.  5.  3.  2.  6. 10.  6.  2.  6.  3.
  3.  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  2.  5.  2.  1.  1.  1.  3.  3.  0. 11.  3.
  7.  2.  2.  1.  1.  3.  1.  2.  1.  2.  1.  1.  1.  0.  0.  5.  0.  9.
  2.  1.  3.  8.  4.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  2. 22.  5.
 22.  6.  4.  3.  3.  6.  7.  6.  9.  3.  8.  2.  1.  5.  8.  4. 10.  3.
  7.  2.  6.  6.  4.  8.  4.  8.  2.  0.  7.  3.  6. 10.  1.  5.  2.  1.
  1.]
Birch %error 25.5 rmsle 0.43445216953629945
<zip object at 0x7f5ccdbfc048>

Ypredict [11.62745098 11.62745098 10.62745098 11.62745098 11.62745098 11.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
PCA %error 22.7 rmsle 0.29103039906541484

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 10.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
  9.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
FastICA %error 22.7 rmsle 0.2907674815926445

Ypredict [11. 13. 15. 12.  6. 11. 11. 14.  9. 10. 10. 10. 11. 13. 14. 12. 16. 12.
 16. 15. 11.  9. 15. 14. 16. 14. 11. 10.  8.  8. 11. 11. 12. 10. 10. 11.
 11.  9.  5.  9.  7.  8. 10. 11. 14. 13. 10. 10.  8. 13. 12.]
Gauss %error 26.4 rmsle 0.32492748817198475

Ypredict [11. 11. 12. 12. 12. 12. 12. 12. 13. 11. 13. 12. 12. 12. 12. 13. 14. 12.
 13. 12. 12. 11. 12. 12. 15. 17. 12. 11. 11. 11. 11. 11. 10. 11. 10. 10.
 11. 10. 10. 10. 10. 10. 10. 11. 10.  9. 10. 10. 10. 11. 14.]
KMeans %error 23.7 rmsle 0.29947584536096505

Ypredict [12. 10. 15. 12. 11. 15. 15. 11. 10.  7. 10. 10. 14. 12. 11. 12. 16. 16.
 19. 13. 11. 12. 13. 12. 10. 11.  6. 10.  7. 10. 10.  8. 10.  8.  9. 14.
  8. 14. 11. 10. 17. 10. 11. 12. 12.  6.  9. 11. 12. 12.  9.]
SparsePCA %error 26.2 rmsle 0.30791999592800995

Ypredict [15. 12. 11. 17. 10. 18. 11. 16.  8.  6. 11. 10. 18. 10. 14. 13. 13. 12.
 14. 10. 10. 11. 14. 11. 10. 17.  8. 13.  6. 16.  9.  4. 12. 10. 19.  9.
  7.  5. 10.  8. 11.  5. 11. 13. 11. 11. 11. 14. 15. 16. 17.]
SparseRP %error 0.0 rmsle 1.1973626668368927e-15

Ypredict [12. 11. 10. 12. 12. 12. 12. 13. 13. 13. 12. 11. 13. 13. 12. 14. 13. 12.
 14. 13. 13. 13. 13. 14. 16. 13.  9.  8.  9.  9. 10. 10. 10. 10. 10.  9.
 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 12. 12. 11. 12. 13.]
Birch %error 22.7 rmsle 0.2894118260852226
<zip object at 0x7f5ccb71cc08>
SVC %error 23.6 rmsle 0.30893639912083243
SVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
kSVC %error 23.6 rmsle 0.30893639912083243
kSVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
KNN %error 0.0 rmsle 0.0
KNN Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
DecisionTree %error 0.0 rmsle 0.0
DecisionTree Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
RandomForestClassifier %error 0.0 rmsle 0.0
RandomForestClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
HuberRegressor %error 0.0 rmsle 2.2766788270036835e-05
HuberRegressor Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Ridge %error 0.0 rmsle 8.509343322242981e-16
Ridge Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Lasso %error 0.4 rmsle 0.0060108424195109015
Lasso Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
LassoCV %error 24.3 rmsle 0.31200318409166783
LassoCV Confusion Matrix
[[ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.00      0.00      0.00        10
        12.0       0.06      1.00      0.11         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.06        51
   macro avg       0.00      0.06      0.01        51
weighted avg       0.00      0.06      0.01        51

--------------------------------------------------------------------------------
Accuracy 5.88 %
Lars %error 19.1 rmsle 0.2583508488153218
Lars Confusion Matrix
[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 3 6 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.19      0.30      0.23        10
        12.0       0.06      0.33      0.10         3
        13.0       0.12      0.25      0.17         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.10        51
   macro avg       0.02      0.06      0.03        51
weighted avg       0.05      0.10      0.06        51

--------------------------------------------------------------------------------
Accuracy 9.8 %
SGDClassifier %error 0.0 rmsle 0.0
SGDClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
RidgeClassifier %error 0.0 rmsle 0.0
RidgeClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]sales-exploration.py:62: RuntimeWarning: invalid value encountered in log1p
  return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
sales-exploration.py:62: RuntimeWarning: invalid value encountered in log1p
  return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20809226236913075, tolerance: 0.0641921568627451
  positive)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
[REPAIR EXEC TIME]: 7.932953119277954Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7fdca749d348>

Traceback (most recent call last):
  File "sales-exploration.py", line 86, in <module>
    res = sm.OLS(Y,Xr).fit()
AttributeError: module 'statsmodels.formula.api' has no attribute 'OLS'
[Try Solution]: OrderedDict([('action', 'fqn'), ('old_fqn', 'statsmodels.formula.api.OLS'), ('new_fqn', 'statsmodels.regression.linear_model.OLS'), ('line_no', 86)])Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f9adebfc348>

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
PCA %error 26.2 rmsle 0.45597248748294594

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
FastICA %error 26.2 rmsle 0.4559724874765883

Ypredict [ 9.  4.  7.  6.  7.  6.  4.  8.  9. 17.  9.  4.  8.  9. 32. 28. 28. 22.
 27.  7.  7.  9.  4. 29. 26. 10. 30. 26. 13. 22.  9.  9.  8. 31. 27. 30.
 27. 32. 26. 29. 30. 24. 31. 26. 21. 33. 31. 29. 28.  7. 16. 26.  4. 27.
 28. 28. 28. 27.  7. 29. 24. 13. 28. 30.  7. 32. 23.  7. 25. 30.  8. 32.
 27.  8. 28. 28.  4. 25. 26. 22.  5.  7. 29. 27. 26. 29. 25. 24. 25. 26.
  8. 29.  8.  6.  8. 32. 30.  3.  9.  9. 29. 27.  5.  5.  6.  8. 14.  2.
  8.  8.  5. 30. 26.  9.  7.  7.  4.  6. 27. 28.  9.  8.  3.  4.  8.  3.
  2. 32. 29. 28. 25. 28. 30. 22. 28. 31. 23. 31. 24. 25. 29. 27. 28.  7.
  7.  6.  8.  3.  9.  2.  5.  8.  7.  7.  4.  4.  7.  4.  4.  7.  6.  7.
  2.  6. 11.  9. 29. 30. 27. 25.  8. 27. 30. 28. 32. 27. 31. 28. 27. 31.
 32. 24. 28. 31. 29. 32. 22. 27. 24. 23. 25. 26. 31. 26.  6. 25.  8.  9.
  2. 13.  4. 14.  5.  1. 10.  3.  4. 19.  7. 11. 10.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  1.  0.  0.  0.  0.  0. 18. 24. 11.  7.  2.  6.  6. 11. 10. 14.
  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 14.  7. 13.  0.  1.
  0.  0.  3.  3.  4.  8.  4.  2.  3.  3.  6.  4.  3.  3.  4.  8.  5.  5.
  5.  4.  9.  3.  5.  4.  3.  9.  4.  3.  3.  3.  5.  4.  5.  4.  4.  6.
  5.  4.  3.  2.  5.  3.  4.  8.  9. 10.  2.  3.  3.  2.  1.  1.  2.  2.
  1.  2.  3.  1.  0.  0.  1.  1.  0.  0.  0.  0.  4.  2.  1.  4.  0.  2.
  1.  0.  9.  4.  3.  2.  5.  3.  2.  2.  3.  1.  2.  0.  0.  1.  1.  0.
  0.  0.  0.  0.  0.  0.  3.  2.  1.  2.  3.  4.  1.  3.  2.  2.  6. 10.
  9.  9.  4.  3. 12.  7. 12. 10. 15.  9. 37. 13. 13.  2.  5.  2.  4.  1.
  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  8.  9.  6.  5.  3.  2.
 10.  8.  4.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.
  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  1.  2.  1.  1.  0.  0.  2.  0.  0.  3.  8. 14.  8. 10.
  5.  6. 14. 10.  8. 11. 10.  9.  4.  3.  7.  5.  5. 10. 10. 11. 10. 11.
 16.  7.  4.  5. 20. 13. 15.  9.  6. 15.  8.  7. 16. 12.  7.  7. 10.  6.
  9. 10.  8.  8.  9. 12.  3.  3. 18.  9. 15.  8. 14. 12.  8. 13. 10.  7.
  9.  9. 16.  8.  7. 25. 24.  8.  7.  6.  4. 13.  9. 12. 20.  9.  8.  6.
  4.  6.  9.  8.  7. 13.  4.  2.  3.  3.  5.  1.  3.  0.  0.  1.  1.  0.
  0.  3.  4.  2.  3.  3.  1.  3.  3.  2.  1.  3.  2.  1.  3.  3.  4.  5.
  1.  9.  2.  3.  1.  1.  1.  1.  1.  2.  0.  1.  2.  4.  3. 13. 16.  4.
 22.  4. 24. 30. 25. 22. 30. 24. 23.  8.  8.  6. 11.  7.  9.  9.  9.  7.
  6.  8.  7.  9.  4.  8.  0. 10.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.
  0.  0.  2.  5.  2.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.
  1.  0.  0.  2.  3.  0.  0.  0.  0.  2.  1.  1.  3.  5.  3.  1.  4.  1.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  2.  0.
  1.  1.  1.  1.  0.  0.  1.  1.  3.  0.  0.  0.  0.  1.  1.  0.  5.  1.
  3.  0.  0.  0.  1.  2.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  2.
  2.  1.  2.  4.  2.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0. 13.  3.
  9.  3.  2.  3.  1.  3.  2.  2.  4.  1.  3.  1.  1.  3.  2.  2.  3.  2.
  4.  0.  2.  2.  3.  4.  1.  2.  1.  1.  2.  1.  3.  5.  1.  2.  0.  0.
  0.]
Gauss %error 39.4 rmsle 0.5535330370990058

Ypredict [ 9.  5.  9.  9.  9.  6.  5.  9. 12. 23. 11.  5. 10. 15. 20. 24. 22. 22.
 21.  9.  9.  9.  4. 21. 22. 12. 21. 23. 14. 23.  8. 10. 14. 27. 24. 17.
 25. 25. 24. 19. 23. 23. 29. 17. 19. 19. 25. 25. 24.  8. 15. 25.  6. 25.
 23. 24. 25. 25.  9. 24. 23. 14. 19. 23.  9. 27. 25. 10. 20. 22.  8. 23.
 22.  9. 21. 20.  5. 23. 21. 28. 12.  8. 24. 21. 22. 20. 21. 26. 20. 22.
 10. 23.  7.  9. 16. 27. 26.  6. 11. 19. 18. 23.  9.  3.  5. 13. 19.  6.
  9.  9.  5. 35. 23.  8. 12.  9.  5. 11. 27. 23.  9.  9.  3.  8. 11.  5.
  4. 22. 19. 21. 21. 21. 21. 27. 22. 23. 22. 26. 25. 25. 26. 26. 25. 11.
 10.  9.  9.  4.  9.  6.  4. 10. 10.  8.  4.  4.  8.  4.  4.  8.  4.  8.
  5. 11.  9.  9. 23. 19. 27. 25.  9. 23. 23. 24. 22. 25. 28. 25. 24. 20.
 22. 24. 23. 24. 24. 21. 27. 28. 23. 24. 21. 26. 19. 22.  6. 27. 12. 14.
  6. 27.  5. 24.  7.  5. 23.  5. 11. 34. 12. 17. 11.  2.  0.  1.  0.  0.
  0.  0.  0.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  2.  1.  0.  0.  0.  0.
  0.  0.  3.  0.  2.  0.  0.  0. 29. 43. 32. 11.  4. 12. 10. 26. 20. 26.
  5.  1.  0.  0.  0.  0.  1.  0.  0.  0.  3.  2.  2. 17. 11. 27.  4.  2.
  1.  0.  5.  4.  6. 12.  7.  5.  6.  5. 10.  4.  6.  4.  5.  9.  7.  4.
  5.  4. 11.  5.  5.  5.  5. 11.  4.  5.  4.  4. 11.  5.  4.  6.  5. 12.
  5.  5.  4.  5.  5.  6.  5.  7. 10. 10.  3.  5.  4.  6.  3.  2.  5.  4.
  4.  4.  5.  1.  1.  1.  1.  0.  0.  0.  0.  0.  6.  4.  3.  8.  3.  5.
  3.  1. 18.  4.  5.  5. 12.  6.  6.  4.  5.  4.  3.  2.  2.  2.  1.  1.
  0.  0.  0.  0.  1.  0.  5.  6.  5.  4.  6.  9.  3.  4.  5.  5. 13. 19.
 22. 26.  4.  5. 22. 13. 27. 22. 22. 20. 71. 25. 26.  5. 13.  6.  4.  3.
  2.  1.  0.  0.  0.  3.  1.  1.  0.  0.  0.  0. 12. 18. 11. 14.  7.  5.
 19. 15.  9.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.
  0.  0.  2.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  1.  0.  0.  5.  3.  2.  3.  3.  2.  3.  2.  1.  6.  9. 23. 15. 13.
  5. 12. 16. 14. 13. 18. 22. 11.  7.  4.  9.  7.  8. 17. 25. 16. 20. 16.
 26.  9.  9. 10. 34. 30. 20. 15. 10. 22. 16. 10. 22. 16. 12. 12. 13. 12.
 16. 26. 12. 17. 15. 22.  6. 12. 31. 13. 27. 16. 24. 19.  9. 20. 13. 16.
 12. 14. 22. 14. 13. 21. 25. 10. 10.  8.  5. 21. 14. 20. 28. 15. 14. 11.
  5.  5. 10. 11. 10. 20.  3.  8.  5.  6.  9.  4.  6.  1.  1.  1.  2.  1.
  0.  4.  5.  4.  5.  4.  4. 11.  5.  5.  4.  4.  5.  3.  5.  5.  5. 11.
  4. 20.  9.  6.  4.  2.  1.  0.  3.  2.  1.  1.  7.  6.  7. 19. 34. 10.
 35.  9. 21. 21. 24. 24. 26. 27. 22.  8.  8.  9. 11.  8. 10. 10.  8. 11.
 10. 11.  8. 14.  6. 21.  0. 26.  4.  6.  0.  0.  0.  0.  0.  0.  1.  1.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  1.  1.  1.  0.  0.  2.
  1.  1.  7. 12.  8.  7.  0.  0.  1.  0.  0.  0.  0.  0.  0.  5.  3.  1.
  1.  1.  1.  5.  5.  1.  1.  1.  2.  5.  3.  2.  6. 10.  5.  2.  6.  4.
  3.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  3.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  3.  6.  2.  1.  0.  1.  3.  2.  0. 10.  3.
  6.  2.  2.  1.  1.  4.  2.  3.  1.  1.  1.  1.  1.  0.  0.  5.  0.  8.
  2.  1.  4.  8.  4.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  1. 24.  5.
 21.  6.  5.  4.  3.  5.  7.  5. 10.  3.  7.  2.  1.  6.  7.  5.  9.  4.
  6.  2.  5.  6.  4.  9.  4.  6.  3.  1.  5.  3.  5. 10.  1.  6.  2.  1.
  1.]
KMeans %error 23.6 rmsle 0.41240303424541275

Ypredict [ 5.  4.  5.  8. 11.  5.  7.  7.  9. 18.  8.  5. 11. 14. 22. 24. 24. 25.
 25.  9.  6.  9.  5. 23. 22.  9. 24. 21. 13. 21.  9. 10. 11. 27. 25. 22.
 28. 27. 29. 17. 23. 21. 32. 23. 21. 20. 27. 23. 25.  7. 11. 26.  6. 28.
 21. 25. 23. 26.  8. 24. 20. 16. 23. 24.  9. 27. 23. 13. 19. 21.  6. 23.
 22. 10. 25. 20.  6. 23. 24. 24. 11. 11. 29. 30. 20. 21. 18. 22. 19. 22.
 11. 24.  6. 10. 15. 31. 25.  5. 14. 20. 22. 20.  9.  3.  5. 11. 17.  4.
 10. 10.  6. 30. 21.  8.  9.  9.  5. 16. 27. 25.  8. 10.  5.  5. 11.  5.
  3. 20. 26. 20. 25. 26. 20. 25. 23. 24. 27. 29. 28. 21. 18. 26. 23. 12.
  6.  9.  7.  3.  6.  7.  3.  9. 11.  8.  6.  5.  7.  4.  5.  7.  5.  8.
  7. 11.  8.  7. 24. 22. 28. 26.  8. 24. 25. 23. 26. 28. 28. 24. 25. 21.
 19. 22. 20. 24. 23. 22. 23. 22. 21. 24. 27. 28. 22. 19.  6. 24. 11. 16.
  5. 24.  6. 22.  5.  3. 23.  5.  9. 40. 11. 13. 10.  2.  1.  2.  0.  0.
  2.  1.  0.  3.  1.  1.  1.  1.  0.  3.  1.  1.  0.  0.  1.  1.  1.  1.
  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  3.  2.  2.  1.  0.  0.  1.  1.
  0.  1.  5.  0.  2.  1.  0.  1. 28. 48. 32. 14.  2. 15. 10. 23. 18. 26.
  5.  2.  2.  1.  1.  1.  1.  0.  0.  1.  2.  3.  3. 16. 13. 26.  5.  2.
  2.  1.  6.  5.  7. 12.  7.  7.  6.  6. 10.  4.  6.  2.  6.  9.  5.  4.
  6.  7. 10.  3.  6.  5.  4. 10.  4.  8.  5.  5. 10.  7.  6.  6.  5. 12.
  5.  7.  4.  7.  6.  7.  5.  8.  9. 10.  4.  4.  4.  6.  3.  2.  4.  4.
  5.  3.  5.  1.  0.  1.  1.  2.  1.  2.  0.  1.  7.  4.  4.  7.  3.  4.
  4.  2. 17.  5.  5.  3.  9.  6.  9.  4.  6.  3.  3.  2.  1.  3.  1.  2.
  1.  0.  0.  0.  1.  1.  5.  7.  5.  5.  6.  7.  4.  6.  6.  4. 13. 16.
 18. 28.  5.  6. 18. 13. 28. 21. 17. 17. 63. 24. 26.  5. 13.  6.  4.  4.
  2.  2.  1.  2.  0.  4. -1.  2.  0.  0.  1.  1. 13. 17.  9. 14.  8.  5.
 17. 16.  8.  2.  2.  1.  0.  0.  1.  1.  2.  2.  2.  2.  2.  2.  2.  1.
  2.  1.  1.  2.  1.  2.  0.  1.  0.  1.  2.  2.  1.  1. -1.  1.  1.  2.
  1.  2.  1.  1.  5.  3.  0.  4.  3.  2.  4.  3.  1.  6.  8. 19. 13. 14.
  5. 13. 17. 15. 12. 16. 22.  9.  6.  5. 12.  5.  5. 17. 25. 15. 19. 16.
 23.  9.  9. 12. 29. 29. 16. 15. 13. 19. 14. 13. 21. 14. 11. 13. 12. 10.
 13. 27. 12. 16. 11. 20.  4. 12. 26. 16. 27. 17. 24. 17.  9. 18. 11. 16.
 12. 12. 23. 12. 10. 21. 23. 10.  8.  5.  4. 20. 14. 22. 21. 14. 12. 12.
  6.  5. 11. 10.  9. 15.  4.  9.  6.  5. 10.  4.  6.  1.  2.  2.  1.  2.
  0.  3.  6.  4.  6.  4.  3. 13.  5.  4.  4.  4.  6.  3.  5.  5.  5.  8.
  6. 19. 10.  7.  4.  2.  2.  1.  4.  4.  1.  3.  6.  8.  6. 17. 32. 11.
 33.  8. 21. 26. 23. 23. 29. 28. 24.  8.  5.  9. 12.  6. 10. 13.  9. 14.
 12. 10.  7.  9.  6. 21.  1. 26.  4.  5.  2.  1.  1.  1.  0.  1.  1.  2.
  1.  1.  1.  1. -1.  1.  1.  1.  1.  0.  2.  2.  1.  2.  0.  0.  1.  3.
  2.  1.  7. 16.  6.  8.  1.  0.  2.  1.  0.  1.  0.  1.  1.  4.  3.  1.
  2.  1.  1.  5.  5.  2.  1.  1.  2.  5.  3.  2.  5. 12.  5.  3.  5.  4.
  3.  0.  2.  1.  1.  1.  1.  0.  1.  1.  0.  2.  0.  1.  1.  2.  5.  1.
  2.  1.  2.  4.  1.  2.  2.  3.  4.  3.  1.  1.  2.  3.  2.  0. 11.  4.
  7.  2.  1.  2.  1.  3.  1.  2.  1.  2.  2.  1.  2.  0.  1.  6.  1. 10.
  2.  2.  3.  9.  5.  1.  0.  2.  2.  1.  0.  1.  1.  1.  2.  2. 22.  4.
 21.  6.  3.  3.  4.  6.  8.  7.  9.  2.  7.  4.  2.  5.  8.  4.  9.  3.
  7.  3.  5.  7.  4.  8.  4.  9.  4.  1.  6.  4.  7.  9.  2.  4.  2.  2.
  1.]
SparsePCA %error 26.7 rmsle 0.5101118378599236

Ypredict [10.  0.  7.  8.  9.  6.  1.  9. 13. 20. 21.  3.  5. 18. 20. 22. 25. 23.
 29.  9.  5.  5.  0. 14. 20. 21. 28. 25. 16. 21.  7.  7. 11. 26. 28. 19.
 29. 24. 25. 22. 27. 16. 27. 16. 20. 14. 20. 27. 23.  9.  7. 20.  9. 26.
 25. 19. 20. 31. 13. 21. 26. 10. 20. 18.  8. 23. 24.  6. 22. 30.  6. 23.
 18. 10. 24. 15.  6. 20. 21. 24. 12.  7. 33. 21. 16. 25. 24. 24. 28. 18.
  9. 19. 10.  9. 14. 21. 25.  5. 11. 21. 15. 29.  6.  4.  4. 21. 20.  4.
 10.  7.  9. 35. 16.  8. 10. 12.  2. 10. 19. 32. 12. 12.  4.  4.  6.  5.
  0. 22. 22. 15. 24. 20. 17. 26. 16. 16. 28. 24. 24. 29. 23. 25. 24. 12.
  7. 13.  3.  7.  5.  3.  4. 11. 10.  6.  5.  4.  5.  5.  7. 11.  6. 13.
  3.  9. 11. 11. 28. 18. 33. 32.  9. 25. 15. 24. 19. 13. 28. 26. 25. 16.
 15. 20. 19. 30. 28. 20. 28. 24. 22. 31. 24. 31. 21. 27.  2. 28. 10. 16.
  2. 26.  4. 23.  7.  4. 17.  5. 12. 45. 15. 14. 14.  1.  1.  0.  0.  1.
  0.  2.  1.  0.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  1.  0.  0.  0.  0.  3.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.
  0.  0.  8.  0.  2.  0.  0.  0. 34. 43. 23.  9.  3. 19. 14. 22. 14. 31.
  5.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0. 18. 15. 29.  1.  2.
  0.  2.  6.  5.  1.  4.  3.  7.  2.  8. 12.  6.  1.  6.  2. 11.  7.  7.
  9.  2.  8.  4.  4.  6.  6. 13.  9.  2.  6.  7.  5. 10.  3.  2.  4.  8.
  6.  1.  1.  5.  3.  6.  6.  9.  9.  9.  6.  4.  3.  5.  2.  1.  8.  6.
  5.  3.  8.  1.  0.  1.  2.  0.  0.  1.  0.  0.  8.  2.  5.  9.  2.  5.
  2.  0. 27.  7.  6.  6. 12.  3.  5.  3.  4.  4.  1.  3.  1.  0.  1.  0.
  2.  0.  0.  0.  2.  0.  9.  4.  5.  2.  9. 10.  5.  6.  3.  5. 11. 18.
 23. 19.  9.  0. 25. 13. 31. 24. 19. 21. 73. 31. 22.  4. 16.  6.  4.  3.
  0.  0.  2.  0.  0.  1.  1.  1.  0.  1.  0.  1. 11. 20. 14. 14. 16.  9.
 28. 14.  9.  0.  1.  0.  0.  0.  0.  0.  0.  0.  3.  4.  2.  3.  0.  2.
  0.  1.  0.  2.  0.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  2.  4.  3.  3.  4.  1.  5.  2.  2.  2.  8. 20. 20.  9.
  4.  6. 21. 10.  9. 15. 24. 11.  4.  4. 12.  6.  8. 15. 21. 12. 11. 13.
 28. 12.  6. 14. 28. 35. 22. 16. 12. 20. 14. 17. 21. 22. 10.  9. 13. 12.
 11. 27. 11. 16. 15. 21.  6. 15. 38. 10. 37. 22. 20. 25.  9. 17. 19. 11.
  9. 10. 22. 15. 14. 30. 18.  6.  7.  8.  3. 16.  6. 29. 30. 21. 14. 15.
  5.  6.  9. 10. 10. 25.  3.  4.  1.  4. 11.  5.  5.  0.  2.  0.  1.  0.
  1.  7.  4.  1.  5.  5.  1. 11.  5.  4.  0.  8.  8.  6.  2.  4.  3.  7.
  4. 21. 16.  6.  3.  1.  0.  2.  4.  2.  1.  5.  9.  5. 10. 15. 30. 12.
 39.  4. 18. 33. 22. 19. 30. 25. 23.  4. 10. 11. 12.  8.  8.  6.  8.  9.
 13.  8.  7. 14.  3. 20.  1. 18.  4.  5.  0.  0.  0.  0.  0.  0.  1.  0.
  1.  0.  0.  0.  1.  2.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  2.
  0.  1.  7. 20.  9. 10.  0.  0.  0.  1.  1.  2.  0.  0.  0.  5.  1.  0.
  4.  0.  1.  4.  7.  0.  0.  1.  2.  3.  1.  5.  6. 17.  8.  1.  2.  5.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2. 12.  4.
  0.  0.  1.  5.  3.  1.  0.  6.  3.  3.  1.  1.  0.  2.  3.  0.  6.  6.
  8.  3.  1.  0.  1.  1.  2.  2.  1.  1.  2.  2.  1.  0.  0.  4.  0. 12.
  4.  2.  1. 11.  8.  1.  0.  0.  0.  0.  0.  0.  2.  1.  0.  0. 30.  3.
 25.  4.  4.  2.  7. 11.  8.  3. 13.  4.  9.  3.  2.  4.  3.  5. 10.  3.
 12.  2.  9.  0.  3.  8.  4. 14.  2.  1.  5.  4.  4. 11.  0.  5.  3.  0.
  1.]
SparseRP %error 0.0 rmsle 0.017457467380136024

Ypredict [ 7.  4.  6.  9. 10.  5.  6.  7. 10. 17.  8.  6. 11. 14. 21. 25. 23. 24.
 23. 10.  5.  9.  5. 23. 21.  9. 24. 21. 13. 21.  9. 10. 13. 29. 24. 22.
 28. 25. 27. 17. 22. 23. 31. 23. 23. 21. 27. 23. 25.  7. 11. 27.  5. 27.
 20. 22. 23. 27.  8. 24. 21. 15. 18. 21. 10. 27. 23. 11. 18. 20.  6. 23.
 22.  9. 24. 20.  6. 20. 24. 25. 12. 10. 27. 27. 21. 20. 18. 22. 20. 22.
 12. 23.  6. 10. 15. 29. 22.  6. 15. 20. 22. 21.  9.  2.  4. 11. 17.  5.
 10.  9.  6. 31. 22.  8.  9.  9.  5. 15. 24. 24.  7. 11.  4.  6. 11.  4.
  3. 22. 26. 20. 24. 27. 21. 24. 25. 26. 27. 29. 26. 23. 22. 26. 23. 13.
  5.  9.  7.  3.  6.  7.  3. 10. 10. 10.  6.  5.  8.  3.  4.  8.  4.  7.
  7. 12.  9.  7. 24. 22. 29. 27.  7. 22. 27. 24. 27. 28. 28. 24. 25. 24.
 19. 21. 20. 26. 24. 21. 24. 23. 21. 25. 25. 28. 21. 17.  6. 24. 11. 16.
  6. 25.  6. 22.  7.  4. 24.  4. 10. 40. 11. 14.  9.  2.  1.  1.  0.  1.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  1.
  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  2.  1.  2.  1.  0.  0.  0.  1.
  0.  0.  4.  0.  3.  1.  0.  0. 30. 49. 33. 14.  2. 16. 10. 22. 19. 25.
  5.  1.  1.  1.  1.  1.  1.  0.  0.  0.  3.  2.  3. 16. 12. 27.  4.  2.
  1.  1.  6.  4.  7. 12.  7.  7.  5.  5. 11.  4.  6.  5.  5. 10.  5.  3.
  6.  6. 10.  4.  5.  6.  5. 10.  5.  6.  6.  5. 10.  6.  5.  6.  5. 14.
  6.  7.  4.  8.  6.  7.  4.  7.  8. 10.  4.  4.  5.  6.  3.  2.  5.  3.
  4.  3.  5.  1.  2.  0.  1.  1.  1.  0.  0.  0.  7.  4.  3.  7.  3.  4.
  3.  1. 18.  4.  5.  4. 11.  4.  8.  4.  5.  3.  3.  2.  1.  2.  1.  1.
  1.  0.  0.  1.  1.  0.  7.  7.  4.  5.  5.  7.  3.  3.  5.  4. 14. 16.
 18. 29.  5.  5. 18. 12. 28. 21. 18. 18. 61. 24. 27.  5. 14.  7.  4.  3.
  2.  2.  1.  0.  0.  3.  1.  1.  1.  0.  1.  1. 14. 17.  9. 14.  7.  5.
 20. 15.  8.  1.  1.  1.  1.  1.  1.  0.  1.  1.  2.  2.  2.  1.  1.  1.
  1.  1.  2.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.
  1.  1.  1.  1.  3.  2.  1.  4.  4.  2.  3.  2.  1.  5.  8. 19. 14. 14.
  5. 13. 17. 15. 12. 17. 22. 11.  7.  4. 12.  5.  6. 17. 25. 14. 19. 18.
 24. 10. 10. 11. 28. 30. 17. 15. 13. 20. 15. 11. 20. 15. 11. 13. 11. 11.
 14. 27. 11. 17. 12. 20.  6. 13. 24. 15. 26. 17. 24. 16.  9. 18. 10. 16.
 12. 12. 22. 13. 10. 21. 22. 10.  8.  6.  5. 19. 12. 20. 23. 14. 12. 11.
  5.  6. 10. 10.  8. 17.  2.  9.  5.  5. 11.  4.  6.  1.  2.  1.  2.  1.
  1.  4.  5.  3.  5.  4.  3. 13.  6.  5.  4.  3.  5.  3.  5.  5.  4.  8.
  5. 20. 11.  6.  4.  2.  1.  0.  3.  3.  1.  2.  7.  8.  8. 17. 34. 11.
 34.  8. 20. 25. 23. 24. 29. 28. 24.  8.  6.  9. 11.  9. 10. 12.  7. 15.
 12. 10.  6. 11.  7. 21.  1. 26.  4.  6.  1.  0.  1.  0.  0.  0.  2.  1.
  1.  0.  1.  0.  1.  1.  0.  1.  0.  0.  2.  1.  1.  1.  0.  0.  0.  2.
  2.  1.  7. 17.  9.  7.  0.  1.  1.  1.  0.  1.  0.  0.  1.  4.  2.  1.
  2.  2.  1.  5.  5.  1.  1.  1.  1.  5.  3.  2.  6. 10.  6.  2.  6.  3.
  3.  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  2.  5.  2.  1.  1.  1.  3.  3.  0. 11.  3.
  7.  2.  2.  1.  1.  3.  1.  2.  1.  2.  1.  1.  1.  0.  0.  5.  0.  9.
  2.  1.  3.  8.  4.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  2. 22.  5.
 22.  6.  4.  3.  3.  6.  7.  6.  9.  3.  8.  2.  1.  5.  8.  4. 10.  3.
  7.  2.  6.  6.  4.  8.  4.  8.  2.  0.  7.  3.  6. 10.  1.  5.  2.  1.
  1.]
Birch %error 25.5 rmsle 0.43445216953912247
<zip object at 0x7f9adebfc048>

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 10.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
PCA %error 22.7 rmsle 0.2908627585359059

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 10.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
  9.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
FastICA %error 22.7 rmsle 0.2907674815926445

Ypredict [11. 12. 13. 12. 11. 13. 16. 13. 11. 13. 12. 13. 13. 11. 16. 13. 13. 11.
 12.  7. 13. 13. 11. 12.  9.  9.  8.  8.  9. 11. 11. 12. 10.  7. 10.  8.
 13.  8.  9. 10.  9. 15. 10. 13. 12. 12. 14. 13. 10.  9. 10.]
Gauss %error 27.1 rmsle 0.3444752941344583

Ypredict [10. 11. 12. 12. 12. 12. 12. 12. 13. 11. 13. 12. 12. 12. 12. 13. 13. 13.
 13. 12. 13. 12. 13. 13. 16. 16. 11. 10. 10. 10. 12. 11. 10. 11. 10.  9.
 10. 10. 11. 10. 10. 11. 11. 11. 10. 10. 11. 11. 11. 12. 13.]
KMeans %error 23.3 rmsle 0.29489684549986206

Ypredict [13. 13. 12. 12. 14. 14. 14. 12. 14. 13. 13. 13. 13. 12. 12. 13. 13. 12.
 13. 13. 12. 10. 14. 12.  7.  8. 10. 13. 10.  7. 10. 11.  8. 11. 13. 11.
  9. 10. 11. 11. 11.  9. 11. 10.  9. 10. 13. 12. 10. 12. 10.]
SparsePCA %error 25.5 rmsle 0.3240501135796634

Ypredict [15. 12. 11. 17. 10. 18. 11. 16.  8.  6. 11. 10. 18. 10. 14. 13. 13. 12.
 14. 10. 10. 11. 14. 11. 10. 17.  8. 13.  6. 16.  9.  4. 12. 10. 19.  9.
  7.  5. 10.  8. 11.  5. 11. 13. 11. 11. 11. 14. 15. 16. 17.]
SparseRP %error 0.0 rmsle 1.1973626668368927e-15

Ypredict [12. 11. 10. 12. 12. 12. 12. 13. 13. 13. 12. 11. 13. 13. 12. 14. 13. 12.
 14. 13. 13. 13. 13. 14. 16. 14.  9.  8.  9.  9. 10. 10. 10. 10. 10.  9.
 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 12. 12. 11. 12. 13.]
Birch %error 22.6 rmsle 0.2892845977072827
<zip object at 0x7f9adc71bd88>
SVC %error 23.6 rmsle 0.30893639912083243
SVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
kSVC %error 23.6 rmsle 0.30893639912083243
kSVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
KNN %error 0.0 rmsle 0.0
KNN Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
DecisionTree %error 0.0 rmsle 0.0
DecisionTree Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
RandomForestClassifier %error 0.0 rmsle 0.0
RandomForestClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
HuberRegressor %error 0.0 rmsle 2.2766788270036835e-05
HuberRegressor Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Ridge %error 0.0 rmsle 8.509343322242981e-16
Ridge Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Lasso %error 0.4 rmsle 0.0060108424195109015
Lasso Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
LassoCV %error 24.3 rmsle 0.31200318409166783
LassoCV Confusion Matrix
[[ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.00      0.00      0.00        10
        12.0       0.06      1.00      0.11         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.06        51
   macro avg       0.00      0.06      0.01        51
weighted avg       0.00      0.06      0.01        51

--------------------------------------------------------------------------------
Accuracy 5.88 %
Lars %error 19.1 rmsle 0.2583508488153218
Lars Confusion Matrix
[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 3 6 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.19      0.30      0.23        10
        12.0       0.06      0.33      0.10         3
        13.0       0.12      0.25      0.17         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.10        51
   macro avg       0.02      0.06      0.03        51
weighted avg       0.05      0.10      0.06        51

--------------------------------------------------------------------------------
Accuracy 9.8 %
SGDClassifier %error 3.5 rmsle 0.18752609281447993
SGDClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       0.40      1.00      0.57         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       0.00      0.00      0.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           0.94        51
   macro avg       0.90      0.94      0.91        51
weighted avg       0.92      0.94      0.92        51

--------------------------------------------------------------------------------
Accuracy 94.12 %
RidgeClassifier %error 0.0 rmsle 0.0
RidgeClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]sales-exploration.py:62: RuntimeWarning: invalid value encountered in log1p
  return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20809226236913075, tolerance: 0.0641921568627451
  positive)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
[REPAIR EXEC TIME]: 7.972841262817383Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f58909293c8>

Traceback (most recent call last):
  File "sales-exploration.py", line 86, in <module>
    res = sm.OLS(Y,Xr).fit()
AttributeError: module 'statsmodels.formula.api' has no attribute 'OLS'
[Try Solution]: OrderedDict([('action', 'fqn'), ('old_fqn', 'statsmodels.formula.api.OLS'), ('new_fqn', 'statsmodels.regression.linear_model.OLS'), ('line_no', 86)])Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f22e36873c8>

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
PCA %error 26.2 rmsle 0.4559724874763852

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
FastICA %error 26.2 rmsle 0.455972487476588

Ypredict [ 9.  3.  4.  5. 13.  3.  3.  4.  8. 14. 10.  2.  6. 10. 33. 23. 25. 30.
 27. 11.  6.  6.  3. 23. 22.  7. 22. 22. 11. 33.  6.  8. 13. 32. 20. 28.
 30. 27. 22. 19. 24. 26. 30. 25. 19. 27. 26. 28. 22.  7. 16. 24.  5. 35.
 31. 31. 25. 27.  8. 29. 32. 24. 30. 25.  8. 32. 24.  9. 28. 26.  8. 29.
 19.  6. 24. 26.  5. 24. 23. 17. 10.  9. 26. 24. 29. 25. 24. 27. 25. 24.
 10. 32.  7.  9. 13. 30. 25.  2. 11. 10. 30. 25.  9.  2.  2.  8. 16.  1.
  9.  7.  2. 28. 16.  7.  9. 11.  4.  8. 25. 26.  5.  5.  3.  4.  7.  4.
  2. 18. 29. 27. 27. 26. 27. 28. 32. 34. 30. 29. 32. 28. 25. 28. 20. 10.
  6. 11.  4.  7.  4.  7.  4.  4.  7.  8.  4.  4.  9.  4.  4. 10.  5.  4.
  6.  9.  8. 10. 23. 27. 22. 24.  6. 32. 26. 23. 31. 26. 24. 35. 37. 25.
 23. 18. 26. 21. 27. 31. 24. 25. 20. 28. 29. 33. 29. 30.  4. 31.  5. 11.
  2. 12.  5. 18.  2.  2. 12.  3.  6. 33.  8. 13.  6.  1.  0.  0.  0.  1.
  0.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.
  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  0.  0.
  0.  0.  1.  0.  1.  0.  0.  0. 18. 38. 12. 10. -1.  9.  6. 13.  8. 18.
  5.  1.  1.  1.  1.  0.  0.  0.  0.  0.  1.  1.  2. 11.  7. 21.  2.  2.
  1.  0.  5.  6.  5. 10.  5.  3.  3.  5. 10.  5.  4.  3.  4.  5.  3.  3.
  3.  2.  7.  4.  2.  7.  5.  9.  2.  4.  7.  3.  9.  7.  4.  3.  3.  8.
  5.  5.  5.  5.  3.  2.  4.  6.  5.  9.  4.  1.  4.  2.  0.  1.  5.  2.
  3.  2.  3.  1.  0.  1.  1.  0.  1.  0.  1.  1.  2.  3.  2.  2.  1.  3.
  1.  1. 10.  4.  5.  2.  2.  2.  4.  1.  2.  2.  1.  0.  2.  1.  1.  0.
  0.  0.  0.  0.  1.  1.  5.  5.  4.  4.  4.  3.  3.  5.  4. -1. 11.  9.
 10. 16.  2.  2. 14.  8. 14. 13. 11. 11. 35. 14. 19.  3. 11.  3.  2.  1.
  1.  1.  1.  0.  0.  3.  1.  1.  1.  0.  0.  0. 10.  8.  3. 11.  5.  3.
 14.  8.  6.  1.  1.  0.  0.  0.  1.  0.  0.  1.  1.  1.  2.  1.  1.  0.
  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  3.  1.  0.  1.  1.  1.  1.  1.  1.  6.  6. 18. 10. 11.
  4.  4. 20. 11.  8.  9. 13.  9.  3.  4. 11.  4.  7. 14. 11. 13. 13. 11.
 14.  3.  5.  9. 27. 19. 10. 10. 14. 15. 13.  7. 20.  9.  8.  9. 10.  7.
  8. 15. 10.  7. 11. 14.  3.  7. 28.  6. 14. 15. 24. 10. 11. 16.  9. 10.
 12.  6. 22. 13.  9. 23. 20. 10. 11.  9.  4. 21. 12. 18. 15. 10. 11. 10.
  4.  3.  8. 10.  6. 10.  4.  3.  3.  1.  7.  2.  3.  1.  0.  1.  1.  1.
  1.  4.  5.  1.  5.  3.  2.  7.  1.  2.  3.  3.  3.  1.  2.  6.  3.  6.
  3. 11.  4.  5.  1.  1.  0.  1.  2.  0.  0.  0.  4.  4.  2. 13. 13.  8.
 21.  3. 27. 29. 28. 22. 27. 35. 29.  8.  6. 10.  4.  8.  6. 11. 11.  8.
 11.  8.  7. 11.  4. 11.  1. 12.  3.  4.  1.  0.  1.  0.  0.  0.  0.  1.
  0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.
  1.  0.  5.  8.  5.  5.  0.  0.  1.  0.  0.  0.  0.  0.  1.  3.  1.  0.
  0.  0.  0.  3.  3.  0.  0.  1.  0.  1.  2.  0.  2.  7.  7.  0.  2.  1.
  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  2.  2.
  0.  0.  1.  3.  1.  0.  2.  1.  2.  1.  1.  1.  1.  1.  2.  1.  6.  1.
  3.  1.  0.  1.  1.  3.  1.  2.  0.  0.  1.  1.  1.  0.  0.  2.  0.  4.
  1.  1.  3.  3.  2.  0.  0.  0.  0.  0.  0.  1.  1.  1.  2.  0. 16.  5.
 10.  5.  1.  3.  1.  4.  3.  3.  8.  1.  4.  2.  1.  2.  2.  2.  6.  2.
  0.  1.  3.  4.  2.  5.  2.  5.  1.  0.  4.  1.  2.  7.  1.  4.  1.  0.
  0.]
Gauss %error 36.6 rmsle 0.5550368841172608

Ypredict [ 8.  5.  8.  8.  9.  7.  5. 10. 13. 24. 12.  5. 10. 16. 21. 23. 24. 21.
 22.  8.  8.  8.  4. 21. 24. 13. 23. 22. 15. 20.  8.  9. 15. 25. 26. 18.
 25. 27. 26. 16. 22. 24. 26. 22. 21. 18. 25. 22. 24.  8. 17. 24.  6. 26.
 25. 22. 24. 23.  8. 22. 20. 16. 20. 23.  9. 27. 22. 10. 19. 19.  8. 22.
 22. 10. 24. 18.  5. 23. 20. 23. 12.  8. 25. 24. 24. 23. 21. 23. 20. 22.
 10. 20.  7.  9. 17. 28. 25.  6. 11. 19. 17. 23.  8.  3.  5. 12. 18.  5.
 10.  9.  5. 33. 24.  9. 12. 10.  5. 11. 25. 24.  9.  9.  4.  7. 10.  5.
  4. 20. 19. 22. 22. 23. 21. 24. 22. 25. 25. 28. 25. 23. 22. 26. 23. 11.
 10.  8.  8.  4.  8.  6.  4. 11.  9.  9.  4.  4.  7.  4.  4.  8.  4.  8.
  5. 11.  9. 10. 25. 18. 30. 32.  9. 22. 22. 25. 24. 24. 29. 25. 23. 20.
 21. 24. 21. 23. 30. 24. 23. 25. 23. 22. 21. 29. 19. 21.  6. 26. 11. 15.
  6. 26.  5. 25.  7.  5. 22.  5. 11. 34. 12. 17. 12.  2.  0.  1.  0.  0.
  0.  0.  0.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  2.  1.  0.  0.  0.  0.
  0.  0.  3.  0.  2.  0.  0.  0. 30. 43. 31. 12.  4. 12. 10. 28. 20. 27.
  5.  1.  0.  0.  0.  0.  1.  0.  0.  0.  3.  2.  2. 19. 11. 30.  4.  2.
  1.  0.  5.  4.  6. 12.  7.  6.  6.  5.  9.  4.  6.  4.  5.  9.  7.  4.
  5.  4. 11.  5.  5.  6.  5. 10.  5.  5.  4.  3. 11.  5.  5.  5.  5. 12.
  4.  5.  4.  6.  5.  6.  5.  7.  9. 12.  3.  5.  5.  6.  3.  2.  5.  4.
  4.  4.  5.  1.  1.  1.  1.  0.  0.  0.  0.  0.  6.  4.  3.  8.  3.  4.
  3.  1. 18.  4.  4.  5. 12.  6.  6.  4.  5.  3.  3.  2.  2.  2.  1.  1.
  0.  0.  0.  0.  1.  0.  5.  6.  5.  4.  6.  9.  3.  4.  5.  4. 13. 19.
 20. 26.  5.  5. 22. 12. 28. 21. 22. 19. 71. 25. 26.  5. 14.  6.  4.  3.
  2.  1.  0.  0.  0.  3.  1.  1.  0.  0.  0.  0. 12. 18. 11. 14.  7.  5.
 20. 14.  9.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.
  0.  0.  2.  1.  0.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.
  0.  1.  0.  0.  4.  3.  2.  3.  3.  2.  3.  2.  1.  6.  9. 20. 15. 14.
  5. 12. 18. 15. 14. 17. 21. 11.  7.  3.  8.  7.  7. 18. 24. 16. 20. 15.
 26. 10.  8. 11. 33. 31. 18. 15. 10. 22. 16. 11. 20. 16. 11. 13. 13. 12.
 16. 25. 11. 16. 15. 20.  6. 11. 30. 14. 29. 17. 25. 19. 11. 19. 13. 15.
 13. 14. 22. 15. 13. 21. 23. 10.  9.  7.  5. 21. 15. 21. 28. 15. 14. 11.
  5.  5. 10. 11.  9. 18.  3.  7.  5.  6.  9.  3.  6.  1.  1.  1.  2.  1.
  0.  4.  5.  4.  5.  4.  4. 11.  5.  5.  4.  5.  5.  3.  5.  6.  5. 11.
  4. 21.  9.  7.  4.  2.  1.  0.  3.  2.  1.  1.  7.  6.  6. 19. 35. 10.
 37.  9. 23. 24. 24. 23. 25. 28. 24.  7.  8.  9. 11.  8.  9.  9.  9. 13.
 11. 12.  8. 14.  6. 21.  0. 26.  4.  6.  0.  0.  0.  0.  0.  0.  1.  1.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  1.  1.  1.  0.  0.  2.
  1.  1.  7. 12.  8.  7.  0.  0.  1.  0.  0.  0.  0.  0.  0.  5.  3.  1.
  1.  1.  1.  5.  5.  1.  1.  1.  2.  5.  3.  2.  6. 10.  6.  2.  6.  4.
  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  3.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  3.  6.  2.  1.  0.  1.  3.  2.  0. 10.  3.
  6.  2.  2.  1.  1.  4.  2.  3.  1.  1.  1.  1.  1.  0.  0.  5.  0.  8.
  2.  1.  4.  7.  4.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  1. 23.  5.
 19.  6.  5.  4.  3.  5.  7.  5. 10.  3.  7.  2.  1.  6.  7.  5.  9.  4.
  6.  2.  5.  6.  4.  9.  4.  7.  3.  1.  6.  3.  5. 10.  1.  6.  2.  1.
  1.]
KMeans %error 23.5 rmsle 0.4119938585158334

Ypredict [ 9.  5.  6.  8. 10.  6.  6.  8. 10. 17.  7.  6.  8. 12. 22. 22. 24. 25.
 22.  9.  4.  7.  5. 25. 23.  7. 23. 22. 12. 24.  7.  8. 12. 28. 26. 24.
 27. 24. 27. 19. 21. 21. 30. 25. 20. 23. 28. 24. 24.  6. 10. 25.  5. 26.
 20. 20. 23. 28.  8. 25. 23. 15. 18. 25. 10. 28. 24. 12. 16. 21.  8. 25.
 24.  9. 23. 21.  5. 20. 22. 23. 12.  9. 26. 29. 22. 18. 18. 23. 21. 21.
 11. 25.  6. 10. 15. 31. 23.  5. 14. 21. 24. 23.  8.  3.  3. 10. 16.  6.
  8. 10.  7. 29. 21.  7.  7.  8.  4. 16. 22. 24.  8.  9.  3.  6. 10.  4.
  1. 21. 27. 22. 22. 28. 22. 24. 27. 23. 26. 29. 27. 24. 22. 24. 22. 14.
  7.  8.  6.  2.  5.  6.  3.  8. 10.  9.  5.  5.  8.  3.  4.  7.  4.  8.
  7. 12.  8.  8. 23. 23. 29. 26.  8. 22. 28. 24. 24. 27. 25. 26. 25. 24.
 21. 22. 21. 24. 27. 19. 26. 23. 21. 26. 27. 30. 22. 17.  5. 21. 10. 16.
  6. 23.  5. 21.  8.  4. 25.  4. 10. 40. 11. 14.  9.  2.  1.  2.  0.  1.
  0.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  0. -1.  1.  1.  0.  0.  1.
  1.  0.  1.  2.  1.  2.  2.  1.  1.  1.  2.  2.  2.  2.  2.  1.  0.  0.
  0.  0.  4.  0.  4.  0.  0.  0. 28. 48. 33. 14.  3. 13. 10. 23. 20. 25.
  6.  2.  1.  1.  1.  1.  1.  1.  0.  1.  3.  2.  3. 14. 13. 27.  5.  2.
  1.  0.  6.  5.  6. 13.  7.  7.  5.  6. 10.  3.  6.  4.  4.  9.  6.  4.
  6.  5.  9.  4.  4.  6.  5. 10.  6.  5.  5.  4.  9.  6.  5.  6.  5. 14.
  4.  7.  5.  7.  6.  7.  4.  7.  8. 11.  4.  5.  5.  6.  2.  1.  5.  3.
  5.  3.  5.  2.  2.  1.  1.  0.  1.  1.  0.  1.  6.  4.  3.  6.  3.  3.
  3.  1. 18.  4.  4.  4. 10.  5.  8.  3.  5.  3.  2.  2.  2.  2.  2.  2.
  1.  0.  1.  1.  0.  0.  5.  7.  5.  5.  6.  7.  3.  3.  6.  4. 15. 16.
 19. 30.  4.  5. 17. 13. 28. 21. 17. 19. 59. 23. 27.  5. 14.  7.  4.  3.
  3.  3.  1.  0.  2.  3.  1.  1.  1.  0.  2.  1. 14. 17. 11. 14. 10.  5.
 18. 17.  9.  1.  1.  2.  0.  1.  1.  2.  1.  2.  2.  1.  2.  1.  1.  2.
  1.  1.  2.  1.  1.  2.  1.  0.  1.  1.  0.  2.  2.  1.  0.  1.  1.  2.
  1.  2.  1.  1.  4.  2.  2.  4.  4.  3.  4.  3.  1.  4.  8. 19. 15. 13.
  5. 12. 17. 14. 12. 18. 21. 10.  7.  4. 11.  5.  5. 17. 25. 14. 19. 15.
 26. 12.  9. 11. 27. 29. 16. 16. 12. 20. 15. 12. 21. 13. 11. 13. 11. 11.
 13. 27. 10. 17. 12. 19.  6. 13. 22. 16. 26. 17. 24. 15. 10. 17. 11. 17.
 12. 10. 22. 12. 12. 23. 24. 10.  8.  6.  4. 18. 12. 22. 24. 14. 11. 13.
  5.  4. 10. 11.  8. 15.  3.  8.  6.  6. 11.  4.  7.  1.  2.  0.  2.  2.
  0.  4.  6.  3.  4.  4.  4. 12.  6.  4.  5.  5.  6.  3.  5.  6.  5. 10.
  5. 20. 10.  6.  4.  1.  1. -1.  3.  3.  2.  3.  8.  7.  8. 17. 34. 11.
 33.  7. 23. 25. 24. 25. 27. 25. 23.  8.  5.  8.  9.  8.  8. 10.  6. 16.
 13.  9.  6. 12.  5. 21.  0. 26.  4.  6.  2.  1.  1.  1.  2.  0.  2.  2.
  1.  1.  1.  0.  0.  1.  0.  1.  0.  1.  2.  2.  2.  2.  0.  0.  0.  4.
  1.  1.  7. 15.  8.  8.  0.  1.  1.  1.  0.  1.  1.  1.  0.  4.  2.  2.
  1.  3.  2.  5.  5.  1.  1.  1.  2.  4.  3.  2.  6. 11.  6.  3.  3.  4.
  2.  1.  2.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  1.  1.  2.  7.  2.
  1. -1.  2.  4.  2.  0.  2.  3.  5.  3.  2.  1.  2.  4.  2.  0. 10.  3.
  6.  2.  3.  1.  1.  2.  1.  3.  2.  2.  2.  2.  1.  1.  1.  6.  0.  9.
  1.  1.  2.  8.  5.  0.  1.  0.  2.  1.  1.  1.  2.  0.  2.  2. 22.  5.
 20.  6.  4.  3.  3.  6.  7.  6. 10.  2.  8.  3.  1.  6.  8.  4. 10.  3.
  7.  2.  6.  6.  5.  8.  4.  9.  3.  1.  6.  3.  7. 10.  1.  5.  2.  2.
  1.]
SparsePCA %error 26.5 rmsle 0.4879709863113386

Ypredict [10.  0.  7.  8.  9.  6.  1.  9. 13. 20. 21.  3.  5. 18. 20. 22. 25. 23.
 29.  9.  5.  5.  0. 14. 20. 21. 28. 25. 16. 21.  7.  7. 11. 26. 28. 19.
 29. 24. 25. 22. 27. 16. 27. 16. 20. 14. 20. 27. 23.  9.  7. 20.  9. 26.
 25. 19. 20. 31. 13. 21. 26. 10. 20. 18.  8. 23. 24.  6. 22. 30.  6. 23.
 18. 10. 24. 15.  6. 20. 21. 24. 12.  7. 33. 21. 16. 25. 24. 24. 28. 18.
  9. 19. 10.  9. 14. 21. 25.  5. 11. 21. 15. 29.  6.  4.  4. 21. 20.  4.
 10.  7.  9. 35. 16.  8. 10. 12.  2. 10. 19. 32. 12. 12.  4.  4.  6.  5.
  0. 22. 22. 15. 24. 20. 17. 26. 16. 16. 28. 24. 24. 29. 23. 25. 24. 12.
  7. 13.  3.  7.  5.  3.  4. 11. 10.  6.  5.  4.  5.  5.  7. 11.  6. 13.
  3.  9. 11. 11. 28. 18. 33. 32.  9. 25. 15. 24. 19. 13. 28. 26. 25. 16.
 15. 20. 19. 30. 28. 20. 28. 24. 22. 31. 24. 31. 21. 27.  2. 28. 10. 16.
  2. 26.  4. 23.  7.  4. 17.  5. 12. 45. 15. 14. 14.  1.  1.  0.  0.  1.
  0.  2.  1.  0.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  1.  0.  0.  0.  0.  3.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.
  0.  0.  8.  0.  2.  0.  0.  0. 34. 43. 23.  9.  3. 19. 14. 22. 14. 31.
  5.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0. 18. 15. 29.  1.  2.
  0.  2.  6.  5.  1.  4.  3.  7.  2.  8. 12.  6.  1.  6.  2. 11.  7.  7.
  9.  2.  8.  4.  4.  6.  6. 13.  9.  2.  6.  7.  5. 10.  3.  2.  4.  8.
  6.  1.  1.  5.  3.  6.  6.  9.  9.  9.  6.  4.  3.  5.  2.  1.  8.  6.
  5.  3.  8.  1.  0.  1.  2.  0.  0.  1.  0.  0.  8.  2.  5.  9.  2.  5.
  2.  0. 27.  7.  6.  6. 12.  3.  5.  3.  4.  4.  1.  3.  1.  0.  1.  0.
  2.  0.  0.  0.  2.  0.  9.  4.  5.  2.  9. 10.  5.  6.  3.  5. 11. 18.
 23. 19.  9.  0. 25. 13. 31. 24. 19. 21. 73. 31. 22.  4. 16.  6.  4.  3.
  0.  0.  2.  0.  0.  1.  1.  1.  0.  1.  0.  1. 11. 20. 14. 14. 16.  9.
 28. 14.  9.  0.  1.  0.  0.  0.  0.  0.  0.  0.  3.  4.  2.  3.  0.  2.
  0.  1.  0.  2.  0.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  2.  4.  3.  3.  4.  1.  5.  2.  2.  2.  8. 20. 20.  9.
  4.  6. 21. 10.  9. 15. 24. 11.  4.  4. 12.  6.  8. 15. 21. 12. 11. 13.
 28. 12.  6. 14. 28. 35. 22. 16. 12. 20. 14. 17. 21. 22. 10.  9. 13. 12.
 11. 27. 11. 16. 15. 21.  6. 15. 38. 10. 37. 22. 20. 25.  9. 17. 19. 11.
  9. 10. 22. 15. 14. 30. 18.  6.  7.  8.  3. 16.  6. 29. 30. 21. 14. 15.
  5.  6.  9. 10. 10. 25.  3.  4.  1.  4. 11.  5.  5.  0.  2.  0.  1.  0.
  1.  7.  4.  1.  5.  5.  1. 11.  5.  4.  0.  8.  8.  6.  2.  4.  3.  7.
  4. 21. 16.  6.  3.  1.  0.  2.  4.  2.  1.  5.  9.  5. 10. 15. 30. 12.
 39.  4. 18. 33. 22. 19. 30. 25. 23.  4. 10. 11. 12.  8.  8.  6.  8.  9.
 13.  8.  7. 14.  3. 20.  1. 18.  4.  5.  0.  0.  0.  0.  0.  0.  1.  0.
  1.  0.  0.  0.  1.  2.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  2.
  0.  1.  7. 20.  9. 10.  0.  0.  0.  1.  1.  2.  0.  0.  0.  5.  1.  0.
  4.  0.  1.  4.  7.  0.  0.  1.  2.  3.  1.  5.  6. 17.  8.  1.  2.  5.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2. 12.  4.
  0.  0.  1.  5.  3.  1.  0.  6.  3.  3.  1.  1.  0.  2.  3.  0.  6.  6.
  8.  3.  1.  0.  1.  1.  2.  2.  1.  1.  2.  2.  1.  0.  0.  4.  0. 12.
  4.  2.  1. 11.  8.  1.  0.  0.  0.  0.  0.  0.  2.  1.  0.  0. 30.  3.
 25.  4.  4.  2.  7. 11.  8.  3. 13.  4.  9.  3.  2.  4.  3.  5. 10.  3.
 12.  2.  9.  0.  3.  8.  4. 14.  2.  1.  5.  4.  4. 11.  0.  5.  3.  0.
  1.]
SparseRP %error 0.0 rmsle 0.017457467380136024

Ypredict [ 7.  4.  6.  9. 10.  5.  6.  7. 10. 17.  8.  6. 11. 14. 21. 25. 23. 24.
 23. 10.  5.  9.  5. 23. 21.  9. 24. 21. 13. 21.  9. 10. 13. 29. 24. 22.
 28. 25. 27. 17. 22. 23. 31. 23. 23. 21. 27. 23. 25.  7. 11. 27.  5. 27.
 20. 22. 23. 27.  8. 24. 21. 15. 18. 21. 10. 27. 23. 11. 18. 20.  6. 23.
 22.  9. 24. 20.  6. 20. 24. 25. 12. 10. 27. 27. 21. 20. 18. 22. 20. 22.
 12. 23.  6. 10. 15. 29. 22.  6. 15. 20. 22. 21.  9.  2.  4. 11. 17.  5.
 10.  9.  6. 31. 22.  8.  9.  9.  5. 15. 24. 24.  7. 11.  4.  6. 11.  4.
  3. 22. 26. 20. 24. 27. 21. 24. 25. 26. 27. 29. 26. 23. 22. 26. 23. 13.
  5.  9.  7.  3.  6.  7.  3. 10. 10. 10.  6.  5.  8.  3.  4.  8.  4.  7.
  7. 12.  9.  7. 24. 22. 29. 27.  7. 22. 27. 24. 27. 28. 28. 24. 25. 24.
 19. 21. 20. 26. 24. 21. 24. 23. 21. 25. 25. 28. 21. 17.  6. 24. 11. 16.
  6. 25.  6. 22.  7.  4. 24.  4. 10. 40. 11. 14.  9.  2.  1.  1.  0.  1.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  1.
  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  2.  1.  2.  1.  0.  0.  0.  1.
  0.  0.  4.  0.  3.  1.  0.  0. 30. 49. 33. 14.  2. 16. 10. 22. 19. 25.
  5.  1.  1.  1.  1.  1.  1.  0.  0.  0.  3.  2.  3. 16. 12. 27.  4.  2.
  1.  1.  6.  4.  7. 12.  7.  7.  5.  5. 11.  4.  6.  5.  5. 10.  5.  3.
  6.  6. 10.  4.  5.  6.  5. 10.  5.  6.  6.  5. 10.  6.  5.  6.  5. 14.
  6.  7.  4.  8.  6.  7.  4.  7.  8. 10.  4.  4.  5.  6.  3.  2.  5.  3.
  4.  3.  5.  1.  2.  0.  1.  1.  1.  0.  0.  0.  7.  4.  3.  7.  3.  4.
  3.  1. 18.  4.  5.  4. 11.  4.  8.  4.  5.  3.  3.  2.  1.  2.  1.  1.
  1.  0.  0.  1.  1.  0.  7.  7.  4.  5.  5.  7.  3.  3.  5.  4. 14. 16.
 18. 29.  5.  5. 18. 12. 28. 21. 18. 18. 61. 24. 27.  5. 14.  7.  4.  3.
  2.  2.  1.  0.  0.  3.  1.  1.  1.  0.  1.  1. 14. 17.  9. 14.  7.  5.
 20. 15.  8.  1.  1.  1.  1.  1.  1.  0.  1.  1.  2.  2.  2.  1.  1.  1.
  1.  1.  2.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.
  1.  1.  1.  1.  3.  2.  1.  4.  4.  2.  3.  2.  1.  5.  8. 19. 14. 14.
  5. 13. 17. 15. 12. 17. 22. 11.  7.  4. 12.  5.  6. 17. 25. 14. 19. 18.
 24. 10. 10. 11. 28. 30. 17. 15. 13. 20. 15. 11. 20. 15. 11. 13. 11. 11.
 14. 27. 11. 17. 12. 20.  6. 13. 24. 15. 26. 17. 24. 16.  9. 18. 10. 16.
 12. 12. 22. 13. 10. 21. 22. 10.  8.  6.  5. 19. 12. 20. 23. 14. 12. 11.
  5.  6. 10. 10.  8. 17.  2.  9.  5.  5. 11.  4.  6.  1.  2.  1.  2.  1.
  1.  4.  5.  3.  5.  4.  3. 13.  6.  5.  4.  3.  5.  3.  5.  5.  4.  8.
  5. 20. 11.  6.  4.  2.  1.  0.  3.  3.  1.  2.  7.  8.  8. 17. 34. 11.
 34.  8. 20. 25. 23. 24. 29. 28. 24.  8.  6.  9. 11.  9. 10. 12.  7. 15.
 12. 10.  6. 11.  7. 21.  1. 26.  4.  6.  1.  0.  1.  0.  0.  0.  2.  1.
  1.  0.  1.  0.  1.  1.  0.  1.  0.  0.  2.  1.  1.  1.  0.  0.  0.  2.
  2.  1.  7. 17.  9.  7.  0.  1.  1.  1.  0.  1.  0.  0.  1.  4.  2.  1.
  2.  2.  1.  5.  5.  1.  1.  1.  1.  5.  3.  2.  6. 10.  6.  2.  6.  3.
  3.  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  2.  5.  2.  1.  1.  1.  3.  3.  0. 11.  3.
  7.  2.  2.  1.  1.  3.  1.  2.  1.  2.  1.  1.  1.  0.  0.  5.  0.  9.
  2.  1.  3.  8.  4.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  2. 22.  5.
 22.  6.  4.  3.  3.  6.  7.  6.  9.  3.  8.  2.  1.  5.  8.  4. 10.  3.
  7.  2.  6.  6.  4.  8.  4.  8.  2.  0.  7.  3.  6. 10.  1.  5.  2.  1.
  1.]
Birch %error 25.5 rmsle 0.4344521695460364
<zip object at 0x7f22e36870c8>

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 11.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
PCA %error 22.7 rmsle 0.29070459179360425

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 10.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
  9.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
FastICA %error 22.7 rmsle 0.29076748159264454

Ypredict [12. 10.  9. 10. 11.  8. 13. 12. 13. 15. 11. 14. 11. 12. 15. 11. 14. 14.
 11. 12. 14. 13. 15. 14. 14. 15. 11. 10.  9.  7.  7.  7. 10.  9. 13.  6.
 10. 11. 12. 11.  8. 12. 10. 12. 12.  9. 12. 11. 13. 11. 12.]
Gauss %error 27.3 rmsle 0.33061211056057177

Ypredict [11. 11. 12. 13. 12. 12. 13. 13. 13. 12. 13. 12. 12. 12. 12. 13. 14. 11.
 13. 12. 12. 10. 11. 12. 16. 17. 12. 11. 10. 11. 11. 11. 10. 11. 10. 10.
 11. 10. 10. 10. 10. 10.  9. 10. 10.  9. 10. 10. 10. 12. 14.]
KMeans %error 23.5 rmsle 0.29585414083689465

Ypredict [ 5. 11.  5. 11. 13.  4. 16.  6. 12.  9.  3. 10.  9. 14. 11. 16.  8. 13.
 11. 16. 11. 10. 15. 14. 16. 10.  6.  9. 11. 11.  8.  8. 10. 10.  8.  8.
 14. 12.  8. 12.  4. 16. 10.  7. 10.  8.  8. 12. 14.  5. 12.]
SparsePCA %error 38.4 rmsle 0.5106207891584381

Ypredict [15. 12. 11. 17. 10. 18. 11. 16.  8.  6. 11. 10. 18. 10. 14. 13. 13. 12.
 14. 10. 10. 11. 14. 11. 10. 17.  8. 13.  6. 16.  9.  4. 12. 10. 19.  9.
  7.  5. 10.  8. 11.  5. 11. 13. 11. 11. 11. 14. 15. 16. 17.]
SparseRP %error 0.0 rmsle 1.1973626668368927e-15

Ypredict [12. 11. 11. 12. 12. 12. 12. 13. 13. 13. 12. 11. 13. 13. 12. 13. 13. 12.
 14. 13. 13. 13. 13. 14. 16. 13.  9.  8.  9.  9. 10. 10. 10. 10. 10.  9.
 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 12. 12. 11. 12. 13.]
Birch %error 22.7 rmsle 0.28958786271420917
<zip object at 0x7f22e11a4708>
SVC %error 23.6 rmsle 0.30893639912083243
SVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
kSVC %error 23.6 rmsle 0.30893639912083243
kSVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
KNN %error 0.0 rmsle 0.0
KNN Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
DecisionTree %error 0.0 rmsle 0.0
DecisionTree Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
RandomForestClassifier %error 0.0 rmsle 0.0
RandomForestClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
HuberRegressor %error 0.0 rmsle 2.2766788270036835e-05
HuberRegressor Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Ridge %error 0.0 rmsle 8.509343322242981e-16
Ridge Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Lasso %error 0.4 rmsle 0.0060108424195109015
Lasso Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
LassoCV %error 24.3 rmsle 0.31200318409166783
LassoCV Confusion Matrix
[[ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.00      0.00      0.00        10
        12.0       0.06      1.00      0.11         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.06        51
   macro avg       0.00      0.06      0.01        51
weighted avg       0.00      0.06      0.01        51

--------------------------------------------------------------------------------
Accuracy 5.88 %
Lars %error 19.1 rmsle 0.2583508488153218
Lars Confusion Matrix
[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 3 6 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.19      0.30      0.23        10
        12.0       0.06      0.33      0.10         3
        13.0       0.12      0.25      0.17         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.10        51
   macro avg       0.02      0.06      0.03        51
weighted avg       0.05      0.10      0.06        51

--------------------------------------------------------------------------------
Accuracy 9.8 %
SGDClassifier %error 4.7 rmsle 0.14509131309949
SGDClassifier Confusion Matrix
[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 4 0 3 0 0 0 0 0 2]
 [0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       0.67      1.00      0.80         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      0.40      0.57        10
        12.0       1.00      1.00      1.00         3
        13.0       0.57      1.00      0.73         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       0.33      1.00      0.50         1

    accuracy                           0.88        51
   macro avg       0.91      0.96      0.91        51
weighted avg       0.94      0.88      0.88        51

--------------------------------------------------------------------------------
Accuracy 88.24 %
RidgeClassifier %error 0.0 rmsle 0.0
RidgeClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20809226236913075, tolerance: 0.0641921568627451
  positive)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
[REPAIR EXEC TIME]: 8.302767753601074