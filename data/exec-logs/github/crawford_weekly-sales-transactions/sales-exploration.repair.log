Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f0a78c0a348>

Traceback (most recent call last):
  File "sales-exploration.py", line 86, in <module>
    res = sm.OLS(Y,Xr).fit()
AttributeError: module 'statsmodels.formula.api' has no attribute 'OLS'
[Try Solution]: OrderedDict([('action', 'fqn'), ('old_fqn', 'statsmodels.formula.api.OLS'), ('new_fqn', 'statsmodels.regression.linear_model.OLS'), ('line_no', 86)])Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f5b40d4a348>

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
PCA %error 26.2 rmsle 0.45597248747321945

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
FastICA %error 26.2 rmsle 0.45597248747658803

Ypredict [ 5.  2.  8.  6.  6.  3.  4.  7.  6. 15.  6.  4.  7. 10. 24. 38. 28. 29.
 20.  5.  8.  4.  4. 40. 30.  5. 22. 29.  6. 22. 10.  9. 10. 34. 28. 25.
 27. 27. 33. 29. 28. 23. 19. 26. 21. 21. 32. 31. 26.  6. 13. 22.  4. 25.
 23. 21. 20. 33.  6. 14. 22. 12. 30. 29.  9. 18. 28.  9. 21. 18.  7. 22.
 35.  9. 31. 25.  3. 28. 24. 32. 12.  6. 30. 22. 26. 30. 26. 24. 25. 25.
 10. 26.  7.  9. 12. 31. 30.  3.  8. 12. 25. 24.  7.  2.  2. 12. 17.  3.
  7.  7.  3. 28. 26. 10.  5.  7.  4.  8. 34. 28. 10. 11.  2.  7.  9.  4.
 -1. 28. 31. 29. 29. 24. 26. 27. 27. 26. 35. 24. 25. 28. 26. 21. 32.  7.
  7.  8.  6.  0.  8.  6.  4.  7. 10.  5.  2.  5.  7.  2.  3.  6.  1.  5.
  5.  8.  7.  6. 25. 27. 30. 32.  5. 34. 31. 27. 26. 29. 26. 27. 32. 28.
 27. 25. 25. 29. 28. 20. 29. 20. 24. 31. 31. 30. 27. 22.  3. 26. 12. 13.
  5. 16.  3. 13.  6.  2. 16.  2.  6. 25.  9. 10.  8.  1.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.
  0.  0.  1.  0.  1.  0.  0.  0. 19. 32. 23.  9.  1. 10. 10. 12. 15. 17.
  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 14. 10. 17.  2.  2.
  0.  0.  3.  3.  4. 11.  3.  3.  3.  3.  7.  1.  2.  2.  2.  8.  5.  4.
  3.  3.  9.  4.  6.  4.  4.  9.  3.  5.  4.  2. 10.  5.  2.  4.  6. 10.
  4.  4.  6.  6.  3.  5.  4.  7. 11.  2.  3.  3.  3.  2.  1.  1.  4.  2.
  3.  2.  2.  0.  1.  0.  0.  0.  0.  0.  0.  0.  4.  2.  2.  5.  1.  3.
  2.  1. 11.  2.  1.  2.  5.  5.  4.  2.  2.  3.  0.  1.  1.  1.  0.  1.
  0.  0.  0.  0.  0.  0.  1.  6.  3.  2.  3.  4.  1.  1.  2.  4. 13. 14.
 17. 17.  3.  3. 15.  9. 24. 14. 19. 14. 42. 15. 16.  3.  8.  3.  3.  1.
  1.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0. 10. 11.  5. 10.  6.  2.
 16.  9.  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  1.
  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  2.  1.  1.  1.  1.  1.  1.  1.  0.  3.  8. 18. 11. 10.
  6.  9. 10. 10.  9. 13. 14.  4.  7.  3.  6.  6.  4.  8. 17. 11. 12.  8.
 19.  7.  5.  7. 22. 20. 12. 13.  6. 19. 12.  9. 17. 12.  7.  7.  8. 10.
 14. 17.  9. 12. 14. 14.  4. 12. 24. 11. 18. 14. 18. 14.  6. 17. 12. 13.
  8. 12. 14.  8. 12. 25. 27. 10. 10.  7.  5. 14. 11.  9. 19. 11. 11. 11.
  4.  3.  5.  7.  5. 14.  3.  5.  2.  2.  5.  3.  2.  0.  1.  0.  1.  0.
  0.  2.  3.  4.  1.  2.  2.  7.  3.  2.  2.  2.  3.  1.  4.  3.  2.  7.
  2. 14.  5.  2.  2.  1.  1.  0.  1.  0.  0.  0.  4.  4.  5. 12. 20.  6.
 21.  6. 26. 20. 23. 22. 27. 27. 28.  7.  5.  9.  7.  6. 10.  4.  1.  9.
  8. 10. 10.  7.  2. 11.  0. 19.  2.  4.  0.  0.  0.  0.  0.  0.  1.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.
  0.  0.  2.  9.  5.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  1.  0.
  0.  0.  1.  1.  2.  0.  0.  0.  0.  4.  0.  1.  4.  6.  4.  1.  2.  2.
  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  3.  0.
  0.  0.  0.  1.  1.  1.  1.  2.  4.  2.  0.  0.  0.  1.  0.  0.  6.  2.
  6.  1.  1.  1.  0.  2.  1.  2.  1.  0.  0.  0.  0.  0.  0.  3.  0.  5.
  0. -1.  1.  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 14.  4.
 17.  4.  1.  3.  1.  3.  5.  3.  5.  3.  4.  0.  0.  5.  4.  2.  5.  0.
  4.  1.  1.  3.  3.  5.  2.  3.  0. -1.  4.  2.  3.  6.  0.  4.  0.  1.
  1.]
Gauss %error 36.0 rmsle 0.5511162017511801

Ypredict [ 8.  5.  8.  9. 10.  6.  6.  9. 11. 23. 12.  6. 12. 16. 21. 21. 24. 24.
 22.  8.  8. 10.  5. 23. 21. 11. 22. 22. 16. 25.  8.  8. 15. 26. 24. 21.
 27. 22. 26. 20. 26. 24. 26. 23. 23. 20. 24. 21. 22.  7. 17. 24.  6. 26.
 24. 22. 25. 22.  9. 22. 23. 16. 16. 21. 10. 25. 22.  9. 18. 21.  8. 23.
 21. 10. 22. 22.  5. 23. 23. 27. 11.  9. 25. 24. 23. 21. 22. 24. 20. 22.
 11. 20.  8. 10. 17. 27. 24.  6. 12. 19. 21. 23.  9.  4.  5. 11. 17.  6.
  9.  9.  5. 36. 24.  8. 11. 10.  5. 12. 24. 20.  9.  9.  5.  6. 11.  5.
  4. 23. 23. 16. 21. 22. 20. 25. 22. 21. 24. 27. 26. 22. 23. 30. 23. 11.
  8.  7.  7.  4.  8.  6.  5. 10.  9. 10.  5.  5.  7.  4.  5. 10.  5.  7.
  6. 13.  9. 10. 23. 18. 29. 25.  7. 23. 21. 26. 24. 23. 29. 25. 23. 22.
 22. 24. 21. 23. 26. 21. 25. 28. 23. 23. 23. 26. 20. 21.  5. 23. 11. 14.
  6. 26.  6. 24.  7.  5. 23.  5.  8. 35. 12. 17. 12.  1.  0.  1.  0.  0.
  1.  0.  0.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.
  0.  0.  3.  0.  2.  0.  0.  0. 28. 39. 31. 12.  4. 13. 10. 28. 20. 28.
  6.  1.  0.  0.  0.  0.  1.  0.  0.  0.  3.  2.  2. 19. 11. 29.  3.  2.
  1.  0.  6.  4.  6. 11.  6.  6.  5.  5.  9.  5.  6.  5.  5.  9.  6.  5.
  5.  5. 10.  5.  5.  6.  6. 10.  6.  5.  5.  5. 10.  5.  6.  5.  5. 12.
  5.  5.  5.  6.  6.  6.  5.  8.  9. 11.  4.  5.  6.  6.  2.  1.  5.  4.
  5.  4.  5.  1.  1.  1.  1.  1.  0.  0.  0.  0.  6.  5.  3.  6.  2.  5.
  3.  1. 18.  5.  5.  5.  9.  6.  6.  5.  5.  3.  3.  1.  1.  2.  1.  1.
  0.  0.  0.  0.  1.  0.  5.  6.  5.  5.  5.  7.  3.  5.  5.  5. 13. 18.
 20. 27.  5.  6. 23. 12. 28. 21. 21. 19. 72. 25. 28.  5. 15.  6.  5.  3.
  1.  1.  1.  0.  0.  2.  1.  1.  0.  0.  0.  0. 12. 18.  9. 13.  7.  5.
 20. 14.  8.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.
  0.  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.
  0.  1.  0.  0.  5.  2.  2.  3.  2.  1.  3.  2.  1.  6.  9. 19. 16. 12.
  5. 11. 18. 15. 12. 17. 20. 11.  6.  5. 10.  6.  6. 18. 24. 14. 21. 16.
 27. 10.  7. 10. 34. 31. 17. 14. 11. 21. 15. 12. 19. 16. 11. 12. 14. 11.
 16. 26. 12. 17. 16. 20.  5. 12. 31. 13. 28. 17. 24. 20. 10. 19. 13. 16.
 12. 14. 23. 15. 11. 22. 23.  9.  9.  7.  5. 20. 13. 22. 30. 16. 14. 11.
  6.  6. 11. 10.  9. 17.  4.  7.  5.  6.  9.  3.  6.  1.  1.  1.  2.  1.
  1.  5.  5.  4.  5.  5.  3.  9.  6.  5.  4.  6.  5.  3.  5.  6.  5.  8.
  4. 21.  8.  6.  4.  2.  1.  0.  2.  2.  1.  1.  6.  6.  6. 18. 35.  7.
 36.  6. 23. 25. 24. 23. 30. 28. 23.  8.  9. 10. 11.  9. 11. 10.  9. 14.
 11. 11.  8. 14.  6. 22.  0. 27.  4.  6.  1.  0.  0.  0.  0.  0.  1.  1.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  1.  1.  1.  0.  0.  2.
  1.  1.  6. 12.  7.  7.  0.  0.  1.  0.  0.  0.  0.  0.  0.  5.  2.  1.
  1.  1.  1.  5.  5.  1.  1.  1.  1.  5.  3.  2.  6.  8.  6.  2.  6.  3.
  2.  0.  1.  0.  0.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  1.  2.  5.  2.  1.  1.  1.  3.  2.  1.  8.  3.
  6.  2.  1.  1.  1.  4.  2.  2.  1.  1.  1.  0.  1.  0.  0.  5.  0.  7.
  2.  1.  3.  7.  4.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  1. 23.  5.
 19.  6.  5.  5.  3.  6.  7.  5.  8.  3.  7.  2.  1.  6.  7.  5.  8.  3.
  6.  2.  6.  6.  5.  7.  4.  7.  2.  1.  6.  3.  6.  9.  1.  6.  1.  1.
  1.]
KMeans %error 24.0 rmsle 0.4107815784773896

Ypredict [ 7.  4.  7.  8. 11.  5.  6.  7. 11. 18.  8.  6.  9. 13. 23. 26. 24. 23.
 24.  9.  5.  7.  5. 23. 24. 10. 26. 23. 13. 20.  8. 10. 12. 28. 26. 24.
 27. 27. 28. 17. 23. 21. 32. 22. 23. 19. 28. 23. 26.  6. 10. 27.  5. 27.
 21. 24. 22. 27.  8. 25. 21. 15. 19. 23. 10. 28. 24. 12. 19. 21.  7. 22.
 20.  9. 26. 18.  6. 22. 25. 23. 11.  9. 27. 27. 20. 21. 19. 23. 20. 21.
 11. 25.  6. 10. 15. 31. 24.  5. 14. 20. 20. 22.  9.  3.  5. 11. 16.  5.
 10. 11.  6. 30. 21.  8.  9.  9.  5. 15. 25. 25.  8. 11.  4.  6. 12.  5.
  2. 22. 26. 20. 25. 27. 21. 26. 24. 24. 27. 30. 26. 22. 20. 25. 22. 12.
  6.  9.  7.  2.  6.  7.  3. 10. 11.  9.  6.  4.  9.  4.  4.  7.  4.  9.
  7. 11.  8.  7. 21. 23. 28. 26.  7. 22. 25. 22. 28. 28. 26. 26. 25. 21.
 20. 22. 20. 25. 26. 21. 24. 19. 21. 25. 26. 27. 24. 20.  6. 22. 11. 16.
  5. 23.  5. 21.  7.  3. 24.  4. 10. 39. 11. 13.  9.  2.  1.  2.  1.  0.
  1.  1.  1.  1.  1.  0.  1.  2.  1.  1.  1.  1.  0. -1.  1.  0.  0.  2.
  1.  1.  1.  2.  1.  2.  2.  2. -1.  1.  3.  2.  2.  1.  1.  2.  0.  1.
  0.  0.  5.  1.  3.  0.  0.  0. 28. 48. 33. 14.  2. 14. 11. 22. 19. 27.
  6.  1.  0.  1.  1.  1.  1.  1.  1.  2.  2.  3.  3. 15. 13. 25.  5.  3.
  1.  0.  6.  5.  7. 13.  6.  8.  6.  6. 10.  4.  6.  3.  4.  9.  7.  4.
  8.  6.  9.  4.  5.  5.  4.  9.  5.  6.  5.  4. 10.  5.  5.  7.  5. 13.
  6.  6.  4.  6.  5.  8.  4.  7.  8. 10.  3.  4.  4.  6.  3.  2.  6.  3.
  4.  3.  6.  2.  2.  1.  2.  1.  1.  0.  0.  0.  8.  4.  4.  7.  3.  5.
  2.  3. 17.  4.  4.  3. 10.  6.  8.  3.  5.  3.  3.  3.  3.  3.  0.  1.
  1.  1.  0.  0.  1.  0.  6.  8.  4.  5.  6.  7.  4.  4.  6.  3. 14. 16.
 19. 29.  5.  5. 18. 13. 28. 21. 18. 18. 62. 24. 27.  5. 12.  6.  4.  3.
  2.  3.  0.  0.  1.  4.  0.  1.  0.  0.  1.  1. 12. 16.  9. 14. 10.  5.
 18. 17.  8.  2.  2.  1.  0. -1.  1.  0.  1.  2.  2.  2.  2.  2.  2.  1.
  2.  1.  2.  1.  1.  2.  1.  2.  0.  1.  2.  1.  2.  1.  0.  0.  0.  1.
  1.  1.  1.  1.  5.  2.  0.  4.  4.  3.  4.  3.  2.  6.  9. 19. 13. 14.
  5. 12. 16. 14. 13. 16. 22.  9.  7.  3. 10.  6.  6. 17. 25. 15. 18. 16.
 25.  9.  9. 12. 29. 29. 14. 15. 12. 20. 15. 11. 20. 14. 12. 13. 11. 12.
 13. 27. 10. 16. 11. 19.  6. 12. 23. 16. 27. 17. 25. 15.  9. 16. 12. 16.
 12. 11. 22. 11. 11. 20. 23. 11.  8.  6.  4. 20. 12. 21. 21. 13. 11. 13.
  6.  5. 10. 12.  8. 16.  3.  9.  6.  5. 10.  4.  7.  2.  3.  1.  2.  2.
  1.  4.  6.  4.  6.  4.  3. 13.  6.  5.  4.  4.  6.  4.  5.  5.  5.  9.
  6. 18. 11.  7.  4.  2.  2.  1.  3.  3.  1.  3.  6.  8.  7. 18. 33. 12.
 35.  8. 21. 25. 23. 23. 29. 28. 24.  8.  6.  9. 10.  7.  9. 12.  7. 14.
 11.  9.  7. 10.  6. 20.  1. 26.  5.  7.  2.  0.  1.  1.  0.  0.  2.  2.
  2.  1.  1.  1.  1.  2.  0.  1.  1.  0.  2.  1.  2.  2.  2.  1.  1.  2.
  1.  1.  8. 17.  8.  8.  1. -1.  2.  1.  2.  1.  1.  1.  1.  4.  3.  2.
  1.  2.  1.  5.  5.  2.  1.  1.  3.  4.  3.  2.  6. 12.  6.  2.  5.  5.
  3.  1.  0.  1.  1.  1.  1.  2.  1.  1.  1.  1.  0.  1.  0.  3.  6.  2.
  2.  1.  1.  5.  2.  2.  3.  3.  5.  3.  2.  1.  2.  3.  2.  0. 11.  3.
  7.  2.  1.  3.  2.  2.  1.  3.  3.  0.  2.  1.  1.  1.  0.  7.  1.  9.
  3.  1.  3.  8.  5.  2.  1.  2.  1.  1.  0.  1.  1.  1.  2.  2. 24.  4.
 21.  6.  4.  3.  4.  6.  7.  6.  9.  2.  8.  3.  1.  6.  8.  4.  9.  3.
  8.  3.  6.  6.  5.  8.  5.  9.  3.  1.  6.  4.  5. 10.  2.  5.  3.  3.
  1.]
SparsePCA %error 26.4 rmsle 0.4999635944126745

Ypredict [10.  0.  7.  8.  9.  6.  1.  9. 13. 20. 21.  3.  5. 18. 20. 22. 25. 23.
 29.  9.  5.  5.  0. 14. 20. 21. 28. 25. 16. 21.  7.  7. 11. 26. 28. 19.
 29. 24. 25. 22. 27. 16. 27. 16. 20. 14. 20. 27. 23.  9.  7. 20.  9. 26.
 25. 19. 20. 31. 13. 21. 26. 10. 20. 18.  8. 23. 24.  6. 22. 30.  6. 23.
 18. 10. 24. 15.  6. 20. 21. 24. 12.  7. 33. 21. 16. 25. 24. 24. 28. 18.
  9. 19. 10.  9. 14. 21. 25.  5. 11. 21. 15. 29.  6.  4.  4. 21. 20.  4.
 10.  7.  9. 35. 16.  8. 10. 12.  2. 10. 19. 32. 12. 12.  4.  4.  6.  5.
  0. 22. 22. 15. 24. 20. 17. 26. 16. 16. 28. 24. 24. 29. 23. 25. 24. 12.
  7. 13.  3.  7.  5.  3.  4. 11. 10.  6.  5.  4.  5.  5.  7. 11.  6. 13.
  3.  9. 11. 11. 28. 18. 33. 32.  9. 25. 15. 24. 19. 13. 28. 26. 25. 16.
 15. 20. 19. 30. 28. 20. 28. 24. 22. 31. 24. 31. 21. 27.  2. 28. 10. 16.
  2. 26.  4. 23.  7.  4. 17.  5. 12. 45. 15. 14. 14.  1.  1.  0.  0.  1.
  0.  2.  1.  0.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  1.  0.  0.  0.  0.  3.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.
  0.  0.  8.  0.  2.  0.  0.  0. 34. 43. 23.  9.  3. 19. 14. 22. 14. 31.
  5.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0. 18. 15. 29.  1.  2.
  0.  2.  6.  5.  1.  4.  3.  7.  2.  8. 12.  6.  1.  6.  2. 11.  7.  7.
  9.  2.  8.  4.  4.  6.  6. 13.  9.  2.  6.  7.  5. 10.  3.  2.  4.  8.
  6.  1.  1.  5.  3.  6.  6.  9.  9.  9.  6.  4.  3.  5.  2.  1.  8.  6.
  5.  3.  8.  1.  0.  1.  2.  0.  0.  1.  0.  0.  8.  2.  5.  9.  2.  5.
  2.  0. 27.  7.  6.  6. 12.  3.  5.  3.  4.  4.  1.  3.  1.  0.  1.  0.
  2.  0.  0.  0.  2.  0.  9.  4.  5.  2.  9. 10.  5.  6.  3.  5. 11. 18.
 23. 19.  9.  0. 25. 13. 31. 24. 19. 21. 73. 31. 22.  4. 16.  6.  4.  3.
  0.  0.  2.  0.  0.  1.  1.  1.  0.  1.  0.  1. 11. 20. 14. 14. 16.  9.
 28. 14.  9.  0.  1.  0.  0.  0.  0.  0.  0.  0.  3.  4.  2.  3.  0.  2.
  0.  1.  0.  2.  0.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  2.  4.  3.  3.  4.  1.  5.  2.  2.  2.  8. 20. 20.  9.
  4.  6. 21. 10.  9. 15. 24. 11.  4.  4. 12.  6.  8. 15. 21. 12. 11. 13.
 28. 12.  6. 14. 28. 35. 22. 16. 12. 20. 14. 17. 21. 22. 10.  9. 13. 12.
 11. 27. 11. 16. 15. 21.  6. 15. 38. 10. 37. 22. 20. 25.  9. 17. 19. 11.
  9. 10. 22. 15. 14. 30. 18.  6.  7.  8.  3. 16.  6. 29. 30. 21. 14. 15.
  5.  6.  9. 10. 10. 25.  3.  4.  1.  4. 11.  5.  5.  0.  2.  0.  1.  0.
  1.  7.  4.  1.  5.  5.  1. 11.  5.  4.  0.  8.  8.  6.  2.  4.  3.  7.
  4. 21. 16.  6.  3.  1.  0.  2.  4.  2.  1.  5.  9.  5. 10. 15. 30. 12.
 39.  4. 18. 33. 22. 19. 30. 25. 23.  4. 10. 11. 12.  8.  8.  6.  8.  9.
 13.  8.  7. 14.  3. 20.  1. 18.  4.  5.  0.  0.  0.  0.  0.  0.  1.  0.
  1.  0.  0.  0.  1.  2.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  2.
  0.  1.  7. 20.  9. 10.  0.  0.  0.  1.  1.  2.  0.  0.  0.  5.  1.  0.
  4.  0.  1.  4.  7.  0.  0.  1.  2.  3.  1.  5.  6. 17.  8.  1.  2.  5.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2. 12.  4.
  0.  0.  1.  5.  3.  1.  0.  6.  3.  3.  1.  1.  0.  2.  3.  0.  6.  6.
  8.  3.  1.  0.  1.  1.  2.  2.  1.  1.  2.  2.  1.  0.  0.  4.  0. 12.
  4.  2.  1. 11.  8.  1.  0.  0.  0.  0.  0.  0.  2.  1.  0.  0. 30.  3.
 25.  4.  4.  2.  7. 11.  8.  3. 13.  4.  9.  3.  2.  4.  3.  5. 10.  3.
 12.  2.  9.  0.  3.  8.  4. 14.  2.  1.  5.  4.  4. 11.  0.  5.  3.  0.
  1.]
SparseRP %error 0.0 rmsle 0.017457467380136024

Ypredict [ 7.  4.  6.  9. 10.  5.  6.  7. 10. 17.  8.  6. 11. 14. 21. 25. 23. 24.
 23. 10.  5.  9.  5. 23. 21.  9. 24. 21. 13. 21.  9. 10. 13. 29. 24. 22.
 28. 25. 27. 17. 22. 23. 31. 23. 23. 21. 27. 23. 25.  7. 11. 27.  5. 27.
 20. 22. 23. 27.  8. 24. 21. 15. 18. 21. 10. 27. 23. 11. 18. 20.  6. 23.
 22.  9. 24. 20.  6. 20. 24. 25. 12. 10. 27. 27. 21. 20. 18. 22. 20. 22.
 12. 23.  6. 10. 15. 29. 22.  6. 15. 20. 22. 21.  9.  2.  4. 11. 17.  5.
 10.  9.  6. 31. 22.  8.  9.  9.  5. 15. 24. 24.  7. 11.  4.  6. 11.  4.
  3. 22. 26. 20. 24. 27. 21. 24. 25. 26. 27. 29. 26. 23. 22. 26. 23. 13.
  5.  9.  7.  3.  6.  7.  3. 10. 10. 10.  6.  5.  8.  3.  4.  8.  4.  7.
  7. 12.  9.  7. 24. 22. 29. 27.  7. 22. 27. 24. 27. 28. 28. 24. 25. 24.
 19. 21. 20. 26. 24. 21. 24. 23. 21. 25. 25. 28. 21. 17.  6. 24. 11. 16.
  6. 25.  6. 22.  7.  4. 24.  4. 10. 40. 11. 14.  9.  2.  1.  1.  0.  1.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  1.
  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  2.  1.  2.  1.  0.  0.  0.  1.
  0.  0.  4.  0.  3.  1.  0.  0. 30. 49. 33. 14.  2. 16. 10. 22. 19. 25.
  5.  1.  1.  1.  1.  1.  1.  0.  0.  0.  3.  2.  3. 16. 12. 27.  4.  2.
  1.  1.  6.  4.  7. 12.  7.  7.  5.  5. 11.  4.  6.  5.  5. 10.  5.  3.
  6.  6. 10.  4.  5.  6.  5. 10.  5.  6.  6.  5. 10.  6.  5.  6.  5. 14.
  6.  7.  4.  8.  6.  7.  4.  7.  8. 10.  4.  4.  5.  6.  3.  2.  5.  3.
  4.  3.  5.  1.  2.  0.  1.  1.  1.  0.  0.  0.  7.  4.  3.  7.  3.  4.
  3.  1. 18.  4.  5.  4. 11.  4.  8.  4.  5.  3.  3.  2.  1.  2.  1.  1.
  1.  0.  0.  1.  1.  0.  7.  7.  4.  5.  5.  7.  3.  3.  5.  4. 14. 16.
 18. 29.  5.  5. 18. 12. 28. 21. 18. 18. 61. 24. 27.  5. 14.  7.  4.  3.
  2.  2.  1.  0.  0.  3.  1.  1.  1.  0.  1.  1. 14. 17.  9. 14.  7.  5.
 20. 15.  8.  1.  1.  1.  1.  1.  1.  0.  1.  1.  2.  2.  2.  1.  1.  1.
  1.  1.  2.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.
  1.  1.  1.  1.  3.  2.  1.  4.  4.  2.  3.  2.  1.  5.  8. 19. 14. 14.
  5. 13. 17. 15. 12. 17. 22. 11.  7.  4. 12.  5.  6. 17. 25. 14. 19. 18.
 24. 10. 10. 11. 28. 30. 17. 15. 13. 20. 15. 11. 20. 15. 11. 13. 11. 11.
 14. 27. 11. 17. 12. 20.  6. 13. 24. 15. 26. 17. 24. 16.  9. 18. 10. 16.
 12. 12. 22. 13. 10. 21. 22. 10.  8.  6.  5. 19. 12. 20. 23. 14. 12. 11.
  5.  6. 10. 10.  8. 17.  2.  9.  5.  5. 11.  4.  6.  1.  2.  1.  2.  1.
  1.  4.  5.  3.  5.  4.  3. 13.  6.  5.  4.  3.  5.  3.  5.  5.  4.  8.
  5. 20. 11.  6.  4.  2.  1.  0.  3.  3.  1.  2.  7.  8.  8. 17. 34. 11.
 34.  8. 20. 25. 23. 24. 29. 28. 24.  8.  6.  9. 11.  9. 10. 12.  7. 15.
 12. 10.  6. 11.  7. 21.  1. 26.  4.  6.  1.  0.  1.  0.  0.  0.  2.  1.
  1.  0.  1.  0.  1.  1.  0.  1.  0.  0.  2.  1.  1.  1.  0.  0.  0.  2.
  2.  1.  7. 17.  9.  7.  0.  1.  1.  1.  0.  1.  0.  0.  1.  4.  2.  1.
  2.  2.  1.  5.  5.  1.  1.  1.  1.  5.  3.  2.  6. 10.  6.  2.  6.  3.
  3.  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  2.  5.  2.  1.  1.  1.  3.  3.  0. 11.  3.
  7.  2.  2.  1.  1.  3.  1.  2.  1.  2.  1.  1.  1.  0.  0.  5.  0.  9.
  2.  1.  3.  8.  4.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  2. 22.  5.
 22.  6.  4.  3.  3.  6.  7.  6.  9.  3.  8.  2.  1.  5.  8.  4. 10.  3.
  7.  2.  6.  6.  4.  8.  4.  8.  2.  0.  7.  3.  6. 10.  1.  5.  2.  1.
  1.]
Birch %error 25.5 rmsle 0.43445216954172244
<zip object at 0x7f5b40d4a048>

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 11.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 11.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
  9.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
PCA %error 22.7 rmsle 0.29073469758421766

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 10.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
  9.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
FastICA %error 22.7 rmsle 0.2907674815926445

Ypredict [ 6.  2. 12.  9. 19.  6. 11. 12.  6.  3.  4. 11.  5. -8.  6. 19.  0.  2.
 11. 12. 13. 17.  6. -1.  8. 11.  7.  1.  4.  6.  2. -1.  3. 14. 18.  8.
  4.  6. -3. 13. 11. -1.  4. -1. -3.  8. 12.  4. 16.  5. 18.]
Gauss %error 53.2 rmsle 1.1388543707591672

Ypredict [11. 11. 12. 12. 12. 12. 12. 12. 13. 11. 12. 12. 12. 12. 12. 13. 13. 13.
 13. 12. 13. 12. 13. 13. 16. 16. 11. 10. 10. 10. 11. 10. 10. 10. 10. 11.
 10. 10. 11. 10. 11. 11. 11. 12. 10. 10. 11. 11. 11. 12. 14.]
KMeans %error 22.9 rmsle 0.2945371544200728

Ypredict [ 9. 14. 12.  9.  9.  9. 13.  9. 13. 12.  7. 13. 16. 11.  7. 19. 11. 10.
 12. 18. 14. 12. 16. 10. 14. 13.  5.  8.  8.  8. 11.  5.  9.  6. 13. 10.
 10.  9. 12.  7. 15. 13. 10. 12. 11.  8. 10. 15. 11. 11. 10.]
SparsePCA %error 30.5 rmsle 0.3531692848591213

Ypredict [15. 12. 11. 17. 10. 18. 11. 16.  8.  6. 11. 10. 18. 10. 14. 13. 13. 12.
 14. 10. 10. 11. 14. 11. 10. 17.  8. 13.  6. 16.  9.  4. 12. 10. 19.  9.
  7.  5. 10.  8. 11.  5. 11. 13. 11. 11. 11. 14. 15. 16. 17.]
SparseRP %error 0.0 rmsle 1.1973626668368927e-15

Ypredict [12. 11. 10. 12. 12. 11. 13. 13. 13. 13. 12. 11. 13. 13. 12. 14. 13. 12.
 14. 13. 12. 13. 13. 14. 16. 14.  9.  8.  9.  9. 11. 10. 10. 10. 11. 10.
 10. 10. 10. 11. 10. 11. 11. 11. 11. 11. 12. 12. 11. 12. 13.]
Birch %error 22.7 rmsle 0.2881449610148202
<zip object at 0x7f5b3dfdc448>
SVC %error 23.6 rmsle 0.30893639912083243
SVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
kSVC %error 23.6 rmsle 0.30893639912083243
kSVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
KNN %error 0.0 rmsle 0.0
KNN Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
DecisionTree %error 0.0 rmsle 0.0
DecisionTree Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
RandomForestClassifier %error 0.0 rmsle 0.0
RandomForestClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
HuberRegressor %error 0.0 rmsle 2.2766788270036835e-05
HuberRegressor Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Ridge %error 0.0 rmsle 8.509343322242981e-16
Ridge Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Lasso %error 0.4 rmsle 0.0060108424195109015
Lasso Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
LassoCV %error 24.3 rmsle 0.31200318409166783
LassoCV Confusion Matrix
[[ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.00      0.00      0.00        10
        12.0       0.06      1.00      0.11         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.06        51
   macro avg       0.00      0.06      0.01        51
weighted avg       0.00      0.06      0.01        51

--------------------------------------------------------------------------------
Accuracy 5.88 %
Lars %error 19.1 rmsle 0.2583508488153218
Lars Confusion Matrix
[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 3 6 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.19      0.30      0.23        10
        12.0       0.06      0.33      0.10         3
        13.0       0.12      0.25      0.17         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.10        51
   macro avg       0.02      0.06      0.03        51
weighted avg       0.05      0.10      0.06        51

--------------------------------------------------------------------------------
Accuracy 9.8 %
SGDClassifier %error 4.2 rmsle 0.22291195338030775
SGDClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       0.00      0.00      0.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       0.75      1.00      0.86         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       0.50      1.00      0.67         1

    accuracy                           0.96        51
   macro avg       0.89      0.94      0.91        51
weighted avg       0.94      0.96      0.95        51

--------------------------------------------------------------------------------
Accuracy 96.08 %
RidgeClassifier %error 0.0 rmsle 0.0
RidgeClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
sales-exploration.py:113: RuntimeWarning: invalid value encountered in log1p
  return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20809226236913075, tolerance: 0.0641921568627451
  positive)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
[REPAIR EXEC TIME]: 8.36858057975769Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7f1be51f1348>

Traceback (most recent call last):
  File "sales-exploration.py", line 86, in <module>
    res = sm.OLS(Y,Xr).fit()
AttributeError: module 'statsmodels.formula.api' has no attribute 'OLS'
[Try Solution]: OrderedDict([('action', 'fqn'), ('old_fqn', 'statsmodels.formula.api.OLS'), ('new_fqn', 'statsmodels.regression.linear_model.OLS'), ('line_no', 86)])Sales_Transactions_Dataset_Weekly.csv

               count      mean        std  min    25%   50%    75%   max
W0             811.0  8.902589  12.067163  0.0  0.000  3.00  12.00  54.0
W1             811.0  9.129470  12.564766  0.0  0.000  3.00  12.00  53.0
W2             811.0  9.389642  13.045073  0.0  0.000  3.00  12.00  56.0
W3             811.0  9.717633  13.553294  0.0  0.000  4.00  13.00  59.0
W4             811.0  9.574599  13.095765  0.0  0.000  4.00  13.00  61.0
...              ...       ...        ...  ...    ...   ...    ...   ...
Normalized 47  811.0  0.314636   0.266029  0.0  0.000  0.31   0.50   1.0
Normalized 48  811.0  0.338150   0.275690  0.0  0.105  0.33   0.50   1.0
Normalized 49  811.0  0.358903   0.286665  0.0  0.100  0.33   0.55   1.0
Normalized 50  811.0  0.373009   0.295197  0.0  0.110  0.35   0.56   1.0
Normalized 51  811.0  0.427941   0.342360  0.0  0.090  0.43   0.67   1.0

[106 rows x 8 columns]
  Product_Code  W0  W1  W2  ...  Normalized 48  Normalized 49  Normalized 50  Normalized 51
0           P1  11  12  10  ...           0.22           0.17           0.11           0.39
1           P2   7   6   3  ...           0.50           0.10           0.60           0.00
2           P3   7  11   8  ...           1.00           0.45           0.45           0.36
3           P4  12   8  13  ...           0.71           0.35           0.29           0.35
4           P5   8   5  13  ...           0.13           0.53           0.33           0.40

[5 rows x 107 columns]
<zip object at 0x7efec2d1f348>

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
PCA %error 26.2 rmsle 0.45597248747204655

Ypredict [ 6.88902589  4.88902589  6.88902589  7.88902589  9.88902589  4.88902589
  5.88902589  6.88902589  9.88902589 17.88902589  6.88902589  5.88902589
  8.88902589 12.88902589 19.88902589 24.88902589 23.88902589 23.88902589
 23.88902589  8.88902589  3.88902589  7.88902589  4.88902589 23.88902589
 20.88902589  7.88902589 24.88902589 22.88902589 11.88902589 20.88902589
  7.88902589  9.88902589 13.88902589 28.88902589 24.88902589 22.88902589
 27.88902589 24.88902589 27.88902589 18.88902589 22.88902589 21.88902589
 29.88902589 23.88902589 21.88902589 20.88902589 26.88902589 23.88902589
 25.88902589  5.88902589 10.88902589 26.88902589  4.88902589 27.88902589
 20.88902589 23.88902589 21.88902589 27.88902589  7.88902589 23.88902589
 20.88902589 13.88902589 17.88902589 21.88902589  9.88902589 27.88902589
 22.88902589 11.88902589 17.88902589 20.88902589  6.88902589 22.88902589
 21.88902589  8.88902589 24.88902589 20.88902589  4.88902589 20.88902589
 22.88902589 23.88902589 10.88902589  9.88902589 24.88902589 26.88902589
 19.88902589 19.88902589 17.88902589 22.88902589 18.88902589 22.88902589
 11.88902589 22.88902589  5.88902589  9.88902589 13.88902589 29.88902589
 22.88902589  5.88902589 14.88902589 18.88902589 21.88902589 21.88902589
  8.88902589  1.88902589  3.88902589  9.88902589 17.88902589  5.88902589
  8.88902589  8.88902589  5.88902589 29.88902589 22.88902589  6.88902589
  8.88902589  8.88902589  3.88902589 14.88902589 24.88902589 23.88902589
  7.88902589  8.88902589  3.88902589  4.88902589  9.88902589  3.88902589
  1.88902589 20.88902589 25.88902589 19.88902589 24.88902589 25.88902589
 22.88902589 24.88902589 25.88902589 25.88902589 25.88902589 28.88902589
 26.88902589 23.88902589 22.88902589 26.88902589 22.88902589 11.88902589
  4.88902589  7.88902589  5.88902589  1.88902589  5.88902589  5.88902589
  1.88902589  8.88902589  8.88902589  8.88902589  4.88902589  4.88902589
  7.88902589  2.88902589  3.88902589  7.88902589  3.88902589  6.88902589
  6.88902589 10.88902589  8.88902589  6.88902589 23.88902589 21.88902589
 27.88902589 25.88902589  6.88902589 21.88902589 28.88902589 23.88902589
 25.88902589 27.88902589 25.88902589 24.88902589 24.88902589 23.88902589
 19.88902589 20.88902589 19.88902589 25.88902589 24.88902589 19.88902589
 23.88902589 22.88902589 20.88902589 25.88902589 25.88902589 28.88902589
 21.88902589 16.88902589  4.88902589 23.88902589 10.88902589 15.88902589
  6.88902589 24.88902589  3.88902589 22.88902589  7.88902589  3.88902589
 23.88902589  3.88902589  9.88902589 40.88902589 10.88902589 14.88902589
  8.88902589  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  1.88902589  0.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  1.88902589  0.88902589  0.88902589
 -0.11097411  0.88902589  0.88902589 -0.11097411  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  2.88902589  1.88902589
  1.88902589  0.88902589 -0.11097411  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  3.88902589 -0.11097411  2.88902589  0.88902589
  0.88902589 -0.11097411 29.88902589 48.88902589 32.88902589 14.88902589
  1.88902589 14.88902589  9.88902589 22.88902589 19.88902589 24.88902589
  4.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  2.88902589  2.88902589
  2.88902589 14.88902589 11.88902589 26.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  5.88902589  3.88902589  6.88902589 10.88902589
  5.88902589  7.88902589  4.88902589  5.88902589 10.88902589  3.88902589
  5.88902589  3.88902589  4.88902589  8.88902589  4.88902589  2.88902589
  5.88902589  5.88902589  9.88902589  3.88902589  3.88902589  4.88902589
  4.88902589  9.88902589  4.88902589  5.88902589  5.88902589  3.88902589
  8.88902589  5.88902589  4.88902589  6.88902589  4.88902589 12.88902589
  4.88902589  5.88902589  3.88902589  7.88902589  5.88902589  6.88902589
  3.88902589  6.88902589  8.88902589  8.88902589  2.88902589  3.88902589
  3.88902589  5.88902589  1.88902589  1.88902589  5.88902589  2.88902589
  3.88902589  3.88902589  4.88902589  1.88902589  1.88902589 -0.11097411
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  6.88902589  3.88902589  2.88902589  5.88902589  2.88902589  3.88902589
  2.88902589  0.88902589 17.88902589  3.88902589  3.88902589  3.88902589
 10.88902589  4.88902589  7.88902589  2.88902589  5.88902589  2.88902589
  2.88902589  1.88902589  1.88902589  2.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  4.88902589  6.88902589  3.88902589  4.88902589  5.88902589  6.88902589
  3.88902589  2.88902589  4.88902589  3.88902589 14.88902589 16.88902589
 17.88902589 28.88902589  3.88902589  4.88902589 18.88902589 12.88902589
 28.88902589 20.88902589 17.88902589 18.88902589 61.88902589 22.88902589
 25.88902589  4.88902589 12.88902589  6.88902589  4.88902589  2.88902589
  1.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  0.88902589  0.88902589
 11.88902589 17.88902589  9.88902589 13.88902589  6.88902589  3.88902589
 18.88902589 15.88902589  7.88902589  0.88902589  0.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589
  1.88902589  1.88902589  1.88902589  1.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  1.88902589  1.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589
  1.88902589  3.88902589  3.88902589  2.88902589  2.88902589  2.88902589
  0.88902589  4.88902589  7.88902589 17.88902589 13.88902589 12.88902589
  4.88902589 12.88902589 16.88902589 14.88902589 11.88902589 15.88902589
 20.88902589  9.88902589  7.88902589  3.88902589 10.88902589  4.88902589
  4.88902589 17.88902589 25.88902589 14.88902589 19.88902589 16.88902589
 23.88902589  9.88902589  8.88902589 10.88902589 28.88902589 29.88902589
 16.88902589 14.88902589 12.88902589 20.88902589 14.88902589 10.88902589
 20.88902589 13.88902589  9.88902589 12.88902589 10.88902589  9.88902589
 13.88902589 27.88902589  9.88902589 15.88902589 11.88902589 20.88902589
  5.88902589 12.88902589 23.88902589 16.88902589 25.88902589 16.88902589
 24.88902589 15.88902589  8.88902589 17.88902589  9.88902589 15.88902589
 11.88902589 10.88902589 22.88902589 11.88902589 10.88902589 20.88902589
 23.88902589  9.88902589  7.88902589  5.88902589  3.88902589 18.88902589
 11.88902589 20.88902589 22.88902589 14.88902589 11.88902589 11.88902589
  5.88902589  5.88902589  8.88902589  7.88902589  7.88902589 15.88902589
  1.88902589  8.88902589  4.88902589  4.88902589 11.88902589  3.88902589
  5.88902589  0.88902589  1.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  3.88902589  4.88902589  3.88902589  4.88902589  3.88902589
  2.88902589 12.88902589  5.88902589  4.88902589  3.88902589  3.88902589
  5.88902589  1.88902589  4.88902589  4.88902589  4.88902589  8.88902589
  5.88902589 19.88902589  9.88902589  6.88902589  3.88902589  1.88902589
  0.88902589  0.88902589  2.88902589  2.88902589  0.88902589  1.88902589
  5.88902589  6.88902589  7.88902589 17.88902589 33.88902589 11.88902589
 33.88902589  7.88902589 20.88902589 24.88902589 22.88902589 24.88902589
 26.88902589 27.88902589 22.88902589  7.88902589  4.88902589  7.88902589
  9.88902589  7.88902589  8.88902589  9.88902589  5.88902589 15.88902589
 12.88902589  9.88902589  5.88902589 11.88902589  6.88902589 20.88902589
  0.88902589 25.88902589  3.88902589  5.88902589  0.88902589  0.88902589
  0.88902589 -0.11097411 -0.11097411  0.88902589  1.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589 -0.11097411  0.88902589  1.88902589  0.88902589
  0.88902589  1.88902589  0.88902589  0.88902589 -0.11097411  2.88902589
  1.88902589  0.88902589  6.88902589 15.88902589  7.88902589  7.88902589
  0.88902589  0.88902589  0.88902589  0.88902589 -0.11097411  0.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  0.88902589  1.88902589  3.88902589  4.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  4.88902589  2.88902589  1.88902589
  5.88902589  9.88902589  5.88902589  1.88902589  4.88902589  3.88902589
  2.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589  4.88902589  1.88902589
  0.88902589  0.88902589  0.88902589  3.88902589  1.88902589  0.88902589
  1.88902589  2.88902589  4.88902589  1.88902589  0.88902589  0.88902589
  0.88902589  3.88902589  1.88902589  0.88902589 11.88902589  2.88902589
  6.88902589  1.88902589  0.88902589  0.88902589  0.88902589  2.88902589
  0.88902589  1.88902589  0.88902589  0.88902589  1.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  5.88902589  0.88902589  8.88902589
  1.88902589  0.88902589  2.88902589  8.88902589  3.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589  0.88902589
  0.88902589  0.88902589  0.88902589  1.88902589 21.88902589  4.88902589
 20.88902589  5.88902589  2.88902589  2.88902589  2.88902589  4.88902589
  6.88902589  5.88902589  8.88902589  2.88902589  7.88902589  2.88902589
  0.88902589  5.88902589  7.88902589  4.88902589  9.88902589  2.88902589
  6.88902589  1.88902589  5.88902589  5.88902589  4.88902589  7.88902589
  3.88902589  8.88902589  2.88902589  0.88902589  6.88902589  2.88902589
  5.88902589  8.88902589  0.88902589  5.88902589  1.88902589  1.88902589
  0.88902589]
FastICA %error 26.2 rmsle 0.45597248747658764

Ypredict [ 8.  3.  5.  6.  7.  5.  4.  7.  7. 14.  9.  4.  5.  9. 28. 32. 25. 31.
 24.  7.  8.  8.  3. 28. 29.  7. 25. 24.  8. 20. 10.  6. 14. 37. 29. 35.
 26. 31. 32. 28. 28. 25. 24. 17. 24. 26. 29. 27. 26.  5. 13. 28.  4. 28.
 23. 26. 29. 32.  9. 27. 30. 17. 28. 30.  8. 23. 27.  8. 25. 22.  6. 24.
 34. 12. 26. 33.  2. 23. 25. 22.  8.  9. 29. 30. 25. 26. 31. 21. 23. 27.
  7. 27. 11.  5.  7. 36. 31.  4. 10.  9. 27. 26.  6.  3.  3. 10. 19.  1.
  6.  7.  3. 27. 27.  7.  4.  8.  3.  8. 22. 25.  7.  8.  3.  6.  6.  2.
  1. 31. 31. 30. 30. 31. 30. 22. 20. 31. 29. 31. 28. 32. 22. 22. 27.  9.
  6. 10.  6.  3.  7.  2.  5.  7.  8.  6.  2.  5.  7.  2.  2.  8.  4.  5.
  5.  6. 11.  5. 28. 34. 31. 28.  6. 32. 29. 28. 24. 24. 25. 25. 30. 29.
 27. 21. 27. 20. 27. 22. 20. 26. 20. 31. 35. 29. 27. 22.  3. 25.  9. 12.
  4. 16.  3. 11.  6.  1. 10.  1.  5. 23.  7. 12.  9.  0.  1.  1.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  1.  0.  0.  0.  0.  0. 21. 29. 19.  8.  2. 11.  9. 13. 11. 12.
  4.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 14. 10. 13.  2.  0.
  0.  0.  2.  3.  2.  8.  4.  4.  2.  3.  7.  2.  3.  2.  4.  8.  3.  3.
  4.  5.  9.  5.  6.  6.  4. 13.  3.  4.  4.  4.  7.  4.  5.  3.  4.  7.
  4.  2.  5.  4.  7.  5.  3.  9. 11.  7.  1.  3.  4.  3.  1.  1.  2.  1.
  2.  2.  3.  1.  0.  0.  0.  0.  0.  0.  0.  0.  4.  2.  2.  4.  1.  2.
  1.  0. 10.  2.  2.  1.  4.  2.  4.  2.  3.  3.  1.  0.  1.  0.  1.  1.
  0.  0.  0.  0.  0.  0.  1.  3.  2.  2.  1.  4.  0.  4.  2.  3.  9. 13.
 10. 13.  6.  1. 13.  8. 19. 15. 17. 14. 39. 11. 10.  2.  8.  3.  5.  1.
  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 11. 14.  6.  8.  2.  0.
 14.  6.  3.  1.  0.  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  1.  0.  1.
  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  0.  0.  0.  1.  2.  1.  1.  1.  1.  1.  0.  0.  5.  7. 12. 11.  7.
  5.  9. 15. 11.  7. 14. 11.  5.  5.  5.  9.  5.  3. 12. 20.  7. 12. 12.
 18. 11.  4.  6. 23. 16. 19. 11.  6. 21.  7.  7. 15. 11.  7.  9. 11.  9.
  8. 14. 11. 10. 11. 12.  6.  6. 23.  7. 17. 12. 16. 11.  7. 18.  9. 11.
  7.  9. 13.  8.  8. 26. 28.  7.  5.  6.  4. 14.  8.  8. 23. 11. 13.  9.
  3.  4.  8.  7.  7. 15.  4.  3.  3.  3.  3.  2.  2.  0.  1.  0.  2.  0.
  0.  4.  4.  3.  1.  4.  1.  6.  4.  1.  1.  4.  3.  2.  4.  2.  5.  4.
  1. 10.  4.  2.  3.  1.  1.  0.  1.  1.  1.  0.  2.  3.  4. 12. 19.  5.
 19.  4. 27. 22. 21. 23. 27. 31. 27.  6.  5.  7. 10.  8.  9.  9.  5.  9.
  8.  8.  8.  7.  5. 12.  0. 14.  3.  2.  0.  0.  0.  0.  0.  0.  1.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  0.  1.  7.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  1.
  0.  0.  1.  0.  2.  0.  0.  0.  0.  2.  0.  2.  1.  2.  3.  0.  3.  2.
  1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  3.  0.
  1.  0.  0.  1.  1.  1.  1.  2.  1.  1.  0.  0.  0.  1.  0.  0.  5.  2.
  2.  2.  2.  0.  0.  2.  0.  2.  1.  1.  0.  0.  0.  0.  0.  2.  0.  2.
  0.  0.  0.  5.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1. 13.  4.
 14.  2.  0.  2.  0.  3.  4.  2.  3.  3.  4.  0.  0.  4.  3.  2.  5.  1.
  3.  0.  3.  3.  3.  3.  2.  3.  1.  0.  2.  2.  3.  7.  0.  5.  1.  1.
  1.]
Gauss %error 37.8 rmsle 0.5659018691895774

Ypredict [ 8.  4.  9.  9.  9.  6.  5. 10. 11. 22. 12.  5. 11. 15. 20. 20. 23. 22.
 23.  8.  8.  9.  4. 21. 21. 11. 24. 24. 15. 25.  7.  9. 14. 27. 25. 22.
 24. 24. 26. 19. 21. 23. 29. 24. 20. 21. 25. 22. 23.  8. 17. 25.  6. 26.
 25. 22. 25. 24.  9. 26. 21. 17. 20. 22.  9. 28. 23. 10. 18. 23.  8. 21.
 22.  9. 23. 20.  6. 23. 19. 26. 12.  8. 23. 25. 22. 19. 23. 22. 21. 22.
  9. 20.  7. 10. 17. 26. 23.  5. 11. 19. 24. 23.  9.  3.  4. 12. 18.  6.
 10.  9.  5. 35. 23.  9. 12. 10.  4. 11. 25. 31.  9.  9.  4.  6. 10.  6.
  4. 23. 22. 24. 20. 24. 19. 25. 25. 17. 23. 24. 25. 25. 23. 30. 24. 11.
  8.  8.  8.  4.  8.  6.  4. 10. 10.  9.  4.  4.  8.  4.  4.  8.  4.  7.
  5. 11.  9. 10. 24. 20. 27. 25.  8. 23. 20. 23. 22. 21. 27. 25. 24. 23.
 22. 22. 21. 22. 24. 22. 25. 24. 25. 25. 20. 25. 16. 20.  5. 25. 11. 14.
  5. 26.  5. 25.  7.  4. 23.  6. 11. 35. 12. 17. 11.  1.  0.  1.  0.  0.
  0.  0.  0.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.
  0.  0.  3.  0.  2.  0.  0.  0. 28. 40. 30. 12.  4. 12. 10. 27. 20. 27.
  5.  1.  0.  0.  0.  0.  1.  0.  0.  0.  3.  2.  2. 19. 11. 29.  4.  2.
  1.  0.  5.  4.  6. 11.  7.  6.  6.  5. 10.  5.  6.  4.  5.  9.  6.  4.
  4.  4. 11.  4.  5.  6.  5. 11.  5.  5.  4.  4. 11.  5.  5.  5.  5. 12.
  5.  5.  4.  5.  4.  6.  5.  7.  9. 10.  3.  5.  5.  6.  3.  2.  5.  5.
  5.  4.  5.  1.  1.  1.  1.  0.  0.  0.  0.  0.  6.  5.  4.  8.  3.  5.
  3.  1. 18.  3.  5.  5. 12.  6.  6.  4.  5.  3.  3.  1.  1.  2.  1.  1.
  0.  0.  0.  0.  1.  0.  5.  5.  5.  5.  5.  9.  4.  4.  6.  5. 13. 18.
 20. 27.  4.  5. 22. 13. 28. 21. 21. 20. 71. 25. 27.  5. 14.  5.  4.  3.
  1.  1.  1.  0.  0.  3.  1.  1.  0.  0.  0.  0. 12. 18. 11. 14.  7.  5.
 19. 14.  9.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.
  0.  0.  1.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  1.  0.  0.  4.  3.  2.  3.  3.  1.  3.  2.  1.  6.  9. 20. 15. 13.
  5. 12. 19. 14. 12. 17. 22. 11.  7.  4.  9.  7.  7. 18. 24. 16. 21. 15.
 25. 10.  9. 10. 36. 30. 18. 16. 11. 21. 15. 11. 21. 15. 11. 12. 13. 11.
 16. 26. 12. 16. 15. 22.  6. 12. 33. 13. 27. 16. 24. 19. 10. 20. 12. 16.
 13. 14. 23. 16. 12. 23. 23.  9.  9.  7.  5. 21. 14. 21. 28. 15. 14. 11.
  5.  5. 10. 10.  9. 17.  3.  7.  5.  5.  9.  3.  6.  1.  1.  1.  2.  1.
  0.  4.  5.  4.  5.  4.  4. 11.  5.  5.  4.  5.  5.  3.  4.  5.  5. 11.
  5. 21.  9.  7.  4.  2.  1.  0.  3.  2.  1.  1.  7.  6.  6. 19. 35. 10.
 36.  8. 23. 25. 26. 25. 22. 29. 24.  8.  8.  9. 11.  9. 10. 10.  9. 12.
 11. 11.  7. 15.  6. 21.  0. 26.  4.  6.  0.  0.  0.  0.  0.  0.  1.  1.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  1.  1.  0.  0.  0.  2.
  1.  1.  7. 13.  9.  7.  0.  0.  1.  0.  0.  0.  0.  0.  0.  5.  2.  1.
  1.  1.  1.  5.  5.  1.  1.  1.  1.  5.  3.  2.  6. 10.  6.  2.  6.  3.
  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  3.  6.  2.  1.  1.  1.  3.  2.  0. 10.  3.
  6.  2.  2.  1.  1.  4.  2.  2.  1.  1.  1.  0.  1.  0.  0.  5.  0.  8.
  2.  1.  4.  8.  4.  0.  1.  1.  0.  0.  0.  1.  1.  0.  1.  1. 24.  5.
 20.  6.  5.  5.  3.  4.  7.  5. 11.  3.  7.  2.  1.  6.  7.  5. 10.  4.
  5.  2.  5.  6.  5. 10.  5.  7.  2.  1.  5.  3.  6. 10.  1.  6.  1.  1.
  1.]
KMeans %error 23.7 rmsle 0.4086339101248666

Ypredict [ 7.  5.  7.  9. 10.  5.  7.  8. 10. 17.  8.  5. 10. 15. 23. 24. 24. 23.
 23.  9.  5.  8.  4. 23. 22. 10. 24. 22. 13. 20.  7.  9. 12. 28. 25. 23.
 29. 26. 28. 19. 24. 22. 32. 25. 23. 20. 29. 25. 25.  6. 10. 27.  5. 27.
 21. 23. 21. 27.  7. 23. 22. 15. 21. 23. 10. 26. 23. 12. 18. 20.  7. 23.
 23.  9. 25. 20.  7. 21. 25. 25. 12. 10. 28. 28. 21. 20. 17. 22. 18. 21.
 11. 25.  5. 11. 15. 30. 23.  6. 14. 21. 22. 21.  8.  3.  4. 11. 16.  6.
  9. 10.  6. 29. 21.  6.  8. 10.  5. 16. 24. 25.  8.  9.  5.  6. 12.  5.
  3. 20. 26. 19. 23. 27. 21. 26. 25. 25. 26. 28. 27. 22. 21. 25. 23. 13.
  7.  8.  5.  2.  6.  6.  4.  9. 11. 10.  6.  4.  7.  3.  3.  7.  4.  8.
  7. 11.  8.  7. 23. 23. 28. 27.  6. 22. 28. 23. 25. 27. 28. 25. 26. 22.
 20. 21. 21. 25. 24. 20. 25. 24. 23. 26. 25. 26. 25. 19.  5. 22. 10. 15.
  6. 24.  4. 21.  6.  4. 25.  4.  9. 40. 11. 13.  9.  3.  1.  2.  1.  1.
  1.  1.  0.  2.  2.  2.  0.  1.  1.  1.  2.  1.  0.  0.  0.  0.  0.  2.
  1.  0.  1.  1.  0.  1.  2.  2.  1.  1.  3.  2.  2.  2.  1.  0.  1.  2.
  1.  0.  5.  1.  2.  0.  0.  0. 28. 50. 33. 14.  3. 13. 10. 24. 19. 28.
  6.  1.  1.  1.  1.  1.  1.  1.  1.  2.  3.  3.  3. 16. 12. 27.  4.  2.
  1.  1.  6.  4.  7. 13.  7.  8.  5.  5. 10.  4.  6.  3.  5.  9.  5.  4.
  6.  6.  9.  3.  5.  6.  6.  9.  6.  6.  5.  4.  9.  5.  5.  6.  5. 14.
  6.  7.  3.  7.  4.  7.  4.  7.  8. 10.  3.  4.  4.  6.  3.  2.  5.  4.
  4.  4.  5.  1.  3.  0.  2.  1.  1.  1.  1.  0.  7.  4.  3.  6.  2.  5.
  3.  2. 17.  5.  5.  4. 10.  5.  9.  4.  6.  3.  4.  3.  1.  2.  2.  1.
  1.  0.  1.  1.  2. -1.  6.  7.  4.  4.  6.  7.  3.  4.  5.  3. 14. 16.
 18. 29.  4.  6. 19. 13. 29. 20. 18. 18. 60. 24. 27.  5. 13.  6.  2.  4.
  0.  3.  1.  1.  0.  3.  2.  2.  1.  1.  1.  1. 13. 16. 10. 14.  9.  5.
 18. 17.  8.  2.  2.  2.  0.  1.  2.  1.  1.  1.  2.  2.  2.  2.  2.  2.
  1.  1.  2.  1.  1.  2.  1.  1.  0.  2.  0.  1.  1.  1.  0.  1.  1.  2.
  1.  1.  1.  1.  5.  2.  2.  4.  4.  3.  3.  2.  1.  5.  9. 18. 13. 13.
  4. 13. 17. 14. 11. 18. 21.  9.  7.  4. 11.  6.  6. 16. 25. 14. 19. 17.
 25. 10. 10. 11. 27. 29. 15. 15. 13. 19. 14. 12. 20. 14. 11. 13. 11. 11.
 14. 27.  9. 16. 12. 19.  5. 13. 25. 15. 27. 17. 24. 17.  9. 17. 11. 17.
 11. 11. 22. 12. 10. 21. 22. 11.  8.  6.  4. 19. 12. 21. 21. 14. 11. 11.
  6.  4. 11. 11.  9. 16.  2.  9.  5.  5. 10.  4.  7.  2.  3.  0.  2.  2.
  1.  3.  5.  4.  5.  4.  3. 13.  6.  4.  4.  4.  6.  4.  5.  5.  5.  9.
  6. 19. 11.  7.  4.  1.  2.  0.  3.  3.  2.  3.  6.  8.  7. 17. 32. 10.
 35.  8. 22. 25. 24. 24. 28. 27. 23.  6.  6.  9. 10.  8.  9. 13.  6. 15.
 11.  9.  6. 10.  6. 21.  1. 27.  3.  7.  2.  1.  1.  1.  0.  1.  1.  2.
  1.  0.  2.  1.  1.  1.  0.  2.  0.  1.  1.  1.  2.  2.  1.  2.  1.  3.
  2.  2.  7. 16.  8.  7.  1.  0.  2.  1.  1.  1.  0.  0.  1.  4.  3.  1.
  2.  2.  1.  5.  5.  2.  1.  1.  2.  5.  3.  2.  7. 11.  6.  2.  6.  3.
  3.  1.  0.  1.  1.  1.  1.  1.  1.  1.  0.  2.  0.  1.  1.  3.  6.  3.
  1.  0.  1.  5.  2.  1.  3.  3.  4.  2.  1.  1.  2.  3.  2.  1. 12.  3.
  6.  2.  2.  2.  1.  2.  1.  2.  2.  1.  1.  0.  0.  0.  1.  6.  1.  9.
  2.  1.  4.  8.  5.  3.  1.  1.  1.  2.  1.  1.  2.  1.  2.  1. 22.  4.
 20.  6.  5.  3.  4.  6.  8.  6. 10.  3.  7.  2.  2.  6.  8.  5.  9.  4.
  8.  4.  5.  6.  5.  9.  5.  9.  4.  1.  6.  3.  6. 10.  1.  4.  3.  2.
  1.]
SparsePCA %error 26.5 rmsle 0.4824409403408959

Ypredict [10.  0.  7.  8.  9.  6.  1.  9. 13. 20. 21.  3.  5. 18. 20. 22. 25. 23.
 29.  9.  5.  5.  0. 14. 20. 21. 28. 25. 16. 21.  7.  7. 11. 26. 28. 19.
 29. 24. 25. 22. 27. 16. 27. 16. 20. 14. 20. 27. 23.  9.  7. 20.  9. 26.
 25. 19. 20. 31. 13. 21. 26. 10. 20. 18.  8. 23. 24.  6. 22. 30.  6. 23.
 18. 10. 24. 15.  6. 20. 21. 24. 12.  7. 33. 21. 16. 25. 24. 24. 28. 18.
  9. 19. 10.  9. 14. 21. 25.  5. 11. 21. 15. 29.  6.  4.  4. 21. 20.  4.
 10.  7.  9. 35. 16.  8. 10. 12.  2. 10. 19. 32. 12. 12.  4.  4.  6.  5.
  0. 22. 22. 15. 24. 20. 17. 26. 16. 16. 28. 24. 24. 29. 23. 25. 24. 12.
  7. 13.  3.  7.  5.  3.  4. 11. 10.  6.  5.  4.  5.  5.  7. 11.  6. 13.
  3.  9. 11. 11. 28. 18. 33. 32.  9. 25. 15. 24. 19. 13. 28. 26. 25. 16.
 15. 20. 19. 30. 28. 20. 28. 24. 22. 31. 24. 31. 21. 27.  2. 28. 10. 16.
  2. 26.  4. 23.  7.  4. 17.  5. 12. 45. 15. 14. 14.  1.  1.  0.  0.  1.
  0.  2.  1.  0.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  1.  0.  0.  0.  0.  3.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.
  0.  0.  8.  0.  2.  0.  0.  0. 34. 43. 23.  9.  3. 19. 14. 22. 14. 31.
  5.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0. 18. 15. 29.  1.  2.
  0.  2.  6.  5.  1.  4.  3.  7.  2.  8. 12.  6.  1.  6.  2. 11.  7.  7.
  9.  2.  8.  4.  4.  6.  6. 13.  9.  2.  6.  7.  5. 10.  3.  2.  4.  8.
  6.  1.  1.  5.  3.  6.  6.  9.  9.  9.  6.  4.  3.  5.  2.  1.  8.  6.
  5.  3.  8.  1.  0.  1.  2.  0.  0.  1.  0.  0.  8.  2.  5.  9.  2.  5.
  2.  0. 27.  7.  6.  6. 12.  3.  5.  3.  4.  4.  1.  3.  1.  0.  1.  0.
  2.  0.  0.  0.  2.  0.  9.  4.  5.  2.  9. 10.  5.  6.  3.  5. 11. 18.
 23. 19.  9.  0. 25. 13. 31. 24. 19. 21. 73. 31. 22.  4. 16.  6.  4.  3.
  0.  0.  2.  0.  0.  1.  1.  1.  0.  1.  0.  1. 11. 20. 14. 14. 16.  9.
 28. 14.  9.  0.  1.  0.  0.  0.  0.  0.  0.  0.  3.  4.  2.  3.  0.  2.
  0.  1.  0.  2.  0.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  2.  4.  3.  3.  4.  1.  5.  2.  2.  2.  8. 20. 20.  9.
  4.  6. 21. 10.  9. 15. 24. 11.  4.  4. 12.  6.  8. 15. 21. 12. 11. 13.
 28. 12.  6. 14. 28. 35. 22. 16. 12. 20. 14. 17. 21. 22. 10.  9. 13. 12.
 11. 27. 11. 16. 15. 21.  6. 15. 38. 10. 37. 22. 20. 25.  9. 17. 19. 11.
  9. 10. 22. 15. 14. 30. 18.  6.  7.  8.  3. 16.  6. 29. 30. 21. 14. 15.
  5.  6.  9. 10. 10. 25.  3.  4.  1.  4. 11.  5.  5.  0.  2.  0.  1.  0.
  1.  7.  4.  1.  5.  5.  1. 11.  5.  4.  0.  8.  8.  6.  2.  4.  3.  7.
  4. 21. 16.  6.  3.  1.  0.  2.  4.  2.  1.  5.  9.  5. 10. 15. 30. 12.
 39.  4. 18. 33. 22. 19. 30. 25. 23.  4. 10. 11. 12.  8.  8.  6.  8.  9.
 13.  8.  7. 14.  3. 20.  1. 18.  4.  5.  0.  0.  0.  0.  0.  0.  1.  0.
  1.  0.  0.  0.  1.  2.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  2.
  0.  1.  7. 20.  9. 10.  0.  0.  0.  1.  1.  2.  0.  0.  0.  5.  1.  0.
  4.  0.  1.  4.  7.  0.  0.  1.  2.  3.  1.  5.  6. 17.  8.  1.  2.  5.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2. 12.  4.
  0.  0.  1.  5.  3.  1.  0.  6.  3.  3.  1.  1.  0.  2.  3.  0.  6.  6.
  8.  3.  1.  0.  1.  1.  2.  2.  1.  1.  2.  2.  1.  0.  0.  4.  0. 12.
  4.  2.  1. 11.  8.  1.  0.  0.  0.  0.  0.  0.  2.  1.  0.  0. 30.  3.
 25.  4.  4.  2.  7. 11.  8.  3. 13.  4.  9.  3.  2.  4.  3.  5. 10.  3.
 12.  2.  9.  0.  3.  8.  4. 14.  2.  1.  5.  4.  4. 11.  0.  5.  3.  0.
  1.]
SparseRP %error 0.0 rmsle 0.017457467380136024

Ypredict [ 7.  4.  6.  9. 10.  5.  6.  7. 10. 17.  8.  6. 11. 14. 21. 25. 23. 24.
 23. 10.  5.  9.  5. 23. 21.  9. 24. 21. 13. 21.  9. 10. 13. 29. 24. 22.
 28. 25. 27. 17. 22. 23. 31. 23. 23. 21. 27. 23. 25.  7. 11. 27.  5. 27.
 20. 22. 23. 27.  8. 24. 21. 15. 18. 21. 10. 27. 23. 11. 18. 20.  6. 23.
 22.  9. 24. 20.  6. 20. 24. 25. 12. 10. 27. 27. 21. 20. 18. 22. 20. 22.
 12. 23.  6. 10. 15. 29. 22.  6. 15. 20. 22. 21.  9.  2.  4. 11. 17.  5.
 10.  9.  6. 31. 22.  8.  9.  9.  5. 15. 24. 24.  7. 11.  4.  6. 11.  4.
  3. 22. 26. 20. 24. 27. 21. 24. 25. 26. 27. 29. 26. 23. 22. 26. 23. 13.
  5.  9.  7.  3.  6.  7.  3. 10. 10. 10.  6.  5.  8.  3.  4.  8.  4.  7.
  7. 12.  9.  7. 24. 22. 29. 27.  7. 22. 27. 24. 27. 28. 28. 24. 25. 24.
 19. 21. 20. 26. 24. 21. 24. 23. 21. 25. 25. 28. 21. 17.  6. 24. 11. 16.
  6. 25.  6. 22.  7.  4. 24.  4. 10. 40. 11. 14.  9.  2.  1.  1.  0.  1.
  1.  0.  0.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  1.
  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  2.  1.  2.  1.  0.  0.  0.  1.
  0.  0.  4.  0.  3.  1.  0.  0. 30. 49. 33. 14.  2. 16. 10. 22. 19. 25.
  5.  1.  1.  1.  1.  1.  1.  0.  0.  0.  3.  2.  3. 16. 12. 27.  4.  2.
  1.  1.  6.  4.  7. 12.  7.  7.  5.  5. 11.  4.  6.  5.  5. 10.  5.  3.
  6.  6. 10.  4.  5.  6.  5. 10.  5.  6.  6.  5. 10.  6.  5.  6.  5. 14.
  6.  7.  4.  8.  6.  7.  4.  7.  8. 10.  4.  4.  5.  6.  3.  2.  5.  3.
  4.  3.  5.  1.  2.  0.  1.  1.  1.  0.  0.  0.  7.  4.  3.  7.  3.  4.
  3.  1. 18.  4.  5.  4. 11.  4.  8.  4.  5.  3.  3.  2.  1.  2.  1.  1.
  1.  0.  0.  1.  1.  0.  7.  7.  4.  5.  5.  7.  3.  3.  5.  4. 14. 16.
 18. 29.  5.  5. 18. 12. 28. 21. 18. 18. 61. 24. 27.  5. 14.  7.  4.  3.
  2.  2.  1.  0.  0.  3.  1.  1.  1.  0.  1.  1. 14. 17.  9. 14.  7.  5.
 20. 15.  8.  1.  1.  1.  1.  1.  1.  0.  1.  1.  2.  2.  2.  1.  1.  1.
  1.  1.  2.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.
  1.  1.  1.  1.  3.  2.  1.  4.  4.  2.  3.  2.  1.  5.  8. 19. 14. 14.
  5. 13. 17. 15. 12. 17. 22. 11.  7.  4. 12.  5.  6. 17. 25. 14. 19. 18.
 24. 10. 10. 11. 28. 30. 17. 15. 13. 20. 15. 11. 20. 15. 11. 13. 11. 11.
 14. 27. 11. 17. 12. 20.  6. 13. 24. 15. 26. 17. 24. 16.  9. 18. 10. 16.
 12. 12. 22. 13. 10. 21. 22. 10.  8.  6.  5. 19. 12. 20. 23. 14. 12. 11.
  5.  6. 10. 10.  8. 17.  2.  9.  5.  5. 11.  4.  6.  1.  2.  1.  2.  1.
  1.  4.  5.  3.  5.  4.  3. 13.  6.  5.  4.  3.  5.  3.  5.  5.  4.  8.
  5. 20. 11.  6.  4.  2.  1.  0.  3.  3.  1.  2.  7.  8.  8. 17. 34. 11.
 34.  8. 20. 25. 23. 24. 29. 28. 24.  8.  6.  9. 11.  9. 10. 12.  7. 15.
 12. 10.  6. 11.  7. 21.  1. 26.  4.  6.  1.  0.  1.  0.  0.  0.  2.  1.
  1.  0.  1.  0.  1.  1.  0.  1.  0.  0.  2.  1.  1.  1.  0.  0.  0.  2.
  2.  1.  7. 17.  9.  7.  0.  1.  1.  1.  0.  1.  0.  0.  1.  4.  2.  1.
  2.  2.  1.  5.  5.  1.  1.  1.  1.  5.  3.  2.  6. 10.  6.  2.  6.  3.
  3.  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  2.  5.  2.
  1.  0.  1.  4.  2.  1.  2.  2.  5.  2.  1.  1.  1.  3.  3.  0. 11.  3.
  7.  2.  2.  1.  1.  3.  1.  2.  1.  2.  1.  1.  1.  0.  0.  5.  0.  9.
  2.  1.  3.  8.  4.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  2. 22.  5.
 22.  6.  4.  3.  3.  6.  7.  6.  9.  3.  8.  2.  1.  5.  8.  4. 10.  3.
  7.  2.  6.  6.  4.  8.  4.  8.  2.  0.  7.  3.  6. 10.  1.  5.  2.  1.
  1.]
Birch %error 25.5 rmsle 0.43445216955480653
<zip object at 0x7efec2d1f048>

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 11.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 11.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
  9.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
PCA %error 22.8 rmsle 0.29067081375776727

Ypredict [11.62745098 10.62745098 10.62745098 11.62745098 11.62745098 10.62745098
 11.62745098 12.62745098 12.62745098 11.62745098 11.62745098 10.62745098
 12.62745098 11.62745098 11.62745098 12.62745098 12.62745098 11.62745098
 13.62745098 12.62745098 12.62745098 12.62745098 12.62745098 13.62745098
 15.62745098 14.62745098  9.62745098  9.62745098  9.62745098 10.62745098
 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098  9.62745098
  9.62745098 10.62745098 10.62745098 10.62745098 10.62745098 10.62745098
 11.62745098 11.62745098 11.62745098 11.62745098 11.62745098 12.62745098
 11.62745098 12.62745098 13.62745098]
FastICA %error 22.7 rmsle 0.29076748159264454

Ypredict [11. 12. 14. 13. 12. 12. 11. 16. 15. 11. 16. 10. 11. 14. 14. 11. 12. 12.
 16. 10. 10. 14. 11. 14. 12.  9. 10.  8.  9. 10. 11. 10.  8. 11. 10. 11.
 10. 11. 11.  9.  9. 11.  9. 12. 10. 12. 12. 11. 12. 10. 10.]
Gauss %error 26.5 rmsle 0.32647321222137776

Ypredict [11. 11. 12. 12. 12. 12. 12. 12. 13. 11. 13. 12. 12. 12. 12. 13. 13. 13.
 13. 12. 13. 11. 13. 13. 15. 16. 12. 11. 11. 11. 11. 12. 10. 10. 10. 10.
 11. 10. 10. 10. 11. 10. 10. 11. 10.  9. 11. 11. 11. 11. 13.]
KMeans %error 23.7 rmsle 0.30044582830925654

Ypredict [12. 14. 14. 14. 12. 15. 13. 12. 13. 11. 13. 14. 13.  9. 14. 10. 11. 12.
 11. 15. 12. 14. 13. 11. 12. 17.  8.  8.  7. 10. 14.  9.  9. 12. 12.  9.
  8. 10.  9. 11. 11.  8.  8. 12.  8. 12. 12. 11. 13. 13. 10.]
SparsePCA %error 22.8 rmsle 0.2716927870713179

Ypredict [15. 12. 11. 17. 10. 18. 11. 16.  8.  6. 11. 10. 18. 10. 14. 13. 13. 12.
 14. 10. 10. 11. 14. 11. 10. 17.  8. 13.  6. 16.  9.  4. 12. 10. 19.  9.
  7.  5. 10.  8. 11.  5. 11. 13. 11. 11. 11. 14. 15. 16. 17.]
SparseRP %error 0.0 rmsle 1.1973626668368927e-15

Ypredict [11. 11. 11. 12. 12. 11. 12. 13. 13. 13. 13. 11. 13. 12. 13. 13. 13. 13.
 14. 13. 13. 13. 13. 14. 16. 13.  9.  8.  9.  9. 10. 10. 10. 10. 10. 10.
 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 12. 12. 11. 12. 13.]
Birch %error 22.6 rmsle 0.29018415969380623
<zip object at 0x7efeb3b40d08>
SVC %error 23.6 rmsle 0.30893639912083243
SVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
kSVC %error 23.6 rmsle 0.30893639912083243
kSVC Confusion Matrix
[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.20      1.00      0.33        10
        12.0       0.00      0.00      0.00         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.20        51
   macro avg       0.01      0.06      0.02        51
weighted avg       0.04      0.20      0.06        51

--------------------------------------------------------------------------------
Accuracy 19.61 %
KNN %error 0.0 rmsle 0.0
KNN Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
DecisionTree %error 0.0 rmsle 0.0
DecisionTree Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
RandomForestClassifier %error 0.0 rmsle 0.0
RandomForestClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
HuberRegressor %error 0.0 rmsle 2.2766788270036835e-05
HuberRegressor Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Ridge %error 0.0 rmsle 8.509343322242981e-16
Ridge Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
Lasso %error 0.4 rmsle 0.0060108424195109015
Lasso Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
LassoCV %error 24.3 rmsle 0.31200318409166783
LassoCV Confusion Matrix
[[ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.00      0.00      0.00        10
        12.0       0.06      1.00      0.11         3
        13.0       0.00      0.00      0.00         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.06        51
   macro avg       0.00      0.06      0.01        51
weighted avg       0.00      0.06      0.01        51

--------------------------------------------------------------------------------
Accuracy 5.88 %
Lars %error 19.1 rmsle 0.2583508488153218
Lars Confusion Matrix
[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 3 6 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       0.00      0.00      0.00         1
         5.0       0.00      0.00      0.00         2
         6.0       0.00      0.00      0.00         2
         7.0       0.00      0.00      0.00         1
         8.0       0.00      0.00      0.00         3
         9.0       0.00      0.00      0.00         2
        10.0       0.00      0.00      0.00         8
        11.0       0.19      0.30      0.23        10
        12.0       0.06      0.33      0.10         3
        13.0       0.12      0.25      0.17         4
        14.0       0.00      0.00      0.00         4
        15.0       0.00      0.00      0.00         2
        16.0       0.00      0.00      0.00         3
        17.0       0.00      0.00      0.00         3
        18.0       0.00      0.00      0.00         2
        19.0       0.00      0.00      0.00         1

    accuracy                           0.10        51
   macro avg       0.02      0.06      0.03        51
weighted avg       0.05      0.10      0.06        51

--------------------------------------------------------------------------------
Accuracy 9.8 %
SGDClassifier %error 2.7 rmsle 0.11827282171993034
SGDClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  1  0  0  0  0  1  0  0  1  0  0  0  0  1]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       0.67      1.00      0.80         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       0.91      1.00      0.95        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      0.25      0.40         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       0.50      1.00      0.67         1

    accuracy                           0.94        51
   macro avg       0.94      0.95      0.93        51
weighted avg       0.96      0.94      0.93        51

--------------------------------------------------------------------------------
Accuracy 94.12 %
RidgeClassifier %error 0.0 rmsle 0.0
RidgeClassifier Confusion Matrix
[[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  " improve convergence." % max_iter, ConvergenceWarning)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20809226236913075, tolerance: 0.0641921568627451
  positive)
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/cgzhu/anaconda3/envs/migration-plus/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

--------------------------------------------------------------------------------
Classification Report
              precision    recall  f1-score   support

         4.0       1.00      1.00      1.00         1
         5.0       1.00      1.00      1.00         2
         6.0       1.00      1.00      1.00         2
         7.0       1.00      1.00      1.00         1
         8.0       1.00      1.00      1.00         3
         9.0       1.00      1.00      1.00         2
        10.0       1.00      1.00      1.00         8
        11.0       1.00      1.00      1.00        10
        12.0       1.00      1.00      1.00         3
        13.0       1.00      1.00      1.00         4
        14.0       1.00      1.00      1.00         4
        15.0       1.00      1.00      1.00         2
        16.0       1.00      1.00      1.00         3
        17.0       1.00      1.00      1.00         3
        18.0       1.00      1.00      1.00         2
        19.0       1.00      1.00      1.00         1

    accuracy                           1.00        51
   macro avg       1.00      1.00      1.00        51
weighted avg       1.00      1.00      1.00        51

--------------------------------------------------------------------------------
Accuracy 100.0 %
[REPAIR EXEC TIME]: 8.349415302276611