fold: 0 - cp:1 train: 0.8021528021528022 test: f1=0.5085780750218086, acc=0.8022813688212928
fold: 0 - cp:2 train: 0.8234175734175734 test: f1=0.5841101989703631, acc=0.8251535536706639
fold: 0 - cp:3 train: 0.831987831987832 test: f1=0.6570862239841426, acc=0.8380813103246564
fold: 0 - cp:4 train: 0.8310810810810811 test: f1=0.6452781371280725, acc=0.8396022228721849
fold: 0 - cp:5 train: 0.8296478296478297 test: f1=0.6632589452686697, acc=0.8420005849663644
fold: 0 - cp:6 train: 0.8978296478296478 test: f1=0.7896136795440153, acc=0.9028370868675051
fold: 0 - cp:7 train: 0.9186264186264186 test: f1=0.8430652311589614, acc=0.9275226674466218
fold: 0 - cp:8 train: 0.9181584181584181 test: f1=0.8344201508758471, acc=0.9242468558057911
fold: 0 - cp:9 train: 0.9176611676611677 test: f1=0.8348623853211009, acc=0.9241883591693477
fold: 0 - cp:10 train: 0.9191236691236692 test: f1=0.8199064449064449, acc=0.9189236618894414
fold: 0 - cp:11 train: 0.9583772083772084 test: f1=0.9235148514851484, acc=0.963849078677976
fold: 0 - cp:12 train: 0.9612437112437112 test: f1=0.9379121562772179, acc=0.9708101784147412
fold: 0 - cp:13 train: 0.982888732888733 test: f1=0.9756334101103165, acc=0.9882421760748757
fold: 0 - cp:14 train: 0.9842634842634843 test: f1=0.971262277191706, acc=0.9861362971629132
fold: 0 - cp:15 train: 0.9808704808704809 test: f1=0.9674531155474895, acc=0.9842644047967242
fold: 0 - cp:16 train: 0.9832104832104833 test: f1=0.9710408336362535, acc=0.9860193038900263
fold: 0 - cp:17 train: 0.9817479817479817 test: f1=0.9664804469273742, acc=0.9838549283416204
fold: 0 - cp:18 train: 0.9819234819234819 test: f1=0.9699819168173598, acc=0.9854343375255923
fold: 0 - cp:19 train: 0.9818064818064818 test: f1=0.9729073016644394, acc=0.9869552500731208
fold: 1 - cp:1 train: 0.8069318558906767 test: f1=0.547112462006079, acc=0.8082368082368082
fold: 1 - cp:2 train: 0.813951520725035 test: f1=0.5546558704453441, acc=0.8198198198198198
fold: 1 - cp:3 train: 0.8316761350804402 test: f1=0.6409651193286127, acc=0.8398268398268398
fold: 1 - cp:4 train: 0.829189814157058 test: f1=0.6756157635467981, acc=0.8459108459108459
fold: 1 - cp:5 train: 0.8224920262531399 test: f1=0.6395964691046657, acc=0.8328068328068328
fold: 1 - cp:6 train: 0.8895582918453766 test: f1=0.7999003860042336, acc=0.905990405990406
fold: 1 - cp:7 train: 0.920444731570141 test: f1=0.856359102244389, acc=0.9326079326079326
fold: 1 - cp:8 train: 0.9180754586697497 test: f1=0.8431715857928965, acc=0.9266409266409267
fold: 1 - cp:9 train: 0.9185436336524311 test: f1=0.8496993987975953, acc=0.9297999297999298
fold: 1 - cp:10 train: 0.9165254329620264 test: f1=0.8566510941726089, acc=0.9317889317889317
fold: 1 - cp:11 train: 0.9606025306189087 test: f1=0.932475884244373, acc=0.9680589680589681
fold: 1 - cp:12 train: 0.9681777988691886 test: f1=0.9389855427591277, acc=0.9708669708669708
fold: 1 - cp:13 train: 0.9804035753965562 test: f1=0.9714354877753572, acc=0.9861939861939862
fold: 1 - cp:14 train: 0.9792336631948236 test: f1=0.9685914472094708, acc=0.9847899847899848
fold: 1 - cp:15 train: 0.9804328288476627 test: f1=0.969946679592826, acc=0.9854919854919855
fold: 1 - cp:16 train: 0.9810471958050339 test: f1=0.9729138831531641, acc=0.986954486954487
fold: 1 - cp:17 train: 0.9795846327491157 test: f1=0.9734106840705825, acc=0.9871299871299871
fold: 1 - cp:18 train: 0.980608441944427 test: f1=0.9724903474903475, acc=0.9866619866619867
fold: 1 - cp:19 train: 0.982480276145461 test: f1=0.9753026634382568, acc=0.9880659880659881
fold: 2 - cp:1 train: 0.8062478062478063 test: f1=0.5496770647244743, acc=0.8083065223749635
fold: 2 - cp:2 train: 0.8168655668655668 test: f1=0.5703014933784164, acc=0.8215852588476162
fold: 2 - cp:3 train: 0.8323680823680824 test: f1=0.6459689534301453, acc=0.834571512138052
fold: 2 - cp:4 train: 0.8326313326313326 test: f1=0.6492819349962207, acc=0.8371453641415618
fold: 2 - cp:5 train: 0.8307885807885809 test: f1=0.6553976181329235, acc=0.8425855513307985
fold: 2 - cp:6 train: 0.8926523926523926 test: f1=0.7855631611698817, acc=0.899210295408014
fold: 2 - cp:7 train: 0.9213174213174213 test: f1=0.8508329126703685, acc=0.9308569757238959
fold: 2 - cp:8 train: 0.91991341991342 test: f1=0.8423101881894874, acc=0.9289265867212635
fold: 2 - cp:9 train: 0.9166374166374166 test: f1=0.8482818508869868, acc=0.9284586136297163
fold: 2 - cp:10 train: 0.9222534222534222 test: f1=0.8458844133099825, acc=0.9279321439017256
fold: 2 - cp:11 train: 0.9540774540774541 test: f1=0.913162478228415, acc=0.9591693477625036
fold: 2 - cp:12 train: 0.9628232128232128 test: f1=0.9350776378530383, acc=0.9689382860485523
fold: 2 - cp:13 train: 0.9792909792909792 test: f1=0.9651644336175395, acc=0.9832699619771863
fold: 2 - cp:14 train: 0.9794372294372294 test: f1=0.9631968905623709, acc=0.9822755191576484
fold: 2 - cp:15 train: 0.9783842283842284 test: f1=0.9708267764193198, acc=0.9859023106171395
fold: 2 - cp:16 train: 0.9798467298467299 test: f1=0.9699384281057589, acc=0.9854343375255923
fold: 2 - cp:17 train: 0.980987480987481 test: f1=0.971262277191706, acc=0.9861362971629132
fold: 2 - cp:18 train: 0.9819527319527319 test: f1=0.9678675754625122, acc=0.9845568879789413
fold: 2 - cp:19 train: 0.97993447993448 test: f1=0.969277474195507, acc=0.9852003509798186
fold: 3 - cp:1 train: 0.8036560579260627 test: f1=0.49617871840094063, acc=0.7994617994617995
fold: 3 - cp:2 train: 0.8250948784991836 test: f1=0.5954814416352879, acc=0.8240318240318241
fold: 3 - cp:3 train: 0.8242761719598407 test: f1=0.521137828068521, acc=0.8217503217503217
fold: 3 - cp:4 train: 0.8253582451055546 test: f1=0.644954481343762, acc=0.838013338013338
fold: 3 - cp:5 train: 0.8255630945441428 test: f1=0.5529746740596434, acc=0.8254943254943254
fold: 3 - cp:6 train: 0.8905822516023733 test: f1=0.7888636936255984, acc=0.9019539019539019
fold: 3 - cp:7 train: 0.9100320085930755 test: f1=0.83562672004003, acc=0.9231309231309232
fold: 3 - cp:8 train: 0.9093885626992787 test: f1=0.8369717860334481, acc=0.9252954252954253
fold: 3 - cp:9 train: 0.914770427785168 test: f1=0.8363864491844417, acc=0.9237159237159237
fold: 3 - cp:10 train: 0.9103246457598353 test: f1=0.8373091000883504, acc=0.9245934245934246
fold: 3 - cp:11 train: 0.9576191884626607 test: f1=0.9336401065633326, acc=0.967941967941968
fold: 3 - cp:12 train: 0.9624158955581511 test: f1=0.938920107500611, acc=0.9707499707499707
fold: 3 - cp:13 train: 0.9840303531810317 test: f1=0.9726392251815981, acc=0.9867789867789868
fold: 3 - cp:14 train: 0.9829775163771419 test: f1=0.9718241437940247, acc=0.9864279864279865
fold: 3 - cp:15 train: 0.9821001147334794 test: f1=0.971019764762944, acc=0.9860184860184861
fold: 3 - cp:16 train: 0.9809300998760522 test: f1=0.9742843279961183, acc=0.9875979875979876
fold: 3 - cp:17 train: 0.9824509200386597 test: f1=0.9698743749237712, acc=0.9855504855504855
fold: 3 - cp:18 train: 0.983913496782004 test: f1=0.974115931461903, acc=0.9875394875394875
fold: 3 - cp:19 train: 0.9822462998644561 test: f1=0.9666302633175585, acc=0.9839124839124839
fold: 0 - cp:13 train: 0.9909977250568737 test: f1=0.9759036144578312, acc=0.988300672711319
fold: 1 - cp:13 train: 0.9896002599935001 test: f1=0.9764350453172206, acc=0.9885931558935361
fold: 2 - cp:13 train: 0.9882027949301267 test: f1=0.983030303030303, acc=0.9918104708979234
fold: 3 - cp:13 train: 0.9898277543061423 test: f1=0.975045648204504, acc=0.9880081895291021
fold: 4 - cp:13 train: 0.9887877803054923 test: f1=0.983050847457627, acc=0.9918104708979234
fold: 5 - cp:13 train: 0.988950276243094 test: f1=0.9774802191113815, acc=0.9891781222579702
fold: 6 - cp:13 train: 0.9870003249918753 test: f1=0.9734299516908212, acc=0.987130739982451
fold: 7 - cp:13 train: 0.9888202794930127 test: f1=0.9806529625151149, acc=0.9906405381690553
fold: 8 - cp:13 train: 0.9890152746181344 test: f1=0.9812235009085404, acc=0.9909330213512723
fold: 9 - cp:13 train: 0.9888856262998882 test: f1=0.9781287970838396, acc=0.9894675248683441



PC9
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	occupation_Armed-Forces

PC8
Features:
	workclass_Local-gov workclass_Never-workeddot: graph is too large for cairo-renderer bitmaps. Scaling by 0.359765 to fit

	marital-status_Married-AF-spouse
	workclass_? workclass_Never-worked
	income workclass_Local-gov

PC4
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay

PC10
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Federal-gov workclass_Local-gov
	occupation_Armed-Forces

PC5
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	occupation_Armed-Forces
	workclass_Never-worked

PC3
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-spouse-absent
	workclass_Without-pay

PC2
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	relationship_Wife

PC7
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	workclass_Never-worked
	workclass_Never-worked^2

PC12
Features:
	workclass_Local-gov workclass_Never-worked
	marital-status_Married-AF-spouse
	workclass_Without-pay
	occupation_Armed-Forces

PC11
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked^2
	workclass_Never-worked
	workclass_Federal-gov workclass_Local-gov

PC6
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	workclass_Federal-gov

PC13
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	workclass_Never-worked
	occupation_Priv-house-serv

PC1
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	occupation_Tech-support
	marital-status_Married-AF-spouse

Normalized confusion matrix
[[9.99228395e-01 7.71604938e-04]
 [7.26392252e-03 9.92736077e-01]]

              precision    recall  f1-score   support

           0       1.00      1.00      1.00      2592
           1       1.00      0.99      1.00       826

    accuracy                           1.00      3418
   macro avg       1.00      1.00      1.00      3418
weighted avg       1.00      1.00      1.00      3418

Traceback (most recent call last):
  File "decision-tree-f1-98-acc-99-auc-99.py", line 383, in <module>
    from sklearn.externals import joblib
ImportError: cannot import name 'joblib'
[Try Solution]: OrderedDict([('action', 'fqn'), ('old_fqn', 'sklearn.externals.joblib'), ('new_fqn', 'joblib'), ('line_no', 383)])fold: 0 - cp:1 train: 0.8044635544635546 test: f1=0.5291167456115909, acc=0.8022813688212928
fold: 0 - cp:2 train: 0.8291798291798291 test: f1=0.5924978687127025, acc=0.8322316466803159
fold: 0 - cp:3 train: 0.8325435825435825 test: f1=0.6236728273692488, acc=0.8320561567709857
fold: 0 - cp:4 train: 0.8303498303498303 test: f1=0.6064753882600683, acc=0.8250950570342205
fold: 0 - cp:5 train: 0.8345618345618346 test: f1=0.6210625929430851, acc=0.8360339280491371
fold: 0 - cp:6 train: 0.8921551421551422 test: f1=0.7999999999999999, acc=0.9051769523252413
fold: 0 - cp:7 train: 0.9202936702936703 test: f1=0.8509081735620586, acc=0.9308569757238959
fold: 0 - cp:8 train: 0.9159061659061659 test: f1=0.8334392072163639, acc=0.9233109096226967
fold: 0 - cp:9 train: 0.9145021645021645 test: f1=0.8393060655945295, acc=0.9257677683533196
fold: 0 - cp:10 train: 0.9141511641511643 test: f1=0.8468424279583078, acc=0.9269377010821878
fold: 0 - cp:11 train: 0.9584357084357085 test: f1=0.9293203404465278, acc=0.9664814273179292
fold: 0 - cp:12 train: 0.963993213993214 test: f1=0.9344202457719917, acc=0.9684703129570049
fold: 0 - cp:13 train: 0.9823037323037324 test: f1=0.9693370500545389, acc=0.9852003509798186
fold: 0 - cp:14 train: 0.9808704808704809 test: f1=0.9745762711864407, acc=0.9877157063468851
fold: 0 - cp:15 train: 0.982040482040482 test: f1=0.9780968605500122, acc=0.9894706054401872
fold: 0 - cp:16 train: 0.9810752310752311 test: f1=0.9737827715355805, acc=0.9873062298917812
fold: 0 - cp:17 train: 0.9817479817479817 test: f1=0.9733868027706891, acc=0.9871892366188945
fold: 0 - cp:18 train: 0.9795834795834796 test: f1=0.971741778319123, acc=0.9864287803451302
fold: 0 - cp:19 train: 0.9806072306072307 test: f1=0.9703163017031631, acc=0.9857268207078093
fold: 1 - cp:1 train: 0.8041824181564471 test: f1=0.5432618146122563, acc=0.8094653094653095
fold: 1 - cp:2 train: 0.8141560108686922 test: f1=0.5587640773895466, acc=0.8212238212238212
fold: 1 - cp:3 train: 0.8281369534001729 test: f1=0.6470121628767849, acc=0.8438048438048438
fold: 1 - cp:4 train: 0.8294239067811833 test: f1=0.6636236845441866, acc=0.8447993447993448
fold: 1 - cp:5 train: 0.8325241053364592 test: f1=0.6833924856198752, acc=0.8486603486603487
fold: 1 - cp:6 train: 0.8909914645759288 test: f1=0.7997504678727386, acc=0.9061074061074061
fold: 1 - cp:7 train: 0.9155892746206269 test: f1=0.856535334584115, acc=0.9329004329004329
fold: 1 - cp:8 train: 0.9165546077104336 test: f1=0.8472942920681986, acc=0.9276939276939277
fold: 1 - cp:9 train: 0.917753790472555 test: f1=0.8501984126984127, acc=0.9293319293319293
fold: 1 - cp:10 train: 0.9197133884487652 test: f1=0.8443204868154158, acc=0.9281619281619281
fold: 1 - cp:11 train: 0.9581456787235917 test: f1=0.9251934651762683, acc=0.9643734643734644
fold: 1 - cp:12 train: 0.9661596084443534 test: f1=0.9322891566265059, acc=0.9671229671229671
fold: 1 - cp:13 train: 0.9777420109369104 test: f1=0.9697848682620257, acc=0.9853749853749854
fold: 1 - cp:14 train: 0.9818952652948909 test: f1=0.9707405177603854, acc=0.9857844857844857
fold: 1 - cp:15 train: 0.9803158287306626 test: f1=0.9688856729377714, acc=0.9849069849069849
fold: 1 - cp:16 train: 0.9816905801054139 test: f1=0.9676562877142167, acc=0.9843219843219844
fold: 1 - cp:17 train: 0.9803450342757783 test: f1=0.9694693481948148, acc=0.9852579852579852
fold: 1 - cp:18 train: 0.9780929804912023 test: f1=0.9718972379688818, acc=0.9863694863694864
fold: 1 - cp:19 train: 0.9795846943425326 test: f1=0.9637091880082534, acc=0.9825084825084826
fold: 2 - cp:1 train: 0.7981747981747982 test: f1=0.5223122339695181, acc=0.7964902018133957
fold: 2 - cp:2 train: 0.8191178191178192 test: f1=0.5556041393382889, acc=0.8216437554840597
fold: 2 - cp:3 train: 0.8323680823680824 test: f1=0.6460890045865874, acc=0.8329921029540801
fold: 2 - cp:4 train: 0.8311103311103312 test: f1=0.6540317834020012, acc=0.8280783854928342
fold: 2 - cp:5 train: 0.8254358254358255 test: f1=0.5549690846026241, acc=0.8273764258555133
fold: 2 - cp:6 train: 0.8895518895518896 test: f1=0.7816061489530878, acc=0.9035975431412694
fold: 2 - cp:7 train: 0.9149994149994151 test: f1=0.8339249492900608, acc=0.9233694062591401
fold: 2 - cp:8 train: 0.9154966654966655 test: f1=0.8496306498059345, acc=0.9297455396314712
fold: 2 - cp:9 train: 0.9142389142389141 test: f1=0.8501863354037268, acc=0.9294530564492541
fold: 2 - cp:10 train: 0.9123084123084122 test: f1=0.8482344102178814, acc=0.9291020766305937
fold: 2 - cp:11 train: 0.9567977067977067 test: f1=0.9288426209430497, acc=0.966013454226382
fold: 2 - cp:12 train: 0.9630864630864631 test: f1=0.9277227722772277, acc=0.9658379643170518
fold: 2 - cp:13 train: 0.9735287235287235 test: f1=0.9560067681895092, acc=0.9787072243346008
fold: 2 - cp:14 train: 0.9804609804609804 test: f1=0.9730122231635, acc=0.9869552500731208
fold: 2 - cp:15 train: 0.9816602316602316 test: f1=0.9700642346382257, acc=0.9855513307984791
fold: 2 - cp:16 train: 0.9825962325962325 test: f1=0.971497877501516, acc=0.9862532904357999
fold: 2 - cp:17 train: 0.9815724815724816 test: f1=0.9724681625227412, acc=0.9867212635273471
fold: 2 - cp:18 train: 0.9812214812214812 test: f1=0.968282901932191, acc=0.9847323778882714
fold: 2 - cp:19 train: 0.9812214812214812 test: f1=0.9738498789346247, acc=0.9873647265282246
fold: 3 - cp:1 train: 0.8102074759187534 test: f1=0.5599214145383103, acc=0.8165438165438166
fold: 3 - cp:2 train: 0.8150631655545086 test: f1=0.5551518674611248, acc=0.8209313209313209
fold: 3 - cp:3 train: 0.8223165089683573 test: f1=0.6178546712802767, acc=0.8384813384813384
fold: 3 - cp:4 train: 0.8191869714155628 test: f1=0.6358185610010427, acc=0.8365508365508365
fold: 3 - cp:5 train: 0.8198597186664571 test: f1=0.6582724754469906, acc=0.8412308412308412
fold: 3 - cp:6 train: 0.8872475258471981 test: f1=0.7784120394395433, acc=0.9000819000819
fold: 3 - cp:7 train: 0.9213512771655028 test: f1=0.8546497132884567, acc=0.9317889317889317
fold: 3 - cp:8 train: 0.9170226526625684 test: f1=0.8435167615433271, acc=0.9276354276354276
fold: 3 - cp:9 train: 0.9202399882091039 test: f1=0.8366462793068298, acc=0.925002925002925
fold: 3 - cp:10 train: 0.9158818809906784 test: f1=0.8365164247517188, acc=0.9248859248859249
fold: 3 - cp:11 train: 0.9589938679784258 test: f1=0.9304918845248421, acc=0.9671814671814671
fold: 3 - cp:12 train: 0.9644631614975555 test: f1=0.9379310344827586, acc=0.9705159705159705
fold: 3 - cp:13 train: 0.9819537277129696 test: f1=0.9705704250938598, acc=0.9857844857844857
fold: 3 - cp:14 train: 0.9832991914180497 test: f1=0.975373043794735, acc=0.9881244881244882
fold: 3 - cp:15 train: 0.9833576675235542 test: f1=0.9761962594121933, acc=0.9885339885339886
fold: 3 - cp:16 train: 0.9832115029237163 test: f1=0.9785480547812386, acc=0.9896454896454896
fold: 3 - cp:17 train: 0.9824218582115166 test: f1=0.975591985428051, acc=0.9882414882414883
fold: 3 - cp:18 train: 0.9841767265141154 test: f1=0.9739299139080877, acc=0.9874224874224874
fold: 3 - cp:19 train: 0.9808422950385982 test: f1=0.9735501091967969, acc=0.9872469872469872
fold: 0 - cp:16 train: 0.9883652908677283 test: f1=0.9781021897810219, acc=0.9894706054401872
fold: 1 - cp:16 train: 0.9898277543061423 test: f1=0.9744525547445255, acc=0.9877157063468851
fold: 2 - cp:16 train: 0.9900227494312641 test: f1=0.9776704888352443, acc=0.9891781222579702
fold: 3 - cp:16 train: 0.9892102697432564 test: f1=0.9819277108433734, acc=0.9912255045334893
fold: 4 - cp:16 train: 0.9889827754306143 test: f1=0.9787492410443229, acc=0.9897630886224043
fold: 5 - cp:16 train: 0.9891777705557363 test: f1=0.9769137302551639, acc=0.9888856390757531
fold: 6 - cp:16 train: 0.989047773805655 test: f1=0.9860859044162129, acc=0.9932728868090085
fold: 7 - cp:16 train: 0.9895352616184596 test: f1=0.9738919247115969, acc=0.987423223164668
fold: 8 - cp:16 train: 0.988007799805005 test: f1=0.9789029535864978, acc=0.9897630886224043
fold: 9 - cp:16 train: 0.9890481222374898 test: f1=0.9811778992106862, acc=0.9909303686366296



PC8
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked^2
	marital-status_Married-AF-spouse
	workclass_Federal-gov workclass_Never-workeddot: graph is too large for cairo-renderer bitmaps. Scaling by 0.358748 to fit


PC9
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_? workclass_Local-gov
	occupation_Armed-Forces

PC4
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Self-emp-inc
	occupation_Armed-Forces

PC5
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	income
	workclass_Never-worked

PC16
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	occupation_Armed-Forces

PC2
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Self-emp-inc
	workclass_Without-pay
	occupation_Armed-Forces

PC14
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC3
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-spouse-absent
	relationship_Unmarried

PC10
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC7
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Federal-gov workclass_Local-gov
	workclass_Never-worked
	occupation_Armed-Forces

PC15
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	race_Amer-Indian-Eskimo
	marital-status_Married-AF-spouse

PC12
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	marital-status_Married-AF-spouse
	marital-status_Married-spouse-absent

PC6
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	workclass_Federal-gov

PC13
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	occupation_Armed-Forces

PC11
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	marital-status_Married-AF-spouse
	occupation_Armed-Forces

PC1
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay
	marital-status_Married-AF-spouse

Normalized confusion matrix
[[9.99228395e-01 7.71604938e-04]
 [1.21065375e-03 9.98789346e-01]]

              precision    recall  f1-score   support

           0       1.00      1.00      1.00      2592
           1       1.00      1.00      1.00       826

    accuracy                           1.00      3418
   macro avg       1.00      1.00      1.00      3418
weighted avg       1.00      1.00      1.00      3418

[REPAIR EXEC TIME]: 358.90931010246277fold: 0 - cp:1 train: 0.8043758043758045 test: f1=0.5348612453403286, acc=0.8029248318221702
fold: 0 - cp:2 train: 0.8188838188838188 test: f1=0.5695894721785152, acc=0.8239836209417959
fold: 0 - cp:3 train: 0.8292090792090792 test: f1=0.673711707432914, acc=0.8392512430535244
fold: 0 - cp:4 train: 0.8311980811980813 test: f1=0.674635786327979, acc=0.847148288973384
fold: 0 - cp:5 train: 0.8289750789750789 test: f1=0.6507409299948901, acc=0.8400701959637321
fold: 0 - cp:6 train: 0.8927986427986427 test: f1=0.7953319111557284, acc=0.9045919859608073
fold: 0 - cp:7 train: 0.9207031707031708 test: f1=0.8360259508968324, acc=0.9245978356244516
fold: 0 - cp:8 train: 0.9168714168714168 test: f1=0.8463544901254502, acc=0.9276396607195087
fold: 0 - cp:9 train: 0.9173979173979172 test: f1=0.832512937018806, acc=0.9223749634396022
fold: 0 - cp:10 train: 0.9154674154674154 test: f1=0.8306604369238542, acc=0.9215560105293945
fold: 0 - cp:11 train: 0.9595472095472095 test: f1=0.9318433009947195, acc=0.9675343667739105
fold: 0 - cp:12 train: 0.9658944658944658 test: f1=0.9431900946831755, acc=0.9726235741444867
fold: 0 - cp:13 train: 0.9814262314262313 test: f1=0.9646544394509899, acc=0.9829774787949693
fold: 0 - cp:14 train: 0.9817187317187317 test: f1=0.967163455713074, acc=0.9841474115238373
fold: 0 - cp:15 train: 0.9817479817479817 test: f1=0.963928097478586, acc=0.9825095057034221
fold: 0 - cp:16 train: 0.9806657306657307 test: f1=0.9682328783669526, acc=0.9846153846153847
fold: 0 - cp:17 train: 0.9804609804609805 test: f1=0.9674609894762308, acc=0.9842644047967242
fold: 0 - cp:18 train: 0.9813384813384813 test: f1=0.9687386843693422, acc=0.9848493711611582
fold: 0 - cp:19 train: 0.9801099801099801 test: f1=0.9706698853349426, acc=0.9857853173442527
fold: 1 - cp:1 train: 0.8035682701978912 test: f1=0.5185722576900754, acc=0.8058968058968059
fold: 1 - cp:2 train: 0.815004302779221 test: f1=0.525115562403698, acc=0.8197028197028197
fold: 1 - cp:3 train: 0.8262358794354583 test: f1=0.6810557474099653, acc=0.8487188487188487
fold: 1 - cp:4 train: 0.8276102475622831 test: f1=0.6709565647815865, acc=0.8444483444483445
fold: 1 - cp:5 train: 0.8291310027096176 test: f1=0.6590286425902864, acc=0.8398268398268398
fold: 1 - cp:6 train: 0.8893828019354459 test: f1=0.7722320749936661, acc=0.8948168948168949
fold: 1 - cp:7 train: 0.9185725757145972 test: f1=0.8563160543532965, acc=0.9331929331929332
fold: 1 - cp:8 train: 0.9181632908820555 test: f1=0.8465662496808781, acc=0.9296829296829296
fold: 1 - cp:9 train: 0.9165546556164245 test: f1=0.8499615285970762, acc=0.9315549315549315
fold: 1 - cp:10 train: 0.9158817270071364 test: f1=0.8496797689313074, acc=0.9299754299754299
fold: 1 - cp:11 train: 0.9570340954875349 test: f1=0.9334630823500791, acc=0.968000468000468
fold: 1 - cp:12 train: 0.9613044252433582 test: f1=0.9345311346295841, acc=0.9688779688779688
fold: 1 - cp:13 train: 0.9789120908096107 test: f1=0.9702373779973491, acc=0.9855504855504855
fold: 1 - cp:14 train: 0.9807252949215981 test: f1=0.9724280335236245, acc=0.9867204867204867
fold: 1 - cp:15 train: 0.9812226241215478 test: f1=0.9718992248062016, acc=0.9864279864279865
fold: 1 - cp:16 train: 0.983416236019184 test: f1=0.9700802724397956, acc=0.9856089856089856
fold: 1 - cp:17 train: 0.9814565148561405 test: f1=0.9728879206003388, acc=0.9868959868959869
fold: 1 - cp:18 train: 0.9822170532570627 test: f1=0.9712134094497753, acc=0.9861354861354862
fold: 1 - cp:19 train: 0.9821879401020721 test: f1=0.9698817217412512, acc=0.9855504855504855
fold: 2 - cp:1 train: 0.7997542997542998 test: f1=0.541033434650456, acc=0.8056741737350103
fold: 2 - cp:2 train: 0.8146425646425647 test: f1=0.5711434274308527, acc=0.8240421175782392
fold: 2 - cp:3 train: 0.8321925821925822 test: f1=0.6623946837545982, acc=0.8335770693185142
fold: 2 - cp:4 train: 0.8254943254943256 test: f1=0.6543303121852971, acc=0.8393682363264112
fold: 2 - cp:5 train: 0.8286240786240786 test: f1=0.6778389194597297, acc=0.84931266452179
fold: 2 - cp:6 train: 0.890985140985141 test: f1=0.8013161225006327, acc=0.9081602807838549
fold: 2 - cp:7 train: 0.917046917046917 test: f1=0.8432609793033821, acc=0.9273471775372916
fold: 2 - cp:8 train: 0.9148824148824148 test: f1=0.8351536703073407, acc=0.924071365896461
fold: 2 - cp:9 train: 0.9175149175149174 test: f1=0.8365798128004049, acc=0.9244223457151214
fold: 2 - cp:10 train: 0.9167544167544168 test: f1=0.8320947222572113, acc=0.9220239836209418
fold: 2 - cp:11 train: 0.9610389610389611 test: f1=0.9359338353289718, acc=0.969640245685873
fold: 2 - cp:12 train: 0.9628232128232128 test: f1=0.9373396848662513, acc=0.9699912255045335
fold: 2 - cp:13 train: 0.9809289809289808 test: f1=0.9783371656783252, acc=0.9895291020766306
fold: 2 - cp:14 train: 0.9828594828594829 test: f1=0.9762826718296224, acc=0.9885346592570927
fold: 2 - cp:15 train: 0.9825377325377325 test: f1=0.9768912673315495, acc=0.9888856390757531
fold: 2 - cp:16 train: 0.9818649818649818 test: f1=0.9717198689161306, acc=0.9863702837086867
fold: 2 - cp:17 train: 0.9826254826254825 test: f1=0.9738625363020329, acc=0.9873647265282246
fold: 2 - cp:18 train: 0.9825377325377327 test: f1=0.9713661732589177, acc=0.9861947937993565
fold: 2 - cp:19 train: 0.9809874809874809 test: f1=0.9708242159008024, acc=0.9859608072535829
fold: 3 - cp:1 train: 0.8080141925193399 test: f1=0.5345789180145009, acc=0.8047268047268047
fold: 3 - cp:2 train: 0.8217314502117965 test: f1=0.5500942438741483, acc=0.8184743184743185
fold: 3 - cp:3 train: 0.8340742200124511 test: f1=0.65296861212656, acc=0.838949338949339
fold: 3 - cp:4 train: 0.8306520829333184 test: f1=0.6714303015623108, acc=0.8412893412893413
fold: 3 - cp:5 train: 0.8331092188427238 test: f1=0.6874536005939124, acc=0.8522288522288523
fold: 3 - cp:6 train: 0.8922198015553187 test: f1=0.7934254762794173, acc=0.902948402948403
fold: 3 - cp:7 train: 0.9184266438010004 test: f1=0.8492073557387445, acc=0.9304434304434305
fold: 3 - cp:8 train: 0.9167885223980218 test: f1=0.8471843215083107, acc=0.9279279279279279
fold: 3 - cp:9 train: 0.9180169620331061 test: f1=0.8532465907669211, acc=0.9313794313794314
fold: 3 - cp:10 train: 0.9176659274635409 test: f1=0.8190750687802961, acc=0.9192114192114192
fold: 3 - cp:11 train: 0.960397636696186 test: f1=0.9275862068965517, acc=0.9656019656019657
fold: 3 - cp:12 train: 0.9632349169082908 test: f1=0.9461790790912405, acc=0.9740844740844741
fold: 3 - cp:13 train: 0.9795846498583981 test: f1=0.9706094916887498, acc=0.9857259857259857
fold: 3 - cp:14 train: 0.981661336919877 test: f1=0.9650518197155942, acc=0.983034983034983
fold: 3 - cp:15 train: 0.9816319910786453 test: f1=0.9719806763285023, acc=0.9864279864279865
fold: 3 - cp:16 train: 0.9800818627152272 test: f1=0.9711330286264134, acc=0.9859599859599859
fold: 3 - cp:17 train: 0.9799356399438289 test: f1=0.9736842105263158, acc=0.9872469872469872
fold: 3 - cp:18 train: 0.9816028402832333 test: f1=0.9722423364711562, acc=0.9865449865449866
fold: 3 - cp:19 train: 0.9830066192665632 test: f1=0.9768059917854555, acc=0.9887679887679888
fold: 0 - cp:13 train: 0.9893402664933377 test: f1=0.9751966122202056, acc=0.9880081895291021
fold: 1 - cp:13 train: 0.9870653233669158 test: f1=0.9731379731379731, acc=0.987130739982451
fold: 2 - cp:13 train: 0.9873253168670784 test: f1=0.9747292418772564, acc=0.9877157063468851
fold: 3 - cp:13 train: 0.9886902827429314 test: f1=0.9765484064942874, acc=0.9885931558935361
fold: 4 - cp:13 train: 0.9884627884302892 test: f1=0.9794933655006032, acc=0.9900555718046212
fold: 5 - cp:13 train: 0.9899902502437439 test: f1=0.9860690490611751, acc=0.9932728868090085
fold: 6 - cp:13 train: 0.9889827754306143 test: f1=0.9824135839902971, acc=0.9915179877157063
fold: 7 - cp:13 train: 0.9904127396815079 test: f1=0.9786975045648205, acc=0.9897630886224043
fold: 8 - cp:13 train: 0.9897302567435814 test: f1=0.9849124924562462, acc=0.9926879204445744
fold: 9 - cp:13 train: 0.9889831133039089 test: f1=0.9758454106280193, acc=0.9882972498537156



PC8
Features:
	workclass_Local-gov workclass_Never-worked
	marital-status_Married-AF-spouse
	workclass_? workclass_Never-worked
	occupation_Armed-Forces

PC9
Features:dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.341739 to fit

	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	workclass_? workclass_Local-gov

PC10
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC4
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC2
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Farming-fishing
	workclass_Without-pay
	workclass_Never-worked

PC3
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay
	marital-status_Married-spouse-absent

PC5
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	workclass_Never-worked
	occupation_Armed-Forces

PC7
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	occupation_Armed-Forces
	workclass_Never-worked

PC6
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	marital-status_Married-AF-spouse

PC12
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	marital-status_Married-AF-spouse
	occupation_Armed-Forces

PC13
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Priv-house-serv
	workclass_Never-worked
	workclass_Without-pay

PC11
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Never-worked
	marital-status_Married-AF-spouse

PC1
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Tech-support
	workclass_Without-pay
	occupation_Armed-Forces

Normalized confusion matrix
[[0.99845679 0.00154321]
 [0.00847458 0.99152542]]

              precision    recall  f1-score   support

           0       1.00      1.00      1.00      2592
           1       1.00      0.99      0.99       826

    accuracy                           1.00      3418
   macro avg       1.00      0.99      1.00      3418
weighted avg       1.00      1.00      1.00      3418

Traceback (most recent call last):
  File "decision-tree-f1-98-acc-99-auc-99.py", line 383, in <module>
    from sklearn.externals import joblib
ImportError: cannot import name 'joblib'
[Try Solution]: OrderedDict([('action', 'fqn'), ('old_fqn', 'sklearn.externals.joblib'), ('new_fqn', 'joblib'), ('line_no', 383)])fold: 0 - cp:1 train: 0.8135895635895636 test: f1=0.5530555555555555, acc=0.8117578239251243
fold: 0 - cp:2 train: 0.818883818883819 test: f1=0.5520023048112936, acc=0.818075460661012
fold: 0 - cp:3 train: 0.8330993330993332 test: f1=0.6730887983203655, acc=0.8451594033343083
fold: 0 - cp:4 train: 0.8265473265473265 test: f1=0.6547288776796975, acc=0.8398362094179584
fold: 0 - cp:5 train: 0.8283608283608284 test: f1=0.6670784434836319, acc=0.8423515647850248
fold: 0 - cp:6 train: 0.893968643968644 test: f1=0.7740273125483124, acc=0.8973968996782685
fold: 0 - cp:7 train: 0.9197086697086697 test: f1=0.8459439345075664, acc=0.9273471775372916
fold: 0 - cp:8 train: 0.917076167076167 test: f1=0.8370313695485846, acc=0.925241298625329
fold: 0 - cp:9 train: 0.9140634140634141 test: f1=0.8296258197248296, acc=0.9224919567124891
fold: 0 - cp:10 train: 0.9140634140634141 test: f1=0.834846192637418, acc=0.9233694062591401
fold: 0 - cp:11 train: 0.9565344565344565 test: f1=0.916935283907761, acc=0.960807253582919
fold: 0 - cp:12 train: 0.9627062127062127 test: f1=0.9360039081582805, acc=0.9693477625036561
fold: 0 - cp:13 train: 0.9800514800514801 test: f1=0.9681173475572796, acc=0.9846153846153847
fold: 0 - cp:14 train: 0.9812507312507313 test: f1=0.9729926123289331, acc=0.9869552500731208
fold: 0 - cp:15 train: 0.9819234819234819 test: f1=0.9716844143272023, acc=0.9863117870722433
fold: 0 - cp:16 train: 0.9827717327717329 test: f1=0.9756805807622504, acc=0.9882421760748757
fold: 0 - cp:17 train: 0.9832397332397333 test: f1=0.9756626506024096, acc=0.9881836794384323
fold: 0 - cp:18 train: 0.9813969813969814 test: f1=0.9713456655785273, acc=0.9861362971629132
fold: 0 - cp:19 train: 0.9817772317772318 test: f1=0.9720710917664127, acc=0.9864872769815736
fold: 1 - cp:1 train: 0.7959052107975833 test: f1=0.5031884057971014, acc=0.7994617994617995
fold: 1 - cp:2 train: 0.8178999769722747 test: f1=0.5508654722137868, acc=0.8269568269568269
fold: 1 - cp:3 train: 0.8245685148469425 test: f1=0.5942617214835549, acc=0.8304083304083304
fold: 1 - cp:4 train: 0.8254751767854248 test: f1=0.6393225721090235, acc=0.8405288405288406
fold: 1 - cp:5 train: 0.8233988250658817 test: f1=0.5323970635173955, acc=0.8285948285948286
fold: 1 - cp:6 train: 0.8898506963258952 test: f1=0.7928464977645305, acc=0.9024219024219025
fold: 1 - cp:7 train: 0.9185434728251761 test: f1=0.8560876209882832, acc=0.9338949338949339
fold: 1 - cp:8 train: 0.9157941822307756 test: f1=0.8511610201751048, acc=0.9313794313794314
fold: 1 - cp:9 train: 0.9166131933153459 test: f1=0.8489335006273526, acc=0.9295659295659295
fold: 1 - cp:10 train: 0.9142731738660611 test: f1=0.8515784114052952, acc=0.9317889317889317
fold: 1 - cp:11 train: 0.9576191405566699 test: f1=0.928361138370952, acc=0.9658359658359659
fold: 1 - cp:12 train: 0.9632055197392119 test: f1=0.941305140357273, acc=0.9717444717444718
fold: 1 - cp:13 train: 0.9797602287365975 test: f1=0.9718924157984009, acc=0.9864279864279865
fold: 1 - cp:14 train: 0.9802866779352506 test: f1=0.9695129357463864, acc=0.9853164853164853
fold: 1 - cp:15 train: 0.9802282120953154 test: f1=0.9702850212249848, acc=0.9856674856674856
fold: 1 - cp:16 train: 0.9807545552164176 test: f1=0.9720710917664127, acc=0.9864864864864865
fold: 1 - cp:17 train: 0.9788243065032958 test: f1=0.9722457883892862, acc=0.9866034866034866
fold: 1 - cp:18 train: 0.9802280204713523 test: f1=0.9704712546703628, acc=0.9856674856674856
fold: 1 - cp:19 train: 0.9793798243728051 test: f1=0.9699685153790264, acc=0.9854919854919855
fold: 2 - cp:1 train: 0.7925880425880426 test: f1=0.5247566652560305, acc=0.8029248318221702
fold: 2 - cp:2 train: 0.8143793143793143 test: f1=0.5958367126250339, acc=0.8250950570342205
fold: 2 - cp:3 train: 0.8213993213993214 test: f1=0.6116358658453114, acc=0.8340450424100614
fold: 2 - cp:4 train: 0.8203463203463204 test: f1=0.6308637135763753, acc=0.8347470020473823
fold: 2 - cp:5 train: 0.8277173277173278 test: f1=0.6595278656607447, acc=0.8363264112313542
fold: 2 - cp:6 train: 0.8954018954018954 test: f1=0.790510857286306, acc=0.9023691137759579
fold: 2 - cp:7 train: 0.9192406692406693 test: f1=0.8577861163227016, acc=0.9334893243638491
fold: 2 - cp:8 train: 0.9176904176904177 test: f1=0.8345286885245903, acc=0.9244223457151214
fold: 2 - cp:9 train: 0.9184509184509184 test: f1=0.845510455104551, acc=0.926528224627084
fold: 2 - cp:10 train: 0.9155844155844156 test: f1=0.8488745980707396, acc=0.9285171102661597
fold: 2 - cp:11 train: 0.9554814554814555 test: f1=0.9221689413500553, acc=0.962971629131325
fold: 2 - cp:12 train: 0.9672984672984672 test: f1=0.932549883706696, acc=0.9677683533196841
fold: 2 - cp:13 train: 0.98016848016848 test: f1=0.9677968161380482, acc=0.9844983913424978
fold: 2 - cp:14 train: 0.9813384813384813 test: f1=0.9703911295235774, acc=0.9857853173442527
fold: 2 - cp:15 train: 0.9824207324207325 test: f1=0.9720330739299611, acc=0.986545773618017
fold: 2 - cp:16 train: 0.9812507312507313 test: f1=0.9656010696487177, acc=0.9834454518865166
fold: 2 - cp:17 train: 0.9782379782379782 test: f1=0.9686131386861313, acc=0.9849078677976016
fold: 2 - cp:18 train: 0.9792617292617293 test: f1=0.9705882352941178, acc=0.9858438139806961
fold: 2 - cp:19 train: 0.9798174798174799 test: f1=0.9763263324025737, acc=0.9885931558935361
fold: 3 - cp:1 train: 0.8164960850676808 test: f1=0.5571719918422842, acc=0.8094653094653095
fold: 3 - cp:2 train: 0.818835995017558 test: f1=0.5400671434827032, acc=0.8156663156663156
fold: 3 - cp:3 train: 0.8278443401864085 test: f1=0.6018068102849201, acc=0.8323973323973324
fold: 3 - cp:4 train: 0.8279613266159827 test: f1=0.6390918580375784, acc=0.8381888381888382
fold: 3 - cp:5 train: 0.826791708693908 test: f1=0.6683113358825707, acc=0.8426933426933427
fold: 3 - cp:6 train: 0.8835038198047088 test: f1=0.7771312926890969, acc=0.896922896922897
fold: 3 - cp:7 train: 0.9144486364011399 test: f1=0.8438472066691552, acc=0.9265824265824266
fold: 3 - cp:8 train: 0.9145071296159271 test: f1=0.840373831775701, acc=0.9250614250614251
fold: 3 - cp:9 train: 0.9143023930986027 test: f1=0.8386119873817035, acc=0.9251784251784252
fold: 3 - cp:10 train: 0.914594927609668 test: f1=0.8460115891998521, acc=0.926933426933427
fold: 3 - cp:11 train: 0.9581749219091288 test: f1=0.9260537342864186, acc=0.9648999648999649
fold: 3 - cp:12 train: 0.9631178962601518 test: f1=0.938382854359474, acc=0.9703989703989704
fold: 3 - cp:13 train: 0.9814272614050341 test: f1=0.9728359290112277, acc=0.9868374868374868
fold: 3 - cp:14 train: 0.980345075338056 test: f1=0.9750906892382104, acc=0.987948987948988
fold: 3 - cp:15 train: 0.9832407255781145 test: f1=0.975266731328807, acc=0.9880659880659881
fold: 3 - cp:16 train: 0.9834161675820544 test: f1=0.9769585253456221, acc=0.9888849888849889
fold: 3 - cp:17 train: 0.9833285098844293 test: f1=0.9736049174400385, acc=0.9871884871884872
fold: 3 - cp:18 train: 0.9833870338959249 test: f1=0.9750726040658276, acc=0.987948987948988
fold: 3 - cp:19 train: 0.9829480849894981 test: f1=0.9742905651224837, acc=0.9875979875979876
fold: 0 - cp:16 train: 0.9888202794930127 test: f1=0.9745454545454545, acc=0.9877157063468851
fold: 1 - cp:16 train: 0.9893402664933376 test: f1=0.9756394640682096, acc=0.988300672711319
fold: 2 - cp:16 train: 0.9890152746181344 test: f1=0.9807228915662651, acc=0.9906405381690553
fold: 3 - cp:16 train: 0.9895677608059797 test: f1=0.9788774894387448, acc=0.9897630886224043
fold: 4 - cp:16 train: 0.9893402664933376 test: f1=0.9812688821752266, acc=0.9909330213512723
fold: 5 - cp:16 train: 0.9895677608059799 test: f1=0.9819711538461539, acc=0.9912255045334893
fold: 6 - cp:16 train: 0.9888202794930127 test: f1=0.9793187347931874, acc=0.9900555718046212
fold: 7 - cp:16 train: 0.9903477413064673 test: f1=0.9769696969696969, acc=0.9888856390757531
fold: 8 - cp:16 train: 0.9887552811179721 test: f1=0.9860521528198908, acc=0.9932728868090085
fold: 9 - cp:16 train: 0.9901531368473424 test: f1=0.9722557297949337, acc=0.9865418373317729



PC8
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Federal-gov workclass_Never-worked
	marital-status_Married-AF-spouse
	workclass_?^2

PC9
Features:
	workclass_Local-gov workclass_Never-workeddot: graph is too large for cairo-renderer bitmaps. Scaling by 0.374836 to fit

	workclass_Never-worked
	workclass_Without-pay
	occupation_?

PC4
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC16
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_? workclass_Never-worked
	occupation_Armed-Forces
	workclass_Never-worked

PC10
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	workclass_Without-pay

PC5
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	marital-status_Married-AF-spouse
	occupation_Armed-Forces

PC3
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	race_Asian-Pac-Islander
	workclass_Without-pay

PC14
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	workclass_Never-worked
	occupation_Tech-support

PC2
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	occupation_Priv-house-serv

PC7
Features:
	workclass_Local-gov workclass_Never-worked
	marital-status_Married-spouse-absent
	workclass_Without-pay
	workclass_Never-worked

PC15
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	workclass_Never-worked
	marital-status_Married-AF-spouse

PC12
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Without-pay
	marital-status_Married-AF-spouse
	workclass_? workclass_Federal-gov

PC6
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	workclass_Without-pay
	marital-status_Married-AF-spouse

PC13
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Priv-house-serv
	occupation_Armed-Forces

PC11
Features:
	workclass_Local-gov workclass_Never-worked
	workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse

PC1
Features:
	workclass_Local-gov workclass_Never-worked
	occupation_Armed-Forces
	marital-status_Married-AF-spouse
	workclass_Without-pay

Normalized confusion matrix
[[9.99228395e-01 7.71604938e-04]
 [8.47457627e-03 9.91525424e-01]]

              precision    recall  f1-score   support

           0       1.00      1.00      1.00      2592
           1       1.00      0.99      0.99       826

    accuracy                           1.00      3418
   macro avg       1.00      1.00      1.00      3418
weighted avg       1.00      1.00      1.00      3418

[REPAIR EXEC TIME]: 363.67363595962524